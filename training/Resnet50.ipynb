{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from torchvision.models import ResNet34_Weights\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor = torch.load('./data_tensor.pth')\n",
    "train_labels_tensor = torch.load('./labels_tensor.pth')\n",
    "val_data_tensor = torch.load('./val_data_tensor.pth')\n",
    "val_labels_tensor = torch.load('./val_labels_tensor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = [], []\n",
    "for i in range(len(train_labels_tensor)):\n",
    "    train_dataset.append((train_data_tensor[i], train_labels_tensor[i]))\n",
    "\n",
    "for i in range(len(val_labels_tensor)):\n",
    "    val_dataset.append((val_data_tensor[i], val_labels_tensor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x166b71830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# total_size = len(dataset)\n",
    "# train_size = int(total_size * 0.7)  # 70% for training\n",
    "# val_size = int(total_size * 0.2)    # 20% for validation\n",
    "\n",
    "# test_size = total_size - train_size - val_size  # Remaining 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet50 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features1 = 256\n",
    "out_features2 = 128\n",
    "\n",
    "class RESNET18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RESNET18, self).__init__()\n",
    "        # remove fully connected layer at the end\n",
    "        self.resnet = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "        # freeze parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.fc1 = nn.Linear(resnet18.fc.in_features, out_features1)\n",
    "        self.fc2 = nn.Linear(out_features1, out_features2)\n",
    "        self.fc3 = nn.Linear(out_features2, 30)\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_features1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(out_features2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        \n",
    "        # Forward pass through your fully connected layers\n",
    "        x = F.relu(self.batchnorm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model prediction and the actual value\n",
    "def get_accuracy(model, train=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if train:\n",
    "        for imgs, labels in train_loader:\n",
    "            if torch.backends.mps.is_built():\n",
    "                imgs = imgs.to(\"mps\")\n",
    "                labels = labels.to(\"mps\")\n",
    "\n",
    "            output = model(imgs)\n",
    "\n",
    "            # select index with maximum prediction\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            total += imgs.shape[0]\n",
    "\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for imgs, labels in val_loader:\n",
    "                model.eval()\n",
    "                if torch.backends.mps.is_built():\n",
    "                    imgs = imgs.to(\"mps\")\n",
    "                    labels = labels.to(\"mps\")\n",
    "                # get the output using alex net\n",
    "                output = model(imgs)\n",
    "\n",
    "                # select index with maximum prediction\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "                total += imgs.shape[0]\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using alex net\n",
    "def train(model, data, batch_size, num_epochs, learning_rate, momentum, verbose=False):\n",
    "    # use cross entropy loss function and SGD with momentum\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    iters, losses, train_acc, val_acc = [], [], [], []\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        mini_b = 0\n",
    "        mini_batch_correct = 0\n",
    "        mini_batch_total = 0\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "\n",
    "        for imgs, labels in iter(data):\n",
    "            if torch.backends.mps.is_built():\n",
    "                imgs = imgs.to(\"mps\")\n",
    "                labels = labels.to(\"mps\")\n",
    "            # calculate loss\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # find the loss and accuracy\n",
    "            pred = out.max(1, keepdim=True)[1]\n",
    "            mini_batch_correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "            mini_batch_total = imgs.shape[0]\n",
    "            train_acc.append(mini_batch_correct / mini_batch_total)\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss) / batch_size)\n",
    "            n += 1\n",
    "            mini_b += 1\n",
    "            \n",
    "            if verbose and n % 10 == 0:\n",
    "                print(\"Iteration: {} Training Accuracy: {} Loss: {}\".format(n, train_acc[-1], losses[-1]))\n",
    "        scheduler.step()\n",
    "        # print the accuracy\n",
    "        val_acc.append(get_accuracy(model, train=False))\n",
    "        print(\"Training Accuracy = {}\".format(train_acc[-1]))\n",
    "        print(\"Validation Accuracy = {}\".format(val_acc[-1]))\n",
    "\n",
    "    # plot the loss curve\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot the training and validation curve\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_acc, label=\"Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # print the final accuracies\n",
    "    print(\"Final Training Accuracy: {}\".format(get_accuracy(model, train=True)))\n",
    "    print(\"Validation Accuracy = {}\".format(get_accuracy(model, train=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.03125 Loss: 0.05357254669070244\n",
      "Iteration: 20 Training Accuracy: 0.09375 Loss: 0.05236569046974182\n",
      "Iteration: 30 Training Accuracy: 0.140625 Loss: 0.048341043293476105\n",
      "Iteration: 40 Training Accuracy: 0.203125 Loss: 0.0458836629986763\n",
      "Iteration: 50 Training Accuracy: 0.1875 Loss: 0.046270884573459625\n",
      "Iteration: 60 Training Accuracy: 0.296875 Loss: 0.040484920144081116\n",
      "Iteration: 70 Training Accuracy: 0.265625 Loss: 0.039922185242176056\n",
      "Iteration: 80 Training Accuracy: 0.328125 Loss: 0.03857525438070297\n",
      "Iteration: 90 Training Accuracy: 0.3125 Loss: 0.037560123950242996\n",
      "Iteration: 100 Training Accuracy: 0.40625 Loss: 0.03436734527349472\n",
      "Iteration: 110 Training Accuracy: 0.390625 Loss: 0.03366949409246445\n",
      "Iteration: 120 Training Accuracy: 0.4375 Loss: 0.027181318029761314\n",
      "Iteration: 130 Training Accuracy: 0.421875 Loss: 0.03192780166864395\n",
      "Iteration: 140 Training Accuracy: 0.375 Loss: 0.030198894441127777\n",
      "Iteration: 150 Training Accuracy: 0.46875 Loss: 0.027744654566049576\n",
      "Iteration: 160 Training Accuracy: 0.453125 Loss: 0.030692361295223236\n",
      "Iteration: 170 Training Accuracy: 0.515625 Loss: 0.027098046615719795\n",
      "Iteration: 180 Training Accuracy: 0.4375 Loss: 0.028683960437774658\n",
      "Iteration: 190 Training Accuracy: 0.5 Loss: 0.02591247484087944\n",
      "Iteration: 200 Training Accuracy: 0.453125 Loss: 0.026488861069083214\n",
      "Iteration: 210 Training Accuracy: 0.421875 Loss: 0.027152681723237038\n",
      "Iteration: 220 Training Accuracy: 0.515625 Loss: 0.02501598559319973\n",
      "Iteration: 230 Training Accuracy: 0.609375 Loss: 0.02596641331911087\n",
      "Iteration: 240 Training Accuracy: 0.578125 Loss: 0.02374875172972679\n",
      "Iteration: 250 Training Accuracy: 0.546875 Loss: 0.021372441202402115\n",
      "Iteration: 260 Training Accuracy: 0.609375 Loss: 0.019608203321695328\n",
      "Iteration: 270 Training Accuracy: 0.546875 Loss: 0.021537110209465027\n",
      "Iteration: 280 Training Accuracy: 0.46875 Loss: 0.02758505567908287\n",
      "Iteration: 290 Training Accuracy: 0.625 Loss: 0.020066557452082634\n",
      "Iteration: 300 Training Accuracy: 0.578125 Loss: 0.022781550884246826\n",
      "Iteration: 310 Training Accuracy: 0.40625 Loss: 0.027113191783428192\n",
      "Iteration: 320 Training Accuracy: 0.65625 Loss: 0.020627986639738083\n",
      "Iteration: 330 Training Accuracy: 0.578125 Loss: 0.020807282999157906\n",
      "Iteration: 340 Training Accuracy: 0.625 Loss: 0.019576430320739746\n",
      "Iteration: 350 Training Accuracy: 0.578125 Loss: 0.022562872618436813\n",
      "Iteration: 360 Training Accuracy: 0.5625 Loss: 0.02120981179177761\n",
      "Iteration: 370 Training Accuracy: 0.515625 Loss: 0.02101924829185009\n",
      "Iteration: 380 Training Accuracy: 0.515625 Loss: 0.02140471152961254\n",
      "Iteration: 390 Training Accuracy: 0.515625 Loss: 0.024976298213005066\n",
      "Iteration: 400 Training Accuracy: 0.609375 Loss: 0.019894573837518692\n",
      "Iteration: 410 Training Accuracy: 0.59375 Loss: 0.01967017725110054\n",
      "Iteration: 420 Training Accuracy: 0.65625 Loss: 0.016424981877207756\n",
      "Iteration: 430 Training Accuracy: 0.578125 Loss: 0.02128373458981514\n",
      "Iteration: 440 Training Accuracy: 0.6875 Loss: 0.016032230108976364\n",
      "Iteration: 450 Training Accuracy: 0.59375 Loss: 0.02191750332713127\n",
      "Iteration: 460 Training Accuracy: 0.5625 Loss: 0.020746860653162003\n",
      "Iteration: 470 Training Accuracy: 0.5 Loss: 0.02306588739156723\n",
      "Iteration: 480 Training Accuracy: 0.578125 Loss: 0.022142384201288223\n",
      "Iteration: 490 Training Accuracy: 0.65625 Loss: 0.01706179603934288\n",
      "Iteration: 500 Training Accuracy: 0.578125 Loss: 0.018738072365522385\n",
      "Iteration: 510 Training Accuracy: 0.71875 Loss: 0.015240609645843506\n",
      "Iteration: 520 Training Accuracy: 0.640625 Loss: 0.016867171972990036\n",
      "Iteration: 530 Training Accuracy: 0.6875 Loss: 0.01648152619600296\n",
      "Iteration: 540 Training Accuracy: 0.65625 Loss: 0.016765432432293892\n",
      "Iteration: 550 Training Accuracy: 0.65625 Loss: 0.01876801811158657\n",
      "Iteration: 560 Training Accuracy: 0.734375 Loss: 0.014992959797382355\n",
      "Iteration: 570 Training Accuracy: 0.703125 Loss: 0.0174516923725605\n",
      "Iteration: 580 Training Accuracy: 0.671875 Loss: 0.018105398863554\n",
      "Iteration: 590 Training Accuracy: 0.71875 Loss: 0.014620382338762283\n",
      "Iteration: 600 Training Accuracy: 0.59375 Loss: 0.018984943628311157\n",
      "Iteration: 610 Training Accuracy: 0.796875 Loss: 0.01208692416548729\n",
      "Iteration: 620 Training Accuracy: 0.640625 Loss: 0.019372254610061646\n",
      "Iteration: 630 Training Accuracy: 0.640625 Loss: 0.015377072617411613\n",
      "Iteration: 640 Training Accuracy: 0.6875 Loss: 0.017885945737361908\n",
      "Iteration: 650 Training Accuracy: 0.734375 Loss: 0.013394612818956375\n",
      "Iteration: 660 Training Accuracy: 0.703125 Loss: 0.016751930117607117\n",
      "Iteration: 670 Training Accuracy: 0.71875 Loss: 0.014926904812455177\n",
      "Iteration: 680 Training Accuracy: 0.625 Loss: 0.01763639599084854\n",
      "Iteration: 690 Training Accuracy: 0.703125 Loss: 0.015575839206576347\n",
      "Iteration: 700 Training Accuracy: 0.71875 Loss: 0.013828231021761894\n",
      "Iteration: 710 Training Accuracy: 0.765625 Loss: 0.012170839123427868\n",
      "Iteration: 720 Training Accuracy: 0.703125 Loss: 0.017308298498392105\n",
      "Iteration: 730 Training Accuracy: 0.671875 Loss: 0.016928739845752716\n",
      "Iteration: 740 Training Accuracy: 0.65625 Loss: 0.016137991100549698\n",
      "Iteration: 750 Training Accuracy: 0.609375 Loss: 0.01886891946196556\n",
      "Iteration: 760 Training Accuracy: 0.609375 Loss: 0.020118290558457375\n",
      "Iteration: 770 Training Accuracy: 0.75 Loss: 0.013382846489548683\n",
      "Iteration: 780 Training Accuracy: 0.703125 Loss: 0.016702154651284218\n",
      "Iteration: 790 Training Accuracy: 0.6875 Loss: 0.017824091017246246\n",
      "Iteration: 800 Training Accuracy: 0.734375 Loss: 0.018092969432473183\n",
      "Iteration: 810 Training Accuracy: 0.703125 Loss: 0.01508673932403326\n",
      "Iteration: 820 Training Accuracy: 0.671875 Loss: 0.016195666044950485\n",
      "Iteration: 830 Training Accuracy: 0.59375 Loss: 0.018187550827860832\n",
      "Iteration: 840 Training Accuracy: 0.734375 Loss: 0.01263202354311943\n",
      "Iteration: 850 Training Accuracy: 0.65625 Loss: 0.01664986088871956\n",
      "Iteration: 860 Training Accuracy: 0.78125 Loss: 0.010430589318275452\n",
      "Iteration: 870 Training Accuracy: 0.765625 Loss: 0.01185663789510727\n",
      "Iteration: 880 Training Accuracy: 0.71875 Loss: 0.015697011724114418\n",
      "Iteration: 890 Training Accuracy: 0.65625 Loss: 0.01493331603705883\n",
      "Iteration: 900 Training Accuracy: 0.75 Loss: 0.012337968684732914\n",
      "Iteration: 910 Training Accuracy: 0.65625 Loss: 0.01673234812915325\n",
      "Iteration: 920 Training Accuracy: 0.671875 Loss: 0.014770553447306156\n",
      "Iteration: 930 Training Accuracy: 0.75 Loss: 0.012915298342704773\n",
      "Training Accuracy = 0.5625\n",
      "Validation Accuracy = 0\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 0.671875 Loss: 0.02025044709444046\n",
      "Iteration: 950 Training Accuracy: 0.71875 Loss: 0.01772846281528473\n",
      "Iteration: 960 Training Accuracy: 0.578125 Loss: 0.01676056534051895\n",
      "Iteration: 970 Training Accuracy: 0.703125 Loss: 0.014355599880218506\n",
      "Iteration: 980 Training Accuracy: 0.703125 Loss: 0.01426820270717144\n",
      "Iteration: 990 Training Accuracy: 0.703125 Loss: 0.01233469694852829\n",
      "Iteration: 1000 Training Accuracy: 0.625 Loss: 0.017569102346897125\n",
      "Iteration: 1010 Training Accuracy: 0.75 Loss: 0.01209929957985878\n",
      "Iteration: 1020 Training Accuracy: 0.8125 Loss: 0.01198794599622488\n",
      "Iteration: 1030 Training Accuracy: 0.75 Loss: 0.015989214181900024\n",
      "Iteration: 1040 Training Accuracy: 0.703125 Loss: 0.015538781881332397\n",
      "Iteration: 1050 Training Accuracy: 0.65625 Loss: 0.015315815806388855\n",
      "Iteration: 1060 Training Accuracy: 0.65625 Loss: 0.016945291310548782\n",
      "Iteration: 1070 Training Accuracy: 0.71875 Loss: 0.015040417201817036\n",
      "Iteration: 1080 Training Accuracy: 0.71875 Loss: 0.013634927570819855\n",
      "Iteration: 1090 Training Accuracy: 0.71875 Loss: 0.014820371754467487\n",
      "Iteration: 1100 Training Accuracy: 0.6875 Loss: 0.014593474566936493\n",
      "Iteration: 1110 Training Accuracy: 0.8125 Loss: 0.011802911758422852\n",
      "Iteration: 1120 Training Accuracy: 0.71875 Loss: 0.012437880039215088\n",
      "Iteration: 1130 Training Accuracy: 0.765625 Loss: 0.012024052441120148\n",
      "Iteration: 1140 Training Accuracy: 0.75 Loss: 0.012077419087290764\n",
      "Iteration: 1150 Training Accuracy: 0.6875 Loss: 0.016565747559070587\n",
      "Iteration: 1160 Training Accuracy: 0.71875 Loss: 0.015631873160600662\n",
      "Iteration: 1170 Training Accuracy: 0.703125 Loss: 0.014965790323913097\n",
      "Iteration: 1180 Training Accuracy: 0.71875 Loss: 0.013091372326016426\n",
      "Iteration: 1190 Training Accuracy: 0.71875 Loss: 0.013908525928854942\n",
      "Iteration: 1200 Training Accuracy: 0.703125 Loss: 0.01512078382074833\n",
      "Iteration: 1210 Training Accuracy: 0.671875 Loss: 0.015455939806997776\n",
      "Iteration: 1220 Training Accuracy: 0.6875 Loss: 0.01651875674724579\n",
      "Iteration: 1230 Training Accuracy: 0.640625 Loss: 0.016200102865695953\n",
      "Iteration: 1240 Training Accuracy: 0.703125 Loss: 0.013000157661736012\n",
      "Iteration: 1250 Training Accuracy: 0.640625 Loss: 0.018650123849511147\n",
      "Iteration: 1260 Training Accuracy: 0.71875 Loss: 0.01329837180674076\n",
      "Iteration: 1270 Training Accuracy: 0.703125 Loss: 0.012382827699184418\n",
      "Iteration: 1280 Training Accuracy: 0.734375 Loss: 0.011976365000009537\n",
      "Iteration: 1290 Training Accuracy: 0.734375 Loss: 0.012692607939243317\n",
      "Iteration: 1300 Training Accuracy: 0.671875 Loss: 0.012921331450343132\n",
      "Iteration: 1310 Training Accuracy: 0.6875 Loss: 0.014726332388818264\n",
      "Iteration: 1320 Training Accuracy: 0.625 Loss: 0.01807265542447567\n",
      "Iteration: 1330 Training Accuracy: 0.734375 Loss: 0.014848467893898487\n",
      "Iteration: 1340 Training Accuracy: 0.6875 Loss: 0.014710834249854088\n",
      "Iteration: 1350 Training Accuracy: 0.625 Loss: 0.016599182039499283\n",
      "Iteration: 1360 Training Accuracy: 0.6875 Loss: 0.015389762818813324\n",
      "Iteration: 1370 Training Accuracy: 0.6875 Loss: 0.015416552312672138\n",
      "Iteration: 1380 Training Accuracy: 0.828125 Loss: 0.011013216339051723\n",
      "Iteration: 1390 Training Accuracy: 0.734375 Loss: 0.016076330095529556\n",
      "Iteration: 1400 Training Accuracy: 0.703125 Loss: 0.013771533966064453\n",
      "Iteration: 1410 Training Accuracy: 0.8125 Loss: 0.008363925851881504\n",
      "Iteration: 1420 Training Accuracy: 0.71875 Loss: 0.01264786534011364\n",
      "Iteration: 1430 Training Accuracy: 0.765625 Loss: 0.009992176666855812\n",
      "Iteration: 1440 Training Accuracy: 0.75 Loss: 0.01584913209080696\n",
      "Iteration: 1450 Training Accuracy: 0.71875 Loss: 0.014293954707682133\n",
      "Iteration: 1460 Training Accuracy: 0.65625 Loss: 0.014303535223007202\n",
      "Iteration: 1470 Training Accuracy: 0.75 Loss: 0.01156199537217617\n",
      "Iteration: 1480 Training Accuracy: 0.65625 Loss: 0.01521899551153183\n",
      "Iteration: 1490 Training Accuracy: 0.640625 Loss: 0.016494790092110634\n",
      "Iteration: 1500 Training Accuracy: 0.75 Loss: 0.012218032963573933\n",
      "Iteration: 1510 Training Accuracy: 0.78125 Loss: 0.013062658719718456\n",
      "Iteration: 1520 Training Accuracy: 0.828125 Loss: 0.010053340345621109\n",
      "Iteration: 1530 Training Accuracy: 0.78125 Loss: 0.01127975806593895\n",
      "Iteration: 1540 Training Accuracy: 0.734375 Loss: 0.012529890984296799\n",
      "Iteration: 1550 Training Accuracy: 0.734375 Loss: 0.014460690319538116\n",
      "Iteration: 1560 Training Accuracy: 0.671875 Loss: 0.01692851632833481\n",
      "Iteration: 1570 Training Accuracy: 0.78125 Loss: 0.010116172954440117\n",
      "Iteration: 1580 Training Accuracy: 0.84375 Loss: 0.008270781487226486\n",
      "Iteration: 1590 Training Accuracy: 0.6875 Loss: 0.013485589995980263\n",
      "Iteration: 1600 Training Accuracy: 0.796875 Loss: 0.011967724189162254\n",
      "Iteration: 1610 Training Accuracy: 0.765625 Loss: 0.014390108175575733\n",
      "Iteration: 1620 Training Accuracy: 0.8125 Loss: 0.010093551129102707\n",
      "Iteration: 1630 Training Accuracy: 0.75 Loss: 0.016323808580636978\n",
      "Iteration: 1640 Training Accuracy: 0.703125 Loss: 0.01516035944223404\n",
      "Iteration: 1650 Training Accuracy: 0.734375 Loss: 0.014806045219302177\n",
      "Iteration: 1660 Training Accuracy: 0.796875 Loss: 0.008616484701633453\n",
      "Iteration: 1670 Training Accuracy: 0.8125 Loss: 0.011503945104777813\n",
      "Iteration: 1680 Training Accuracy: 0.828125 Loss: 0.010160203091800213\n",
      "Iteration: 1690 Training Accuracy: 0.765625 Loss: 0.012842363677918911\n",
      "Iteration: 1700 Training Accuracy: 0.703125 Loss: 0.01344267651438713\n",
      "Iteration: 1710 Training Accuracy: 0.71875 Loss: 0.014355846680700779\n",
      "Iteration: 1720 Training Accuracy: 0.71875 Loss: 0.013467289507389069\n",
      "Iteration: 1730 Training Accuracy: 0.640625 Loss: 0.014547509141266346\n",
      "Iteration: 1740 Training Accuracy: 0.78125 Loss: 0.012300681322813034\n",
      "Iteration: 1750 Training Accuracy: 0.734375 Loss: 0.012097195722162724\n",
      "Iteration: 1760 Training Accuracy: 0.75 Loss: 0.013239624910056591\n",
      "Iteration: 1770 Training Accuracy: 0.75 Loss: 0.011244643479585648\n",
      "Iteration: 1780 Training Accuracy: 0.609375 Loss: 0.015563299879431725\n",
      "Iteration: 1790 Training Accuracy: 0.71875 Loss: 0.01675456389784813\n",
      "Iteration: 1800 Training Accuracy: 0.796875 Loss: 0.011583523824810982\n",
      "Iteration: 1810 Training Accuracy: 0.796875 Loss: 0.009445259347558022\n",
      "Iteration: 1820 Training Accuracy: 0.6875 Loss: 0.011516258120536804\n",
      "Iteration: 1830 Training Accuracy: 0.796875 Loss: 0.011669116094708443\n",
      "Iteration: 1840 Training Accuracy: 0.8125 Loss: 0.009005039930343628\n",
      "Iteration: 1850 Training Accuracy: 0.78125 Loss: 0.01391182653605938\n",
      "Iteration: 1860 Training Accuracy: 0.78125 Loss: 0.009958719834685326\n",
      "Iteration: 1870 Training Accuracy: 0.71875 Loss: 0.01342723984271288\n",
      "Training Accuracy = 0.65625\n",
      "Validation Accuracy = 0\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 0.640625 Loss: 0.01695415750145912\n",
      "Iteration: 1890 Training Accuracy: 0.75 Loss: 0.012134213000535965\n",
      "Iteration: 1900 Training Accuracy: 0.859375 Loss: 0.010137208737432957\n",
      "Iteration: 1910 Training Accuracy: 0.671875 Loss: 0.013498910702764988\n",
      "Iteration: 1920 Training Accuracy: 0.6875 Loss: 0.01336807943880558\n",
      "Iteration: 1930 Training Accuracy: 0.71875 Loss: 0.013231275603175163\n",
      "Iteration: 1940 Training Accuracy: 0.6875 Loss: 0.011378074996173382\n",
      "Iteration: 1950 Training Accuracy: 0.765625 Loss: 0.014847327955067158\n",
      "Iteration: 1960 Training Accuracy: 0.78125 Loss: 0.008681820705533028\n",
      "Iteration: 1970 Training Accuracy: 0.6875 Loss: 0.014986900612711906\n",
      "Iteration: 1980 Training Accuracy: 0.8125 Loss: 0.011097857728600502\n",
      "Iteration: 1990 Training Accuracy: 0.8125 Loss: 0.010324652306735516\n",
      "Iteration: 2000 Training Accuracy: 0.78125 Loss: 0.010503377765417099\n",
      "Iteration: 2010 Training Accuracy: 0.6875 Loss: 0.018786679953336716\n",
      "Iteration: 2020 Training Accuracy: 0.84375 Loss: 0.008943596854805946\n",
      "Iteration: 2030 Training Accuracy: 0.75 Loss: 0.011795870028436184\n",
      "Iteration: 2040 Training Accuracy: 0.765625 Loss: 0.012698424980044365\n",
      "Iteration: 2050 Training Accuracy: 0.765625 Loss: 0.01215955801308155\n",
      "Iteration: 2060 Training Accuracy: 0.78125 Loss: 0.0118003124371171\n",
      "Iteration: 2070 Training Accuracy: 0.71875 Loss: 0.013589419424533844\n",
      "Iteration: 2080 Training Accuracy: 0.8125 Loss: 0.008014822378754616\n",
      "Iteration: 2090 Training Accuracy: 0.78125 Loss: 0.011292696930468082\n",
      "Iteration: 2100 Training Accuracy: 0.75 Loss: 0.012170973233878613\n",
      "Iteration: 2110 Training Accuracy: 0.71875 Loss: 0.013647288084030151\n",
      "Iteration: 2120 Training Accuracy: 0.65625 Loss: 0.015624787658452988\n",
      "Iteration: 2130 Training Accuracy: 0.796875 Loss: 0.00988126639276743\n",
      "Iteration: 2140 Training Accuracy: 0.828125 Loss: 0.009093649685382843\n",
      "Iteration: 2150 Training Accuracy: 0.765625 Loss: 0.010933801531791687\n",
      "Iteration: 2160 Training Accuracy: 0.828125 Loss: 0.01000124029815197\n",
      "Iteration: 2170 Training Accuracy: 0.59375 Loss: 0.017506612464785576\n",
      "Iteration: 2180 Training Accuracy: 0.828125 Loss: 0.007696098648011684\n",
      "Iteration: 2190 Training Accuracy: 0.84375 Loss: 0.00867029745131731\n",
      "Iteration: 2200 Training Accuracy: 0.796875 Loss: 0.01016555167734623\n",
      "Iteration: 2210 Training Accuracy: 0.8125 Loss: 0.01157812774181366\n",
      "Iteration: 2220 Training Accuracy: 0.671875 Loss: 0.014799287542700768\n",
      "Iteration: 2230 Training Accuracy: 0.6875 Loss: 0.012400940991938114\n",
      "Iteration: 2240 Training Accuracy: 0.796875 Loss: 0.012822914868593216\n",
      "Iteration: 2250 Training Accuracy: 0.8125 Loss: 0.010825278237462044\n",
      "Iteration: 2260 Training Accuracy: 0.71875 Loss: 0.012885641306638718\n",
      "Iteration: 2270 Training Accuracy: 0.765625 Loss: 0.011877020820975304\n",
      "Iteration: 2280 Training Accuracy: 0.765625 Loss: 0.013532651588320732\n",
      "Iteration: 2290 Training Accuracy: 0.859375 Loss: 0.008134977892041206\n",
      "Iteration: 2300 Training Accuracy: 0.8125 Loss: 0.010113053023815155\n",
      "Iteration: 2310 Training Accuracy: 0.828125 Loss: 0.00879728514701128\n",
      "Iteration: 2320 Training Accuracy: 0.78125 Loss: 0.010034555569291115\n",
      "Iteration: 2330 Training Accuracy: 0.671875 Loss: 0.016235467046499252\n",
      "Iteration: 2340 Training Accuracy: 0.796875 Loss: 0.01042184792459011\n",
      "Iteration: 2350 Training Accuracy: 0.78125 Loss: 0.009861770085990429\n",
      "Iteration: 2360 Training Accuracy: 0.625 Loss: 0.017126549035310745\n",
      "Iteration: 2370 Training Accuracy: 0.75 Loss: 0.011970169842243195\n",
      "Iteration: 2380 Training Accuracy: 0.609375 Loss: 0.015344197861850262\n",
      "Iteration: 2390 Training Accuracy: 0.734375 Loss: 0.01522127166390419\n",
      "Iteration: 2400 Training Accuracy: 0.8125 Loss: 0.010833735577762127\n",
      "Iteration: 2410 Training Accuracy: 0.75 Loss: 0.010623661801218987\n",
      "Iteration: 2420 Training Accuracy: 0.796875 Loss: 0.011573480442166328\n",
      "Iteration: 2430 Training Accuracy: 0.78125 Loss: 0.010899793356657028\n",
      "Iteration: 2440 Training Accuracy: 0.796875 Loss: 0.009061842225492\n",
      "Iteration: 2450 Training Accuracy: 0.734375 Loss: 0.011783061549067497\n",
      "Iteration: 2460 Training Accuracy: 0.875 Loss: 0.007208374794572592\n",
      "Iteration: 2470 Training Accuracy: 0.84375 Loss: 0.010746793821454048\n",
      "Iteration: 2480 Training Accuracy: 0.765625 Loss: 0.011138774454593658\n",
      "Iteration: 2490 Training Accuracy: 0.828125 Loss: 0.009328034706413746\n",
      "Iteration: 2500 Training Accuracy: 0.6875 Loss: 0.013347936794161797\n",
      "Iteration: 2510 Training Accuracy: 0.765625 Loss: 0.010256470181047916\n",
      "Iteration: 2520 Training Accuracy: 0.796875 Loss: 0.008996402844786644\n",
      "Iteration: 2530 Training Accuracy: 0.6875 Loss: 0.012849733233451843\n",
      "Iteration: 2540 Training Accuracy: 0.796875 Loss: 0.010685380548238754\n",
      "Iteration: 2550 Training Accuracy: 0.8125 Loss: 0.010711412876844406\n",
      "Iteration: 2560 Training Accuracy: 0.765625 Loss: 0.013386651873588562\n",
      "Iteration: 2570 Training Accuracy: 0.78125 Loss: 0.011642436496913433\n",
      "Iteration: 2580 Training Accuracy: 0.75 Loss: 0.014281600713729858\n",
      "Iteration: 2590 Training Accuracy: 0.78125 Loss: 0.009065970778465271\n",
      "Iteration: 2600 Training Accuracy: 0.78125 Loss: 0.009708475321531296\n",
      "Iteration: 2610 Training Accuracy: 0.671875 Loss: 0.012230625376105309\n",
      "Iteration: 2620 Training Accuracy: 0.671875 Loss: 0.0132211372256279\n",
      "Iteration: 2630 Training Accuracy: 0.6875 Loss: 0.014223257079720497\n",
      "Iteration: 2640 Training Accuracy: 0.65625 Loss: 0.016217460855841637\n",
      "Iteration: 2650 Training Accuracy: 0.703125 Loss: 0.01278744451701641\n",
      "Iteration: 2660 Training Accuracy: 0.75 Loss: 0.011002011597156525\n",
      "Iteration: 2670 Training Accuracy: 0.78125 Loss: 0.011715016327798367\n",
      "Iteration: 2680 Training Accuracy: 0.71875 Loss: 0.017221637070178986\n",
      "Iteration: 2690 Training Accuracy: 0.78125 Loss: 0.00994871649891138\n",
      "Iteration: 2700 Training Accuracy: 0.796875 Loss: 0.01102328859269619\n",
      "Iteration: 2710 Training Accuracy: 0.765625 Loss: 0.009988231584429741\n",
      "Iteration: 2720 Training Accuracy: 0.828125 Loss: 0.010515100322663784\n",
      "Iteration: 2730 Training Accuracy: 0.75 Loss: 0.0137980617582798\n",
      "Iteration: 2740 Training Accuracy: 0.796875 Loss: 0.009190352633595467\n",
      "Iteration: 2750 Training Accuracy: 0.8125 Loss: 0.010441023856401443\n",
      "Iteration: 2760 Training Accuracy: 0.765625 Loss: 0.011606136336922646\n",
      "Iteration: 2770 Training Accuracy: 0.75 Loss: 0.013283965177834034\n",
      "Iteration: 2780 Training Accuracy: 0.859375 Loss: 0.008054399862885475\n",
      "Iteration: 2790 Training Accuracy: 0.75 Loss: 0.01302744448184967\n",
      "Iteration: 2800 Training Accuracy: 0.6875 Loss: 0.015204571187496185\n",
      "Iteration: 2810 Training Accuracy: 0.875 Loss: 0.008048301562666893\n",
      "Training Accuracy = 0.71875\n",
      "Validation Accuracy = 0\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 0.796875 Loss: 0.009183820337057114\n",
      "Iteration: 2830 Training Accuracy: 0.8125 Loss: 0.008743178099393845\n",
      "Iteration: 2840 Training Accuracy: 0.765625 Loss: 0.010477038100361824\n",
      "Iteration: 2850 Training Accuracy: 0.828125 Loss: 0.010967539623379707\n",
      "Iteration: 2860 Training Accuracy: 0.796875 Loss: 0.011261038482189178\n",
      "Iteration: 2870 Training Accuracy: 0.78125 Loss: 0.011140113696455956\n",
      "Iteration: 2880 Training Accuracy: 0.78125 Loss: 0.014984183013439178\n",
      "Iteration: 2890 Training Accuracy: 0.78125 Loss: 0.009195918217301369\n",
      "Iteration: 2900 Training Accuracy: 0.8125 Loss: 0.01095881499350071\n",
      "Iteration: 2910 Training Accuracy: 0.765625 Loss: 0.009118907153606415\n",
      "Iteration: 2920 Training Accuracy: 0.703125 Loss: 0.0118337357416749\n",
      "Iteration: 2930 Training Accuracy: 0.84375 Loss: 0.010026111267507076\n",
      "Iteration: 2940 Training Accuracy: 0.71875 Loss: 0.012654310092329979\n",
      "Iteration: 2950 Training Accuracy: 0.75 Loss: 0.010199380107223988\n",
      "Iteration: 2960 Training Accuracy: 0.859375 Loss: 0.009679210372269154\n",
      "Iteration: 2970 Training Accuracy: 0.71875 Loss: 0.013682848773896694\n",
      "Iteration: 2980 Training Accuracy: 0.8125 Loss: 0.01111422199755907\n",
      "Iteration: 2990 Training Accuracy: 0.796875 Loss: 0.009825450368225574\n",
      "Iteration: 3000 Training Accuracy: 0.796875 Loss: 0.010742660611867905\n",
      "Iteration: 3010 Training Accuracy: 0.78125 Loss: 0.010835761204361916\n",
      "Iteration: 3020 Training Accuracy: 0.84375 Loss: 0.008254824206233025\n",
      "Iteration: 3030 Training Accuracy: 0.71875 Loss: 0.014354397542774677\n",
      "Iteration: 3040 Training Accuracy: 0.84375 Loss: 0.008082509972155094\n",
      "Iteration: 3050 Training Accuracy: 0.828125 Loss: 0.009294005110859871\n",
      "Iteration: 3060 Training Accuracy: 0.75 Loss: 0.013140799477696419\n",
      "Iteration: 3070 Training Accuracy: 0.828125 Loss: 0.00878829788416624\n",
      "Iteration: 3080 Training Accuracy: 0.796875 Loss: 0.010564686730504036\n",
      "Iteration: 3090 Training Accuracy: 0.828125 Loss: 0.008714258670806885\n",
      "Iteration: 3100 Training Accuracy: 0.859375 Loss: 0.006441943813115358\n",
      "Iteration: 3110 Training Accuracy: 0.78125 Loss: 0.010315780527889729\n",
      "Iteration: 3120 Training Accuracy: 0.875 Loss: 0.008931232616305351\n",
      "Iteration: 3130 Training Accuracy: 0.75 Loss: 0.012212999165058136\n",
      "Iteration: 3140 Training Accuracy: 0.8125 Loss: 0.010450990870594978\n",
      "Iteration: 3150 Training Accuracy: 0.6875 Loss: 0.011995055712759495\n",
      "Iteration: 3160 Training Accuracy: 0.859375 Loss: 0.007439448498189449\n",
      "Iteration: 3170 Training Accuracy: 0.765625 Loss: 0.008544307202100754\n",
      "Iteration: 3180 Training Accuracy: 0.84375 Loss: 0.007201865315437317\n",
      "Iteration: 3190 Training Accuracy: 0.84375 Loss: 0.009651204571127892\n",
      "Iteration: 3200 Training Accuracy: 0.765625 Loss: 0.01252390630543232\n",
      "Iteration: 3210 Training Accuracy: 0.75 Loss: 0.009720150381326675\n",
      "Iteration: 3220 Training Accuracy: 0.890625 Loss: 0.006982735823839903\n",
      "Iteration: 3230 Training Accuracy: 0.671875 Loss: 0.01565452478826046\n",
      "Iteration: 3240 Training Accuracy: 0.8125 Loss: 0.010176003910601139\n",
      "Iteration: 3250 Training Accuracy: 0.796875 Loss: 0.009986421093344688\n",
      "Iteration: 3260 Training Accuracy: 0.78125 Loss: 0.010432041250169277\n",
      "Iteration: 3270 Training Accuracy: 0.8125 Loss: 0.00981655903160572\n",
      "Iteration: 3280 Training Accuracy: 0.734375 Loss: 0.012908563017845154\n",
      "Iteration: 3290 Training Accuracy: 0.75 Loss: 0.01179539319127798\n",
      "Iteration: 3300 Training Accuracy: 0.75 Loss: 0.011264226399362087\n",
      "Iteration: 3310 Training Accuracy: 0.796875 Loss: 0.009431088343262672\n",
      "Iteration: 3320 Training Accuracy: 0.859375 Loss: 0.009534008800983429\n",
      "Iteration: 3330 Training Accuracy: 0.765625 Loss: 0.012240088544785976\n",
      "Iteration: 3340 Training Accuracy: 0.765625 Loss: 0.008640885353088379\n",
      "Iteration: 3350 Training Accuracy: 0.8125 Loss: 0.009987259283661842\n",
      "Iteration: 3360 Training Accuracy: 0.703125 Loss: 0.01212934497743845\n",
      "Iteration: 3370 Training Accuracy: 0.828125 Loss: 0.0080430768430233\n",
      "Iteration: 3380 Training Accuracy: 0.78125 Loss: 0.00994611345231533\n",
      "Iteration: 3390 Training Accuracy: 0.84375 Loss: 0.008309795521199703\n",
      "Iteration: 3400 Training Accuracy: 0.84375 Loss: 0.008702315390110016\n",
      "Iteration: 3410 Training Accuracy: 0.796875 Loss: 0.00940767489373684\n",
      "Iteration: 3420 Training Accuracy: 0.796875 Loss: 0.010434390977025032\n",
      "Iteration: 3430 Training Accuracy: 0.84375 Loss: 0.00816432572901249\n",
      "Iteration: 3440 Training Accuracy: 0.84375 Loss: 0.008003516122698784\n",
      "Iteration: 3450 Training Accuracy: 0.8125 Loss: 0.008495746180415154\n",
      "Iteration: 3460 Training Accuracy: 0.75 Loss: 0.012616039253771305\n",
      "Iteration: 3470 Training Accuracy: 0.75 Loss: 0.01149021741002798\n",
      "Iteration: 3480 Training Accuracy: 0.859375 Loss: 0.006964072119444609\n",
      "Iteration: 3490 Training Accuracy: 0.796875 Loss: 0.008698641322553158\n",
      "Iteration: 3500 Training Accuracy: 0.78125 Loss: 0.013307145796716213\n",
      "Iteration: 3510 Training Accuracy: 0.84375 Loss: 0.009769919328391552\n",
      "Iteration: 3520 Training Accuracy: 0.765625 Loss: 0.010766308754682541\n",
      "Iteration: 3530 Training Accuracy: 0.8125 Loss: 0.010031755082309246\n",
      "Iteration: 3540 Training Accuracy: 0.84375 Loss: 0.0072074225172400475\n",
      "Iteration: 3550 Training Accuracy: 0.78125 Loss: 0.014915629290044308\n",
      "Iteration: 3560 Training Accuracy: 0.828125 Loss: 0.008743412792682648\n",
      "Iteration: 3570 Training Accuracy: 0.8125 Loss: 0.00867027323693037\n",
      "Iteration: 3580 Training Accuracy: 0.75 Loss: 0.01029239408671856\n",
      "Iteration: 3590 Training Accuracy: 0.796875 Loss: 0.009308572858572006\n",
      "Iteration: 3600 Training Accuracy: 0.734375 Loss: 0.012836567126214504\n",
      "Iteration: 3610 Training Accuracy: 0.828125 Loss: 0.007439580745995045\n",
      "Iteration: 3620 Training Accuracy: 0.828125 Loss: 0.007981841452419758\n",
      "Iteration: 3630 Training Accuracy: 0.734375 Loss: 0.010869068093597889\n",
      "Iteration: 3640 Training Accuracy: 0.75 Loss: 0.011925230734050274\n",
      "Iteration: 3650 Training Accuracy: 0.796875 Loss: 0.011619576252996922\n",
      "Iteration: 3660 Training Accuracy: 0.765625 Loss: 0.009276222437620163\n",
      "Iteration: 3670 Training Accuracy: 0.734375 Loss: 0.016677850857377052\n",
      "Iteration: 3680 Training Accuracy: 0.828125 Loss: 0.008604181930422783\n",
      "Iteration: 3690 Training Accuracy: 0.75 Loss: 0.012309286743402481\n",
      "Iteration: 3700 Training Accuracy: 0.703125 Loss: 0.01584116742014885\n",
      "Iteration: 3710 Training Accuracy: 0.796875 Loss: 0.008865075185894966\n",
      "Iteration: 3720 Training Accuracy: 0.8125 Loss: 0.010235747322440147\n",
      "Iteration: 3730 Training Accuracy: 0.859375 Loss: 0.00815375056117773\n",
      "Iteration: 3740 Training Accuracy: 0.84375 Loss: 0.009140527807176113\n",
      "Iteration: 3750 Training Accuracy: 0.875 Loss: 0.006130238063633442\n",
      "Training Accuracy = 0.65625\n",
      "Validation Accuracy = 0\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 0.8125 Loss: 0.009572946466505527\n",
      "Iteration: 3770 Training Accuracy: 0.828125 Loss: 0.0077385976910591125\n",
      "Iteration: 3780 Training Accuracy: 0.828125 Loss: 0.011662838980555534\n",
      "Iteration: 3790 Training Accuracy: 0.8125 Loss: 0.010764947161078453\n",
      "Iteration: 3800 Training Accuracy: 0.796875 Loss: 0.010781505145132542\n",
      "Iteration: 3810 Training Accuracy: 0.796875 Loss: 0.01172146201133728\n",
      "Iteration: 3820 Training Accuracy: 0.71875 Loss: 0.01213054172694683\n",
      "Iteration: 3830 Training Accuracy: 0.828125 Loss: 0.008936693891882896\n",
      "Iteration: 3840 Training Accuracy: 0.734375 Loss: 0.010269628837704659\n",
      "Iteration: 3850 Training Accuracy: 0.828125 Loss: 0.008060606196522713\n",
      "Iteration: 3860 Training Accuracy: 0.796875 Loss: 0.01176530309021473\n",
      "Iteration: 3870 Training Accuracy: 0.71875 Loss: 0.013299698941409588\n",
      "Iteration: 3880 Training Accuracy: 0.890625 Loss: 0.006475592032074928\n",
      "Iteration: 3890 Training Accuracy: 0.796875 Loss: 0.009707926772534847\n",
      "Iteration: 3900 Training Accuracy: 0.765625 Loss: 0.011215616017580032\n",
      "Iteration: 3910 Training Accuracy: 0.8125 Loss: 0.0075268857181072235\n",
      "Iteration: 3920 Training Accuracy: 0.84375 Loss: 0.007998136803507805\n",
      "Iteration: 3930 Training Accuracy: 0.84375 Loss: 0.006753416731953621\n",
      "Iteration: 3940 Training Accuracy: 0.859375 Loss: 0.007930010557174683\n",
      "Iteration: 3950 Training Accuracy: 0.78125 Loss: 0.010351036675274372\n",
      "Iteration: 3960 Training Accuracy: 0.765625 Loss: 0.01063523069024086\n",
      "Iteration: 3970 Training Accuracy: 0.734375 Loss: 0.010302694514393806\n",
      "Iteration: 3980 Training Accuracy: 0.78125 Loss: 0.008417004719376564\n",
      "Iteration: 3990 Training Accuracy: 0.828125 Loss: 0.009710527956485748\n",
      "Iteration: 4000 Training Accuracy: 0.796875 Loss: 0.010666515678167343\n",
      "Iteration: 4010 Training Accuracy: 0.640625 Loss: 0.016322245821356773\n",
      "Iteration: 4020 Training Accuracy: 0.796875 Loss: 0.012750593945384026\n",
      "Iteration: 4030 Training Accuracy: 0.828125 Loss: 0.008556525222957134\n",
      "Iteration: 4040 Training Accuracy: 0.8125 Loss: 0.010437490418553352\n",
      "Iteration: 4050 Training Accuracy: 0.84375 Loss: 0.008810071274638176\n",
      "Iteration: 4060 Training Accuracy: 0.828125 Loss: 0.00987391546368599\n",
      "Iteration: 4070 Training Accuracy: 0.71875 Loss: 0.012254889123141766\n",
      "Iteration: 4080 Training Accuracy: 0.875 Loss: 0.00846889242529869\n",
      "Iteration: 4090 Training Accuracy: 0.796875 Loss: 0.007615315727889538\n",
      "Iteration: 4100 Training Accuracy: 0.796875 Loss: 0.010050680488348007\n",
      "Iteration: 4110 Training Accuracy: 0.765625 Loss: 0.010530166327953339\n",
      "Iteration: 4120 Training Accuracy: 0.8125 Loss: 0.009820416569709778\n",
      "Iteration: 4130 Training Accuracy: 0.8125 Loss: 0.011274024844169617\n",
      "Iteration: 4140 Training Accuracy: 0.734375 Loss: 0.011609010398387909\n",
      "Iteration: 4150 Training Accuracy: 0.765625 Loss: 0.010225104168057442\n",
      "Iteration: 4160 Training Accuracy: 0.8125 Loss: 0.007807690650224686\n",
      "Iteration: 4170 Training Accuracy: 0.84375 Loss: 0.007732950150966644\n",
      "Iteration: 4180 Training Accuracy: 0.8125 Loss: 0.010736498050391674\n",
      "Iteration: 4190 Training Accuracy: 0.84375 Loss: 0.008570346049964428\n",
      "Iteration: 4200 Training Accuracy: 0.75 Loss: 0.009923677891492844\n",
      "Iteration: 4210 Training Accuracy: 0.828125 Loss: 0.009225787594914436\n",
      "Iteration: 4220 Training Accuracy: 0.84375 Loss: 0.008114291355013847\n",
      "Iteration: 4230 Training Accuracy: 0.75 Loss: 0.012930180877447128\n",
      "Iteration: 4240 Training Accuracy: 0.78125 Loss: 0.011581086553633213\n",
      "Iteration: 4250 Training Accuracy: 0.828125 Loss: 0.008198024705052376\n",
      "Iteration: 4260 Training Accuracy: 0.890625 Loss: 0.00808035023510456\n",
      "Iteration: 4270 Training Accuracy: 0.828125 Loss: 0.009277318604290485\n",
      "Iteration: 4280 Training Accuracy: 0.828125 Loss: 0.00933581218123436\n",
      "Iteration: 4290 Training Accuracy: 0.828125 Loss: 0.008529557846486568\n",
      "Iteration: 4300 Training Accuracy: 0.8125 Loss: 0.008925318717956543\n",
      "Iteration: 4310 Training Accuracy: 0.71875 Loss: 0.01474049873650074\n",
      "Iteration: 4320 Training Accuracy: 0.84375 Loss: 0.009545545093715191\n",
      "Iteration: 4330 Training Accuracy: 0.84375 Loss: 0.00877643283456564\n",
      "Iteration: 4340 Training Accuracy: 0.796875 Loss: 0.01014026626944542\n",
      "Iteration: 4350 Training Accuracy: 0.796875 Loss: 0.008327542804181576\n",
      "Iteration: 4360 Training Accuracy: 0.859375 Loss: 0.006730986759066582\n",
      "Iteration: 4370 Training Accuracy: 0.796875 Loss: 0.008794172666966915\n",
      "Iteration: 4380 Training Accuracy: 0.796875 Loss: 0.009304841980338097\n",
      "Iteration: 4390 Training Accuracy: 0.8125 Loss: 0.010715720243752003\n",
      "Iteration: 4400 Training Accuracy: 0.84375 Loss: 0.009996579959988594\n",
      "Iteration: 4410 Training Accuracy: 0.75 Loss: 0.010317247360944748\n",
      "Iteration: 4420 Training Accuracy: 0.734375 Loss: 0.014759417623281479\n",
      "Iteration: 4430 Training Accuracy: 0.78125 Loss: 0.00911801215261221\n",
      "Iteration: 4440 Training Accuracy: 0.734375 Loss: 0.013490783981978893\n",
      "Iteration: 4450 Training Accuracy: 0.921875 Loss: 0.005122768692672253\n",
      "Iteration: 4460 Training Accuracy: 0.84375 Loss: 0.006469239480793476\n",
      "Iteration: 4470 Training Accuracy: 0.859375 Loss: 0.009143233299255371\n",
      "Iteration: 4480 Training Accuracy: 0.75 Loss: 0.012413332238793373\n",
      "Iteration: 4490 Training Accuracy: 0.828125 Loss: 0.0071338750422000885\n",
      "Iteration: 4500 Training Accuracy: 0.859375 Loss: 0.009309028275310993\n",
      "Iteration: 4510 Training Accuracy: 0.765625 Loss: 0.011729737743735313\n",
      "Iteration: 4520 Training Accuracy: 0.8125 Loss: 0.009517883881926537\n",
      "Iteration: 4530 Training Accuracy: 0.765625 Loss: 0.011601057834923267\n",
      "Iteration: 4540 Training Accuracy: 0.828125 Loss: 0.010289897210896015\n",
      "Iteration: 4550 Training Accuracy: 0.78125 Loss: 0.00976644642651081\n",
      "Iteration: 4560 Training Accuracy: 0.734375 Loss: 0.012886643409729004\n",
      "Iteration: 4570 Training Accuracy: 0.796875 Loss: 0.009701879695057869\n",
      "Iteration: 4580 Training Accuracy: 0.828125 Loss: 0.006832947954535484\n",
      "Iteration: 4590 Training Accuracy: 0.828125 Loss: 0.0063840849325060844\n",
      "Iteration: 4600 Training Accuracy: 0.796875 Loss: 0.008005480282008648\n",
      "Iteration: 4610 Training Accuracy: 0.78125 Loss: 0.011443122290074825\n",
      "Iteration: 4620 Training Accuracy: 0.859375 Loss: 0.007247447967529297\n",
      "Iteration: 4630 Training Accuracy: 0.796875 Loss: 0.007813476957380772\n",
      "Iteration: 4640 Training Accuracy: 0.796875 Loss: 0.011068128049373627\n",
      "Iteration: 4650 Training Accuracy: 0.84375 Loss: 0.009216086007654667\n",
      "Iteration: 4660 Training Accuracy: 0.734375 Loss: 0.008778316900134087\n",
      "Iteration: 4670 Training Accuracy: 0.78125 Loss: 0.01116122305393219\n",
      "Iteration: 4680 Training Accuracy: 0.875 Loss: 0.007098394446074963\n",
      "Iteration: 4690 Training Accuracy: 0.78125 Loss: 0.012194430455565453\n",
      "Training Accuracy = 0.78125\n",
      "Validation Accuracy = 0\n",
      "epoch: 5\n",
      "Iteration: 4700 Training Accuracy: 0.796875 Loss: 0.010035453364253044\n",
      "Iteration: 4710 Training Accuracy: 0.859375 Loss: 0.006579637993127108\n",
      "Iteration: 4720 Training Accuracy: 0.8125 Loss: 0.007219675928354263\n",
      "Iteration: 4730 Training Accuracy: 0.796875 Loss: 0.009415867738425732\n",
      "Iteration: 4740 Training Accuracy: 0.703125 Loss: 0.013319907709956169\n",
      "Iteration: 4750 Training Accuracy: 0.796875 Loss: 0.009963161312043667\n",
      "Iteration: 4760 Training Accuracy: 0.875 Loss: 0.008117103949189186\n",
      "Iteration: 4770 Training Accuracy: 0.828125 Loss: 0.00993698462843895\n",
      "Iteration: 4780 Training Accuracy: 0.828125 Loss: 0.007952169515192509\n",
      "Iteration: 4790 Training Accuracy: 0.78125 Loss: 0.011650729924440384\n",
      "Iteration: 4800 Training Accuracy: 0.828125 Loss: 0.007893156260251999\n",
      "Iteration: 4810 Training Accuracy: 0.890625 Loss: 0.006632003001868725\n",
      "Iteration: 4820 Training Accuracy: 0.84375 Loss: 0.008634964935481548\n",
      "Iteration: 4830 Training Accuracy: 0.703125 Loss: 0.013006919994950294\n",
      "Iteration: 4840 Training Accuracy: 0.796875 Loss: 0.009508388116955757\n",
      "Iteration: 4850 Training Accuracy: 0.796875 Loss: 0.009344356134533882\n",
      "Iteration: 4860 Training Accuracy: 0.765625 Loss: 0.013071884401142597\n",
      "Iteration: 4870 Training Accuracy: 0.75 Loss: 0.009316554293036461\n",
      "Iteration: 4880 Training Accuracy: 0.8125 Loss: 0.008410567417740822\n",
      "Iteration: 4890 Training Accuracy: 0.828125 Loss: 0.00840754620730877\n",
      "Iteration: 4900 Training Accuracy: 0.78125 Loss: 0.009767059236764908\n",
      "Iteration: 4910 Training Accuracy: 0.8125 Loss: 0.013245901092886925\n",
      "Iteration: 4920 Training Accuracy: 0.828125 Loss: 0.008221322670578957\n",
      "Iteration: 4930 Training Accuracy: 0.765625 Loss: 0.011614575982093811\n",
      "Iteration: 4940 Training Accuracy: 0.84375 Loss: 0.007583296857774258\n",
      "Iteration: 4950 Training Accuracy: 0.84375 Loss: 0.008433829993009567\n",
      "Iteration: 4960 Training Accuracy: 0.828125 Loss: 0.0068422956392169\n",
      "Iteration: 4970 Training Accuracy: 0.71875 Loss: 0.012871538288891315\n",
      "Iteration: 4980 Training Accuracy: 0.8125 Loss: 0.008007261902093887\n",
      "Iteration: 4990 Training Accuracy: 0.828125 Loss: 0.009160512126982212\n",
      "Iteration: 5000 Training Accuracy: 0.75 Loss: 0.014922115951776505\n",
      "Iteration: 5010 Training Accuracy: 0.78125 Loss: 0.011286722496151924\n",
      "Iteration: 5020 Training Accuracy: 0.828125 Loss: 0.009567681699991226\n",
      "Iteration: 5030 Training Accuracy: 0.84375 Loss: 0.00836486741900444\n",
      "Iteration: 5040 Training Accuracy: 0.734375 Loss: 0.011046865954995155\n",
      "Iteration: 5050 Training Accuracy: 0.734375 Loss: 0.011334938928484917\n",
      "Iteration: 5060 Training Accuracy: 0.828125 Loss: 0.010232953354716301\n",
      "Iteration: 5070 Training Accuracy: 0.796875 Loss: 0.01072151679545641\n",
      "Iteration: 5080 Training Accuracy: 0.71875 Loss: 0.011198915541172028\n",
      "Iteration: 5090 Training Accuracy: 0.8125 Loss: 0.009980954229831696\n",
      "Iteration: 5100 Training Accuracy: 0.84375 Loss: 0.00884193554520607\n",
      "Iteration: 5110 Training Accuracy: 0.8125 Loss: 0.009827183559536934\n",
      "Iteration: 5120 Training Accuracy: 0.78125 Loss: 0.00964929349720478\n",
      "Iteration: 5130 Training Accuracy: 0.703125 Loss: 0.010972831398248672\n",
      "Iteration: 5140 Training Accuracy: 0.78125 Loss: 0.010940374806523323\n",
      "Iteration: 5150 Training Accuracy: 0.796875 Loss: 0.008137336932122707\n",
      "Iteration: 5160 Training Accuracy: 0.875 Loss: 0.007731424644589424\n",
      "Iteration: 5170 Training Accuracy: 0.8125 Loss: 0.007431063801050186\n",
      "Iteration: 5180 Training Accuracy: 0.8125 Loss: 0.008756138384342194\n",
      "Iteration: 5190 Training Accuracy: 0.828125 Loss: 0.00938873179256916\n",
      "Iteration: 5200 Training Accuracy: 0.875 Loss: 0.007435343228280544\n",
      "Iteration: 5210 Training Accuracy: 0.828125 Loss: 0.00798884779214859\n",
      "Iteration: 5220 Training Accuracy: 0.84375 Loss: 0.007189466618001461\n",
      "Iteration: 5230 Training Accuracy: 0.828125 Loss: 0.007711359765380621\n",
      "Iteration: 5240 Training Accuracy: 0.8125 Loss: 0.010508104227483273\n",
      "Iteration: 5250 Training Accuracy: 0.828125 Loss: 0.007930402643978596\n",
      "Iteration: 5260 Training Accuracy: 0.765625 Loss: 0.009747311472892761\n",
      "Iteration: 5270 Training Accuracy: 0.71875 Loss: 0.01326778158545494\n",
      "Iteration: 5280 Training Accuracy: 0.9375 Loss: 0.004199401941150427\n",
      "Iteration: 5290 Training Accuracy: 0.78125 Loss: 0.011409273371100426\n",
      "Iteration: 5300 Training Accuracy: 0.8125 Loss: 0.008377743884921074\n",
      "Iteration: 5310 Training Accuracy: 0.765625 Loss: 0.012020252645015717\n",
      "Iteration: 5320 Training Accuracy: 0.828125 Loss: 0.008583031594753265\n",
      "Iteration: 5330 Training Accuracy: 0.78125 Loss: 0.01203388161957264\n",
      "Iteration: 5340 Training Accuracy: 0.859375 Loss: 0.007481599226593971\n",
      "Iteration: 5350 Training Accuracy: 0.875 Loss: 0.0063543482683598995\n",
      "Iteration: 5360 Training Accuracy: 0.875 Loss: 0.006932959891855717\n",
      "Iteration: 5370 Training Accuracy: 0.78125 Loss: 0.010328935459256172\n",
      "Iteration: 5380 Training Accuracy: 0.84375 Loss: 0.008047270588576794\n",
      "Iteration: 5390 Training Accuracy: 0.765625 Loss: 0.008444955572485924\n",
      "Iteration: 5400 Training Accuracy: 0.828125 Loss: 0.0079114381223917\n",
      "Iteration: 5410 Training Accuracy: 0.828125 Loss: 0.008933894336223602\n",
      "Iteration: 5420 Training Accuracy: 0.796875 Loss: 0.010396622121334076\n",
      "Iteration: 5430 Training Accuracy: 0.78125 Loss: 0.01070243027061224\n",
      "Iteration: 5440 Training Accuracy: 0.796875 Loss: 0.010692025534808636\n",
      "Iteration: 5450 Training Accuracy: 0.8125 Loss: 0.009882664307951927\n",
      "Iteration: 5460 Training Accuracy: 0.828125 Loss: 0.008467592298984528\n",
      "Iteration: 5470 Training Accuracy: 0.796875 Loss: 0.00894252397119999\n",
      "Iteration: 5480 Training Accuracy: 0.84375 Loss: 0.008888944052159786\n",
      "Iteration: 5490 Training Accuracy: 0.734375 Loss: 0.012534377165138721\n",
      "Iteration: 5500 Training Accuracy: 0.734375 Loss: 0.010754001326858997\n",
      "Iteration: 5510 Training Accuracy: 0.859375 Loss: 0.007571727968752384\n",
      "Iteration: 5520 Training Accuracy: 0.796875 Loss: 0.008744108490645885\n",
      "Iteration: 5530 Training Accuracy: 0.859375 Loss: 0.006610900163650513\n",
      "Iteration: 5540 Training Accuracy: 0.84375 Loss: 0.009349200874567032\n",
      "Iteration: 5550 Training Accuracy: 0.84375 Loss: 0.005897825583815575\n",
      "Iteration: 5560 Training Accuracy: 0.859375 Loss: 0.007437585853040218\n",
      "Iteration: 5570 Training Accuracy: 0.859375 Loss: 0.008325095288455486\n",
      "Iteration: 5580 Training Accuracy: 0.859375 Loss: 0.005704541224986315\n",
      "Iteration: 5590 Training Accuracy: 0.859375 Loss: 0.00477256067097187\n",
      "Iteration: 5600 Training Accuracy: 0.859375 Loss: 0.008402654901146889\n",
      "Iteration: 5610 Training Accuracy: 0.859375 Loss: 0.0068375468254089355\n",
      "Iteration: 5620 Training Accuracy: 0.875 Loss: 0.007695757783949375\n",
      "Training Accuracy = 0.8125\n",
      "Validation Accuracy = 0\n",
      "epoch: 6\n",
      "Iteration: 5630 Training Accuracy: 0.75 Loss: 0.01209309883415699\n",
      "Iteration: 5640 Training Accuracy: 0.765625 Loss: 0.013146137818694115\n",
      "Iteration: 5650 Training Accuracy: 0.796875 Loss: 0.009329691529273987\n",
      "Iteration: 5660 Training Accuracy: 0.796875 Loss: 0.009065866470336914\n",
      "Iteration: 5670 Training Accuracy: 0.828125 Loss: 0.008810460567474365\n",
      "Iteration: 5680 Training Accuracy: 0.875 Loss: 0.005684012547135353\n",
      "Iteration: 5690 Training Accuracy: 0.921875 Loss: 0.0054230596870183945\n",
      "Iteration: 5700 Training Accuracy: 0.8125 Loss: 0.006763235665857792\n",
      "Iteration: 5710 Training Accuracy: 0.953125 Loss: 0.004458681680262089\n",
      "Iteration: 5720 Training Accuracy: 0.828125 Loss: 0.007911663502454758\n",
      "Iteration: 5730 Training Accuracy: 0.734375 Loss: 0.01105319894850254\n",
      "Iteration: 5740 Training Accuracy: 0.78125 Loss: 0.009153854101896286\n",
      "Iteration: 5750 Training Accuracy: 0.8125 Loss: 0.008636482059955597\n",
      "Iteration: 5760 Training Accuracy: 0.8125 Loss: 0.008583315648138523\n",
      "Iteration: 5770 Training Accuracy: 0.796875 Loss: 0.010597378015518188\n",
      "Iteration: 5780 Training Accuracy: 0.8125 Loss: 0.0090772844851017\n",
      "Iteration: 5790 Training Accuracy: 0.90625 Loss: 0.00669363047927618\n",
      "Iteration: 5800 Training Accuracy: 0.8125 Loss: 0.010017629712820053\n",
      "Iteration: 5810 Training Accuracy: 0.859375 Loss: 0.007234268356114626\n",
      "Iteration: 5820 Training Accuracy: 0.90625 Loss: 0.006182172801345587\n",
      "Iteration: 5830 Training Accuracy: 0.890625 Loss: 0.00615828949958086\n",
      "Iteration: 5840 Training Accuracy: 0.78125 Loss: 0.01018708199262619\n",
      "Iteration: 5850 Training Accuracy: 0.734375 Loss: 0.01117799337953329\n",
      "Iteration: 5860 Training Accuracy: 0.84375 Loss: 0.008797405287623405\n",
      "Iteration: 5870 Training Accuracy: 0.8125 Loss: 0.007483834400773048\n",
      "Iteration: 5880 Training Accuracy: 0.828125 Loss: 0.00997832790017128\n",
      "Iteration: 5890 Training Accuracy: 0.859375 Loss: 0.0076097650453448296\n",
      "Iteration: 5900 Training Accuracy: 0.796875 Loss: 0.012336752377450466\n",
      "Iteration: 5910 Training Accuracy: 0.828125 Loss: 0.008222654461860657\n",
      "Iteration: 5920 Training Accuracy: 0.859375 Loss: 0.007657208479940891\n",
      "Iteration: 5930 Training Accuracy: 0.859375 Loss: 0.006680775433778763\n",
      "Iteration: 5940 Training Accuracy: 0.78125 Loss: 0.009959697723388672\n",
      "Iteration: 5950 Training Accuracy: 0.84375 Loss: 0.010466131381690502\n",
      "Iteration: 5960 Training Accuracy: 0.859375 Loss: 0.006127378903329372\n",
      "Iteration: 5970 Training Accuracy: 0.875 Loss: 0.0068494947627186775\n",
      "Iteration: 5980 Training Accuracy: 0.84375 Loss: 0.006579124368727207\n",
      "Iteration: 5990 Training Accuracy: 0.8125 Loss: 0.009046848863363266\n",
      "Iteration: 6000 Training Accuracy: 0.765625 Loss: 0.009440813213586807\n",
      "Iteration: 6010 Training Accuracy: 0.78125 Loss: 0.01083054207265377\n",
      "Iteration: 6020 Training Accuracy: 0.765625 Loss: 0.008996311575174332\n",
      "Iteration: 6030 Training Accuracy: 0.8125 Loss: 0.01036139763891697\n",
      "Iteration: 6040 Training Accuracy: 0.6875 Loss: 0.013681802898645401\n",
      "Iteration: 6050 Training Accuracy: 0.8125 Loss: 0.008741213008761406\n",
      "Iteration: 6060 Training Accuracy: 0.734375 Loss: 0.010494736954569817\n",
      "Iteration: 6070 Training Accuracy: 0.90625 Loss: 0.005340720526874065\n",
      "Iteration: 6080 Training Accuracy: 0.8125 Loss: 0.010398251004517078\n",
      "Iteration: 6090 Training Accuracy: 0.8125 Loss: 0.008671285584568977\n",
      "Iteration: 6100 Training Accuracy: 0.875 Loss: 0.006617126055061817\n",
      "Iteration: 6110 Training Accuracy: 0.765625 Loss: 0.009470259770751\n",
      "Iteration: 6120 Training Accuracy: 0.875 Loss: 0.006136028561741114\n",
      "Iteration: 6130 Training Accuracy: 0.890625 Loss: 0.008498935028910637\n",
      "Iteration: 6140 Training Accuracy: 0.75 Loss: 0.012359384447336197\n",
      "Iteration: 6150 Training Accuracy: 0.828125 Loss: 0.007949929684400558\n",
      "Iteration: 6160 Training Accuracy: 0.8125 Loss: 0.0076128835789859295\n",
      "Iteration: 6170 Training Accuracy: 0.796875 Loss: 0.010387352667748928\n",
      "Iteration: 6180 Training Accuracy: 0.8125 Loss: 0.0080151641741395\n",
      "Iteration: 6190 Training Accuracy: 0.8125 Loss: 0.006725528743118048\n",
      "Iteration: 6200 Training Accuracy: 0.71875 Loss: 0.009924831800162792\n",
      "Iteration: 6210 Training Accuracy: 0.828125 Loss: 0.008746236562728882\n",
      "Iteration: 6220 Training Accuracy: 0.8125 Loss: 0.009024347178637981\n",
      "Iteration: 6230 Training Accuracy: 0.828125 Loss: 0.007077218033373356\n",
      "Iteration: 6240 Training Accuracy: 0.765625 Loss: 0.01021723635494709\n",
      "Iteration: 6250 Training Accuracy: 0.859375 Loss: 0.006753729656338692\n",
      "Iteration: 6260 Training Accuracy: 0.859375 Loss: 0.007479802705347538\n",
      "Iteration: 6270 Training Accuracy: 0.890625 Loss: 0.006823879666626453\n",
      "Iteration: 6280 Training Accuracy: 0.8125 Loss: 0.008977248333394527\n",
      "Iteration: 6290 Training Accuracy: 0.84375 Loss: 0.008812131360173225\n",
      "Iteration: 6300 Training Accuracy: 0.796875 Loss: 0.007756701670587063\n",
      "Iteration: 6310 Training Accuracy: 0.859375 Loss: 0.00797516293823719\n",
      "Iteration: 6320 Training Accuracy: 0.859375 Loss: 0.007825705222785473\n",
      "Iteration: 6330 Training Accuracy: 0.828125 Loss: 0.010130597278475761\n",
      "Iteration: 6340 Training Accuracy: 0.890625 Loss: 0.006264734081923962\n",
      "Iteration: 6350 Training Accuracy: 0.859375 Loss: 0.0050822412595152855\n",
      "Iteration: 6360 Training Accuracy: 0.796875 Loss: 0.010350121185183525\n",
      "Iteration: 6370 Training Accuracy: 0.859375 Loss: 0.005926830694079399\n",
      "Iteration: 6380 Training Accuracy: 0.8125 Loss: 0.00892406702041626\n",
      "Iteration: 6390 Training Accuracy: 0.8125 Loss: 0.007397455163300037\n",
      "Iteration: 6400 Training Accuracy: 0.84375 Loss: 0.007502837106585503\n",
      "Iteration: 6410 Training Accuracy: 0.796875 Loss: 0.008122408762574196\n",
      "Iteration: 6420 Training Accuracy: 0.796875 Loss: 0.00837208516895771\n",
      "Iteration: 6430 Training Accuracy: 0.828125 Loss: 0.00885709747672081\n",
      "Iteration: 6440 Training Accuracy: 0.765625 Loss: 0.011144561693072319\n",
      "Iteration: 6450 Training Accuracy: 0.828125 Loss: 0.008745977655053139\n",
      "Iteration: 6460 Training Accuracy: 0.84375 Loss: 0.008349450305104256\n",
      "Iteration: 6470 Training Accuracy: 0.78125 Loss: 0.011818398721516132\n",
      "Iteration: 6480 Training Accuracy: 0.765625 Loss: 0.012078605592250824\n",
      "Iteration: 6490 Training Accuracy: 0.8125 Loss: 0.007920948788523674\n",
      "Iteration: 6500 Training Accuracy: 0.84375 Loss: 0.006533877924084663\n",
      "Iteration: 6510 Training Accuracy: 0.859375 Loss: 0.0066120680421590805\n",
      "Iteration: 6520 Training Accuracy: 0.8125 Loss: 0.009404812008142471\n",
      "Iteration: 6530 Training Accuracy: 0.921875 Loss: 0.00455764215439558\n",
      "Iteration: 6540 Training Accuracy: 0.734375 Loss: 0.012752754613757133\n",
      "Iteration: 6550 Training Accuracy: 0.890625 Loss: 0.007485850248485804\n",
      "Iteration: 6560 Training Accuracy: 0.765625 Loss: 0.01025210227817297\n",
      "Training Accuracy = 0.6875\n",
      "Validation Accuracy = 0\n",
      "epoch: 7\n",
      "Iteration: 6570 Training Accuracy: 0.71875 Loss: 0.012544429861009121\n",
      "Iteration: 6580 Training Accuracy: 0.84375 Loss: 0.006417080760002136\n",
      "Iteration: 6590 Training Accuracy: 0.828125 Loss: 0.00792758259922266\n",
      "Iteration: 6600 Training Accuracy: 0.875 Loss: 0.007808858063071966\n",
      "Iteration: 6610 Training Accuracy: 0.875 Loss: 0.00782678835093975\n",
      "Iteration: 6620 Training Accuracy: 0.78125 Loss: 0.01036171056330204\n",
      "Iteration: 6630 Training Accuracy: 0.8125 Loss: 0.00814751349389553\n",
      "Iteration: 6640 Training Accuracy: 0.765625 Loss: 0.011809407733380795\n",
      "Iteration: 6650 Training Accuracy: 0.921875 Loss: 0.004574405960738659\n",
      "Iteration: 6660 Training Accuracy: 0.8125 Loss: 0.0077445898205041885\n",
      "Iteration: 6670 Training Accuracy: 0.875 Loss: 0.007854300551116467\n",
      "Iteration: 6680 Training Accuracy: 0.859375 Loss: 0.006718623451888561\n",
      "Iteration: 6690 Training Accuracy: 0.8125 Loss: 0.007994004525244236\n",
      "Iteration: 6700 Training Accuracy: 0.796875 Loss: 0.0115163279697299\n",
      "Iteration: 6710 Training Accuracy: 0.828125 Loss: 0.007454833015799522\n",
      "Iteration: 6720 Training Accuracy: 0.84375 Loss: 0.008244898170232773\n",
      "Iteration: 6730 Training Accuracy: 0.8125 Loss: 0.008261779323220253\n",
      "Iteration: 6740 Training Accuracy: 0.84375 Loss: 0.007504629902541637\n",
      "Iteration: 6750 Training Accuracy: 0.796875 Loss: 0.009875943884253502\n",
      "Iteration: 6760 Training Accuracy: 0.75 Loss: 0.01038145087659359\n",
      "Iteration: 6770 Training Accuracy: 0.875 Loss: 0.0073946258053183556\n",
      "Iteration: 6780 Training Accuracy: 0.828125 Loss: 0.007999015972018242\n",
      "Iteration: 6790 Training Accuracy: 0.828125 Loss: 0.007296840660274029\n",
      "Iteration: 6800 Training Accuracy: 0.8125 Loss: 0.010102233849465847\n",
      "Iteration: 6810 Training Accuracy: 0.765625 Loss: 0.008696609176695347\n",
      "Iteration: 6820 Training Accuracy: 0.828125 Loss: 0.0074008312076330185\n",
      "Iteration: 6830 Training Accuracy: 0.84375 Loss: 0.0062603214755654335\n",
      "Iteration: 6840 Training Accuracy: 0.890625 Loss: 0.0069205062463879585\n",
      "Iteration: 6850 Training Accuracy: 0.84375 Loss: 0.006633252836763859\n",
      "Iteration: 6860 Training Accuracy: 0.6875 Loss: 0.015325543470680714\n",
      "Iteration: 6870 Training Accuracy: 0.875 Loss: 0.00535567756742239\n",
      "Iteration: 6880 Training Accuracy: 0.921875 Loss: 0.006299040745943785\n",
      "Iteration: 6890 Training Accuracy: 0.828125 Loss: 0.00917541142553091\n",
      "Iteration: 6900 Training Accuracy: 0.828125 Loss: 0.00858787540346384\n",
      "Iteration: 6910 Training Accuracy: 0.8125 Loss: 0.009062347002327442\n",
      "Iteration: 6920 Training Accuracy: 0.875 Loss: 0.00559614785015583\n",
      "Iteration: 6930 Training Accuracy: 0.859375 Loss: 0.007571663241833448\n",
      "Iteration: 6940 Training Accuracy: 0.8125 Loss: 0.007502332329750061\n",
      "Iteration: 6950 Training Accuracy: 0.84375 Loss: 0.010420434176921844\n",
      "Iteration: 6960 Training Accuracy: 0.828125 Loss: 0.008714227005839348\n",
      "Iteration: 6970 Training Accuracy: 0.84375 Loss: 0.007053159177303314\n",
      "Iteration: 6980 Training Accuracy: 0.90625 Loss: 0.0067504895851016045\n",
      "Iteration: 6990 Training Accuracy: 0.8125 Loss: 0.007363536395132542\n",
      "Iteration: 7000 Training Accuracy: 0.890625 Loss: 0.006201299838721752\n",
      "Iteration: 7010 Training Accuracy: 0.828125 Loss: 0.0072835772298276424\n",
      "Iteration: 7020 Training Accuracy: 0.859375 Loss: 0.00701089296489954\n",
      "Iteration: 7030 Training Accuracy: 0.84375 Loss: 0.007017315365374088\n",
      "Iteration: 7040 Training Accuracy: 0.859375 Loss: 0.009443026036024094\n",
      "Iteration: 7050 Training Accuracy: 0.796875 Loss: 0.007899689488112926\n",
      "Iteration: 7060 Training Accuracy: 0.84375 Loss: 0.008932234719395638\n",
      "Iteration: 7070 Training Accuracy: 0.8125 Loss: 0.00912080891430378\n",
      "Iteration: 7080 Training Accuracy: 0.796875 Loss: 0.010471087880432606\n",
      "Iteration: 7090 Training Accuracy: 0.84375 Loss: 0.009025049395859241\n",
      "Iteration: 7100 Training Accuracy: 0.765625 Loss: 0.01141544058918953\n",
      "Iteration: 7110 Training Accuracy: 0.75 Loss: 0.012613233178853989\n",
      "Iteration: 7120 Training Accuracy: 0.890625 Loss: 0.0050604213029146194\n",
      "Iteration: 7130 Training Accuracy: 0.8125 Loss: 0.007949886843562126\n",
      "Iteration: 7140 Training Accuracy: 0.859375 Loss: 0.006463975179940462\n",
      "Iteration: 7150 Training Accuracy: 0.90625 Loss: 0.004863161593675613\n",
      "Iteration: 7160 Training Accuracy: 0.84375 Loss: 0.007355391979217529\n",
      "Iteration: 7170 Training Accuracy: 0.78125 Loss: 0.011738676577806473\n",
      "Iteration: 7180 Training Accuracy: 0.875 Loss: 0.007130901794880629\n",
      "Iteration: 7190 Training Accuracy: 0.71875 Loss: 0.01582890935242176\n",
      "Iteration: 7200 Training Accuracy: 0.890625 Loss: 0.005951796658337116\n",
      "Iteration: 7210 Training Accuracy: 0.921875 Loss: 0.0063639916479587555\n",
      "Iteration: 7220 Training Accuracy: 0.75 Loss: 0.0076163411140441895\n",
      "Iteration: 7230 Training Accuracy: 0.8125 Loss: 0.010533679276704788\n",
      "Iteration: 7240 Training Accuracy: 0.9375 Loss: 0.004619831219315529\n",
      "Iteration: 7250 Training Accuracy: 0.890625 Loss: 0.006361485458910465\n",
      "Iteration: 7260 Training Accuracy: 0.90625 Loss: 0.0072155059315264225\n",
      "Iteration: 7270 Training Accuracy: 0.796875 Loss: 0.011334897950291634\n",
      "Iteration: 7280 Training Accuracy: 0.921875 Loss: 0.00567250233143568\n",
      "Iteration: 7290 Training Accuracy: 0.84375 Loss: 0.007342023309320211\n",
      "Iteration: 7300 Training Accuracy: 0.8125 Loss: 0.009328623302280903\n",
      "Iteration: 7310 Training Accuracy: 0.8125 Loss: 0.008570334874093533\n",
      "Iteration: 7320 Training Accuracy: 0.859375 Loss: 0.007797371596097946\n",
      "Iteration: 7330 Training Accuracy: 0.78125 Loss: 0.010342828929424286\n",
      "Iteration: 7340 Training Accuracy: 0.78125 Loss: 0.008728718385100365\n",
      "Iteration: 7350 Training Accuracy: 0.796875 Loss: 0.009806981310248375\n",
      "Iteration: 7360 Training Accuracy: 0.84375 Loss: 0.0068765925243496895\n",
      "Iteration: 7370 Training Accuracy: 0.75 Loss: 0.012240464799106121\n",
      "Iteration: 7380 Training Accuracy: 0.8125 Loss: 0.007855739444494247\n",
      "Iteration: 7390 Training Accuracy: 0.859375 Loss: 0.006013743579387665\n",
      "Iteration: 7400 Training Accuracy: 0.734375 Loss: 0.01097444910556078\n",
      "Iteration: 7410 Training Accuracy: 0.921875 Loss: 0.005883399397134781\n",
      "Iteration: 7420 Training Accuracy: 0.6875 Loss: 0.014938628301024437\n",
      "Iteration: 7430 Training Accuracy: 0.84375 Loss: 0.010576986707746983\n",
      "Iteration: 7440 Training Accuracy: 0.828125 Loss: 0.009476987645030022\n",
      "Iteration: 7450 Training Accuracy: 0.859375 Loss: 0.007941406220197678\n",
      "Iteration: 7460 Training Accuracy: 0.84375 Loss: 0.008947296068072319\n",
      "Iteration: 7470 Training Accuracy: 0.859375 Loss: 0.007116678170859814\n",
      "Iteration: 7480 Training Accuracy: 0.84375 Loss: 0.008141320198774338\n",
      "Iteration: 7490 Training Accuracy: 0.796875 Loss: 0.00971386581659317\n",
      "Iteration: 7500 Training Accuracy: 0.859375 Loss: 0.00589609332382679\n",
      "Training Accuracy = 0.78125\n",
      "Validation Accuracy = 0\n",
      "epoch: 8\n",
      "Iteration: 7510 Training Accuracy: 0.84375 Loss: 0.007522949483245611\n",
      "Iteration: 7520 Training Accuracy: 0.84375 Loss: 0.007991831749677658\n",
      "Iteration: 7530 Training Accuracy: 0.875 Loss: 0.00736479926854372\n",
      "Iteration: 7540 Training Accuracy: 0.875 Loss: 0.007271548267453909\n",
      "Iteration: 7550 Training Accuracy: 0.859375 Loss: 0.005923332180827856\n",
      "Iteration: 7560 Training Accuracy: 0.84375 Loss: 0.008061273023486137\n",
      "Iteration: 7570 Training Accuracy: 0.796875 Loss: 0.010521626099944115\n",
      "Iteration: 7580 Training Accuracy: 0.828125 Loss: 0.007920969277620316\n",
      "Iteration: 7590 Training Accuracy: 0.828125 Loss: 0.011019062250852585\n",
      "Iteration: 7600 Training Accuracy: 0.875 Loss: 0.006631313823163509\n",
      "Iteration: 7610 Training Accuracy: 0.75 Loss: 0.009311942383646965\n",
      "Iteration: 7620 Training Accuracy: 0.859375 Loss: 0.006235093344002962\n",
      "Iteration: 7630 Training Accuracy: 0.828125 Loss: 0.0068405140191316605\n",
      "Iteration: 7640 Training Accuracy: 0.859375 Loss: 0.0060358308255672455\n",
      "Iteration: 7650 Training Accuracy: 0.84375 Loss: 0.008532637730240822\n",
      "Iteration: 7660 Training Accuracy: 0.734375 Loss: 0.013405850157141685\n",
      "Iteration: 7670 Training Accuracy: 0.84375 Loss: 0.009597462601959705\n",
      "Iteration: 7680 Training Accuracy: 0.875 Loss: 0.006619804073125124\n",
      "Iteration: 7690 Training Accuracy: 0.921875 Loss: 0.004895997233688831\n",
      "Iteration: 7700 Training Accuracy: 0.828125 Loss: 0.007670870050787926\n",
      "Iteration: 7710 Training Accuracy: 0.828125 Loss: 0.007959302514791489\n",
      "Iteration: 7720 Training Accuracy: 0.765625 Loss: 0.009569672867655754\n",
      "Iteration: 7730 Training Accuracy: 0.875 Loss: 0.006423247512429953\n",
      "Iteration: 7740 Training Accuracy: 0.828125 Loss: 0.007436650339514017\n",
      "Iteration: 7750 Training Accuracy: 0.828125 Loss: 0.009407458826899529\n",
      "Iteration: 7760 Training Accuracy: 0.828125 Loss: 0.005558556877076626\n",
      "Iteration: 7770 Training Accuracy: 0.90625 Loss: 0.006150121800601482\n",
      "Iteration: 7780 Training Accuracy: 0.859375 Loss: 0.007708657067269087\n",
      "Iteration: 7790 Training Accuracy: 0.859375 Loss: 0.005563963204622269\n",
      "Iteration: 7800 Training Accuracy: 0.75 Loss: 0.011629577726125717\n",
      "Iteration: 7810 Training Accuracy: 0.875 Loss: 0.00554873701184988\n",
      "Iteration: 7820 Training Accuracy: 0.78125 Loss: 0.009526582434773445\n",
      "Iteration: 7830 Training Accuracy: 0.890625 Loss: 0.005462829954922199\n",
      "Iteration: 7840 Training Accuracy: 0.8125 Loss: 0.01248922012746334\n",
      "Iteration: 7850 Training Accuracy: 0.90625 Loss: 0.004692643880844116\n",
      "Iteration: 7860 Training Accuracy: 0.859375 Loss: 0.006027987226843834\n",
      "Iteration: 7870 Training Accuracy: 0.890625 Loss: 0.004905359819531441\n",
      "Iteration: 7880 Training Accuracy: 0.828125 Loss: 0.00834463257342577\n",
      "Iteration: 7890 Training Accuracy: 0.734375 Loss: 0.012680410407483578\n",
      "Iteration: 7900 Training Accuracy: 0.890625 Loss: 0.0064940061420202255\n",
      "Iteration: 7910 Training Accuracy: 0.890625 Loss: 0.006681783124804497\n",
      "Iteration: 7920 Training Accuracy: 0.78125 Loss: 0.00890842080116272\n",
      "Iteration: 7930 Training Accuracy: 0.84375 Loss: 0.0073093450628221035\n",
      "Iteration: 7940 Training Accuracy: 0.875 Loss: 0.005849701352417469\n",
      "Iteration: 7950 Training Accuracy: 0.828125 Loss: 0.00932101160287857\n",
      "Iteration: 7960 Training Accuracy: 0.890625 Loss: 0.006313113495707512\n",
      "Iteration: 7970 Training Accuracy: 0.859375 Loss: 0.007062262389808893\n",
      "Iteration: 7980 Training Accuracy: 0.859375 Loss: 0.006200512871146202\n",
      "Iteration: 7990 Training Accuracy: 0.890625 Loss: 0.0056315818801522255\n",
      "Iteration: 8000 Training Accuracy: 0.890625 Loss: 0.0056925807148218155\n",
      "Iteration: 8010 Training Accuracy: 0.828125 Loss: 0.010495987720787525\n",
      "Iteration: 8020 Training Accuracy: 0.859375 Loss: 0.0052538770250976086\n",
      "Iteration: 8030 Training Accuracy: 0.796875 Loss: 0.009130819700658321\n",
      "Iteration: 8040 Training Accuracy: 0.84375 Loss: 0.007786722853779793\n",
      "Iteration: 8050 Training Accuracy: 0.828125 Loss: 0.008892986923456192\n",
      "Iteration: 8060 Training Accuracy: 0.890625 Loss: 0.006211156491190195\n",
      "Iteration: 8070 Training Accuracy: 0.875 Loss: 0.0052458117716014385\n",
      "Iteration: 8080 Training Accuracy: 0.84375 Loss: 0.006017147563397884\n",
      "Iteration: 8090 Training Accuracy: 0.875 Loss: 0.005813624709844589\n",
      "Iteration: 8100 Training Accuracy: 0.84375 Loss: 0.008973296731710434\n",
      "Iteration: 8110 Training Accuracy: 0.9375 Loss: 0.005188877694308758\n",
      "Iteration: 8120 Training Accuracy: 0.875 Loss: 0.006247521378099918\n",
      "Iteration: 8130 Training Accuracy: 0.8125 Loss: 0.007402168586850166\n",
      "Iteration: 8140 Training Accuracy: 0.8125 Loss: 0.008081359788775444\n",
      "Iteration: 8150 Training Accuracy: 0.84375 Loss: 0.007836535573005676\n",
      "Iteration: 8160 Training Accuracy: 0.859375 Loss: 0.006815733853727579\n",
      "Iteration: 8170 Training Accuracy: 0.875 Loss: 0.00565602071583271\n",
      "Iteration: 8180 Training Accuracy: 0.875 Loss: 0.006669130176305771\n",
      "Iteration: 8190 Training Accuracy: 0.796875 Loss: 0.011288268491625786\n",
      "Iteration: 8200 Training Accuracy: 0.8125 Loss: 0.006951158866286278\n",
      "Iteration: 8210 Training Accuracy: 0.84375 Loss: 0.006282895803451538\n",
      "Iteration: 8220 Training Accuracy: 0.859375 Loss: 0.0095154894515872\n",
      "Iteration: 8230 Training Accuracy: 0.90625 Loss: 0.004896235652267933\n",
      "Iteration: 8240 Training Accuracy: 0.828125 Loss: 0.009854943491518497\n",
      "Iteration: 8250 Training Accuracy: 0.875 Loss: 0.007549930363893509\n",
      "Iteration: 8260 Training Accuracy: 0.796875 Loss: 0.008734695613384247\n",
      "Iteration: 8270 Training Accuracy: 0.859375 Loss: 0.006928290706127882\n",
      "Iteration: 8280 Training Accuracy: 0.828125 Loss: 0.008433316834270954\n",
      "Iteration: 8290 Training Accuracy: 0.875 Loss: 0.008339445106685162\n",
      "Iteration: 8300 Training Accuracy: 0.875 Loss: 0.007084147538989782\n",
      "Iteration: 8310 Training Accuracy: 0.84375 Loss: 0.006188433151692152\n",
      "Iteration: 8320 Training Accuracy: 0.828125 Loss: 0.00810767151415348\n",
      "Iteration: 8330 Training Accuracy: 0.78125 Loss: 0.011402368545532227\n",
      "Iteration: 8340 Training Accuracy: 0.875 Loss: 0.006384559907019138\n",
      "Iteration: 8350 Training Accuracy: 0.84375 Loss: 0.007045607082545757\n",
      "Iteration: 8360 Training Accuracy: 0.78125 Loss: 0.010447345674037933\n",
      "Iteration: 8370 Training Accuracy: 0.84375 Loss: 0.007231090217828751\n",
      "Iteration: 8380 Training Accuracy: 0.828125 Loss: 0.009353982284665108\n",
      "Iteration: 8390 Training Accuracy: 0.734375 Loss: 0.010469790548086166\n",
      "Iteration: 8400 Training Accuracy: 0.859375 Loss: 0.005560525692999363\n",
      "Iteration: 8410 Training Accuracy: 0.8125 Loss: 0.0104160625487566\n",
      "Iteration: 8420 Training Accuracy: 0.875 Loss: 0.008425164967775345\n",
      "Iteration: 8430 Training Accuracy: 0.796875 Loss: 0.008842870593070984\n",
      "Iteration: 8440 Training Accuracy: 0.859375 Loss: 0.005519458558410406\n",
      "Training Accuracy = 0.8125\n",
      "Validation Accuracy = 0\n",
      "epoch: 9\n",
      "Iteration: 8450 Training Accuracy: 0.84375 Loss: 0.0070200953632593155\n",
      "Iteration: 8460 Training Accuracy: 0.90625 Loss: 0.004273255355656147\n",
      "Iteration: 8470 Training Accuracy: 0.8125 Loss: 0.008070481941103935\n",
      "Iteration: 8480 Training Accuracy: 0.765625 Loss: 0.01139861810952425\n",
      "Iteration: 8490 Training Accuracy: 0.84375 Loss: 0.009213827550411224\n",
      "Iteration: 8500 Training Accuracy: 0.84375 Loss: 0.00898925680667162\n",
      "Iteration: 8510 Training Accuracy: 0.84375 Loss: 0.009256690740585327\n",
      "Iteration: 8520 Training Accuracy: 0.859375 Loss: 0.007086200639605522\n",
      "Iteration: 8530 Training Accuracy: 0.828125 Loss: 0.007180945016443729\n",
      "Iteration: 8540 Training Accuracy: 0.890625 Loss: 0.006138266995549202\n",
      "Iteration: 8550 Training Accuracy: 0.8125 Loss: 0.00851995125412941\n",
      "Iteration: 8560 Training Accuracy: 0.65625 Loss: 0.01249530166387558\n",
      "Iteration: 8570 Training Accuracy: 0.90625 Loss: 0.00643573421984911\n",
      "Iteration: 8580 Training Accuracy: 0.84375 Loss: 0.009212397038936615\n",
      "Iteration: 8590 Training Accuracy: 0.828125 Loss: 0.007789814379066229\n",
      "Iteration: 8600 Training Accuracy: 0.90625 Loss: 0.005304298363626003\n",
      "Iteration: 8610 Training Accuracy: 0.8125 Loss: 0.007507422007620335\n",
      "Iteration: 8620 Training Accuracy: 0.859375 Loss: 0.007972104474902153\n",
      "Iteration: 8630 Training Accuracy: 0.875 Loss: 0.007410641293972731\n",
      "Iteration: 8640 Training Accuracy: 0.90625 Loss: 0.004973023198544979\n",
      "Iteration: 8650 Training Accuracy: 0.8125 Loss: 0.01021544449031353\n",
      "Iteration: 8660 Training Accuracy: 0.921875 Loss: 0.007501658983528614\n",
      "Iteration: 8670 Training Accuracy: 0.90625 Loss: 0.004934320226311684\n",
      "Iteration: 8680 Training Accuracy: 0.765625 Loss: 0.008843082003295422\n",
      "Iteration: 8690 Training Accuracy: 0.8125 Loss: 0.00797311495989561\n",
      "Iteration: 8700 Training Accuracy: 0.765625 Loss: 0.0126182921230793\n",
      "Iteration: 8710 Training Accuracy: 0.828125 Loss: 0.009610123932361603\n",
      "Iteration: 8720 Training Accuracy: 0.875 Loss: 0.007171512581408024\n",
      "Iteration: 8730 Training Accuracy: 0.875 Loss: 0.0066821156069636345\n",
      "Iteration: 8740 Training Accuracy: 0.859375 Loss: 0.0053657786920666695\n",
      "Iteration: 8750 Training Accuracy: 0.828125 Loss: 0.0073662446811795235\n",
      "Iteration: 8760 Training Accuracy: 0.8125 Loss: 0.007714071776717901\n",
      "Iteration: 8770 Training Accuracy: 0.875 Loss: 0.006027002818882465\n",
      "Iteration: 8780 Training Accuracy: 0.84375 Loss: 0.006094885058701038\n",
      "Iteration: 8790 Training Accuracy: 0.796875 Loss: 0.007635465823113918\n",
      "Iteration: 8800 Training Accuracy: 0.8125 Loss: 0.008491436950862408\n",
      "Iteration: 8810 Training Accuracy: 0.875 Loss: 0.006994219496846199\n",
      "Iteration: 8820 Training Accuracy: 0.8125 Loss: 0.009556489065289497\n",
      "Iteration: 8830 Training Accuracy: 0.84375 Loss: 0.007713498082011938\n",
      "Iteration: 8840 Training Accuracy: 0.734375 Loss: 0.012059353291988373\n",
      "Iteration: 8850 Training Accuracy: 0.84375 Loss: 0.007102564908564091\n",
      "Iteration: 8860 Training Accuracy: 0.875 Loss: 0.00698588602244854\n",
      "Iteration: 8870 Training Accuracy: 0.8125 Loss: 0.010920943692326546\n",
      "Iteration: 8880 Training Accuracy: 0.75 Loss: 0.008290068246424198\n",
      "Iteration: 8890 Training Accuracy: 0.796875 Loss: 0.00830535963177681\n",
      "Iteration: 8900 Training Accuracy: 0.890625 Loss: 0.006696848664432764\n",
      "Iteration: 8910 Training Accuracy: 0.828125 Loss: 0.010187400504946709\n",
      "Iteration: 8920 Training Accuracy: 0.859375 Loss: 0.007446739822626114\n",
      "Iteration: 8930 Training Accuracy: 0.75 Loss: 0.012640397064387798\n",
      "Iteration: 8940 Training Accuracy: 0.875 Loss: 0.006508094724267721\n",
      "Iteration: 8950 Training Accuracy: 0.84375 Loss: 0.005868718959391117\n",
      "Iteration: 8960 Training Accuracy: 0.828125 Loss: 0.008951294235885143\n",
      "Iteration: 8970 Training Accuracy: 0.84375 Loss: 0.007981415838003159\n",
      "Iteration: 8980 Training Accuracy: 0.875 Loss: 0.005742118693888187\n",
      "Iteration: 8990 Training Accuracy: 0.90625 Loss: 0.005599934607744217\n",
      "Iteration: 9000 Training Accuracy: 0.796875 Loss: 0.008672301657497883\n",
      "Iteration: 9010 Training Accuracy: 0.859375 Loss: 0.007487941533327103\n",
      "Iteration: 9020 Training Accuracy: 0.875 Loss: 0.005526567809283733\n",
      "Iteration: 9030 Training Accuracy: 0.8125 Loss: 0.00916164182126522\n",
      "Iteration: 9040 Training Accuracy: 0.84375 Loss: 0.006365320645272732\n",
      "Iteration: 9050 Training Accuracy: 0.828125 Loss: 0.006550122052431107\n",
      "Iteration: 9060 Training Accuracy: 0.890625 Loss: 0.005467734299600124\n",
      "Iteration: 9070 Training Accuracy: 0.796875 Loss: 0.0073299892246723175\n",
      "Iteration: 9080 Training Accuracy: 0.796875 Loss: 0.00832891184836626\n",
      "Iteration: 9090 Training Accuracy: 0.84375 Loss: 0.0068701631389558315\n",
      "Iteration: 9100 Training Accuracy: 0.84375 Loss: 0.009701356291770935\n",
      "Iteration: 9110 Training Accuracy: 0.828125 Loss: 0.010920656844973564\n",
      "Iteration: 9120 Training Accuracy: 0.90625 Loss: 0.0052447933703660965\n",
      "Iteration: 9130 Training Accuracy: 0.78125 Loss: 0.00987138319760561\n",
      "Iteration: 9140 Training Accuracy: 0.875 Loss: 0.00739938672631979\n",
      "Iteration: 9150 Training Accuracy: 0.859375 Loss: 0.007231442723423243\n",
      "Iteration: 9160 Training Accuracy: 0.875 Loss: 0.006635548546910286\n",
      "Iteration: 9170 Training Accuracy: 0.828125 Loss: 0.00900119449943304\n",
      "Iteration: 9180 Training Accuracy: 0.8125 Loss: 0.009404425509274006\n",
      "Iteration: 9190 Training Accuracy: 0.921875 Loss: 0.005041183438152075\n",
      "Iteration: 9200 Training Accuracy: 0.859375 Loss: 0.00780303543433547\n",
      "Iteration: 9210 Training Accuracy: 0.8125 Loss: 0.008205784484744072\n",
      "Iteration: 9220 Training Accuracy: 0.8125 Loss: 0.00938498042523861\n",
      "Iteration: 9230 Training Accuracy: 0.84375 Loss: 0.00855185929685831\n",
      "Iteration: 9240 Training Accuracy: 0.90625 Loss: 0.004948315676301718\n",
      "Iteration: 9250 Training Accuracy: 0.828125 Loss: 0.008656884543597698\n",
      "Iteration: 9260 Training Accuracy: 0.828125 Loss: 0.006865066941827536\n",
      "Iteration: 9270 Training Accuracy: 0.921875 Loss: 0.005898537114262581\n",
      "Iteration: 9280 Training Accuracy: 0.875 Loss: 0.005701283924281597\n",
      "Iteration: 9290 Training Accuracy: 0.875 Loss: 0.006687098182737827\n",
      "Iteration: 9300 Training Accuracy: 0.796875 Loss: 0.008447573520243168\n",
      "Iteration: 9310 Training Accuracy: 0.859375 Loss: 0.005465921945869923\n",
      "Iteration: 9320 Training Accuracy: 0.828125 Loss: 0.006629357114434242\n",
      "Iteration: 9330 Training Accuracy: 0.828125 Loss: 0.0077127679251134396\n",
      "Iteration: 9340 Training Accuracy: 0.84375 Loss: 0.006443082354962826\n",
      "Iteration: 9350 Training Accuracy: 0.90625 Loss: 0.004828603006899357\n",
      "Iteration: 9360 Training Accuracy: 0.8125 Loss: 0.0089981434866786\n",
      "Iteration: 9370 Training Accuracy: 0.890625 Loss: 0.005954319145530462\n",
      "Iteration: 9380 Training Accuracy: 0.75 Loss: 0.012246527709066868\n",
      "Training Accuracy = 0.75\n",
      "Validation Accuracy = 0\n",
      "epoch: 10\n",
      "Iteration: 9390 Training Accuracy: 0.78125 Loss: 0.009816420264542103\n",
      "Iteration: 9400 Training Accuracy: 0.890625 Loss: 0.005705148912966251\n",
      "Iteration: 9410 Training Accuracy: 0.875 Loss: 0.007278613280504942\n",
      "Iteration: 9420 Training Accuracy: 0.796875 Loss: 0.00984359160065651\n",
      "Iteration: 9430 Training Accuracy: 0.8125 Loss: 0.007938220165669918\n",
      "Iteration: 9440 Training Accuracy: 0.828125 Loss: 0.009896169416606426\n",
      "Iteration: 9450 Training Accuracy: 0.828125 Loss: 0.007840492762625217\n",
      "Iteration: 9460 Training Accuracy: 0.828125 Loss: 0.00793773215264082\n",
      "Iteration: 9470 Training Accuracy: 0.875 Loss: 0.0059486436657607555\n",
      "Iteration: 9480 Training Accuracy: 0.78125 Loss: 0.010048200376331806\n",
      "Iteration: 9490 Training Accuracy: 0.859375 Loss: 0.006619146093726158\n",
      "Iteration: 9500 Training Accuracy: 0.953125 Loss: 0.004578946623951197\n",
      "Iteration: 9510 Training Accuracy: 0.796875 Loss: 0.011095410212874413\n",
      "Iteration: 9520 Training Accuracy: 0.78125 Loss: 0.010208464227616787\n",
      "Iteration: 9530 Training Accuracy: 0.8125 Loss: 0.008252676576375961\n",
      "Iteration: 9540 Training Accuracy: 0.828125 Loss: 0.0071608880534768105\n",
      "Iteration: 9550 Training Accuracy: 0.890625 Loss: 0.008171872235834599\n",
      "Iteration: 9560 Training Accuracy: 0.8125 Loss: 0.009912379086017609\n",
      "Iteration: 9570 Training Accuracy: 0.796875 Loss: 0.010200975462794304\n",
      "Iteration: 9580 Training Accuracy: 0.921875 Loss: 0.005060573108494282\n",
      "Iteration: 9590 Training Accuracy: 0.890625 Loss: 0.005605321377515793\n",
      "Iteration: 9600 Training Accuracy: 0.828125 Loss: 0.008201365359127522\n",
      "Iteration: 9610 Training Accuracy: 0.84375 Loss: 0.007246837019920349\n",
      "Iteration: 9620 Training Accuracy: 0.78125 Loss: 0.015807317569851875\n",
      "Iteration: 9630 Training Accuracy: 0.859375 Loss: 0.006901311222463846\n",
      "Iteration: 9640 Training Accuracy: 0.9375 Loss: 0.0036527407355606556\n",
      "Iteration: 9650 Training Accuracy: 0.84375 Loss: 0.006776066496968269\n",
      "Iteration: 9660 Training Accuracy: 0.828125 Loss: 0.009527303278446198\n",
      "Iteration: 9670 Training Accuracy: 0.84375 Loss: 0.007987415418028831\n",
      "Iteration: 9680 Training Accuracy: 0.890625 Loss: 0.007824989035725594\n",
      "Iteration: 9690 Training Accuracy: 0.765625 Loss: 0.010941382497549057\n",
      "Iteration: 9700 Training Accuracy: 0.859375 Loss: 0.007293902337551117\n",
      "Iteration: 9710 Training Accuracy: 0.890625 Loss: 0.005566488020122051\n",
      "Iteration: 9720 Training Accuracy: 0.890625 Loss: 0.0048437342047691345\n",
      "Iteration: 9730 Training Accuracy: 0.828125 Loss: 0.007701711729168892\n",
      "Iteration: 9740 Training Accuracy: 0.84375 Loss: 0.008206857368350029\n",
      "Iteration: 9750 Training Accuracy: 0.890625 Loss: 0.006338910665363073\n",
      "Iteration: 9760 Training Accuracy: 0.828125 Loss: 0.009662584401667118\n",
      "Iteration: 9770 Training Accuracy: 0.859375 Loss: 0.009710142388939857\n",
      "Iteration: 9780 Training Accuracy: 0.8125 Loss: 0.008857275359332561\n",
      "Iteration: 9790 Training Accuracy: 0.875 Loss: 0.005693177692592144\n",
      "Iteration: 9800 Training Accuracy: 0.765625 Loss: 0.006861330475658178\n",
      "Iteration: 9810 Training Accuracy: 0.875 Loss: 0.007269065361469984\n",
      "Iteration: 9820 Training Accuracy: 0.890625 Loss: 0.005536273587495089\n",
      "Iteration: 9830 Training Accuracy: 0.734375 Loss: 0.011077319271862507\n",
      "Iteration: 9840 Training Accuracy: 0.828125 Loss: 0.007898516952991486\n",
      "Iteration: 9850 Training Accuracy: 0.890625 Loss: 0.005338133312761784\n",
      "Iteration: 9860 Training Accuracy: 0.859375 Loss: 0.005502158775925636\n",
      "Iteration: 9870 Training Accuracy: 0.84375 Loss: 0.008392153307795525\n",
      "Iteration: 9880 Training Accuracy: 0.734375 Loss: 0.009805923327803612\n",
      "Iteration: 9890 Training Accuracy: 0.9375 Loss: 0.004392524715512991\n",
      "Iteration: 9900 Training Accuracy: 0.84375 Loss: 0.009875084273517132\n",
      "Iteration: 9910 Training Accuracy: 0.875 Loss: 0.00582381896674633\n",
      "Iteration: 9920 Training Accuracy: 0.875 Loss: 0.005380056798458099\n",
      "Iteration: 9930 Training Accuracy: 0.796875 Loss: 0.009976385161280632\n",
      "Iteration: 9940 Training Accuracy: 0.84375 Loss: 0.006403544917702675\n",
      "Iteration: 9950 Training Accuracy: 0.84375 Loss: 0.011143184266984463\n",
      "Iteration: 9960 Training Accuracy: 0.796875 Loss: 0.008988579735159874\n",
      "Iteration: 9970 Training Accuracy: 0.984375 Loss: 0.0026625171303749084\n",
      "Iteration: 9980 Training Accuracy: 0.8125 Loss: 0.007558773271739483\n",
      "Iteration: 9990 Training Accuracy: 0.921875 Loss: 0.0032453336752951145\n",
      "Iteration: 10000 Training Accuracy: 0.796875 Loss: 0.009313410148024559\n",
      "Iteration: 10010 Training Accuracy: 0.890625 Loss: 0.005141738802194595\n",
      "Iteration: 10020 Training Accuracy: 0.828125 Loss: 0.008043840527534485\n",
      "Iteration: 10030 Training Accuracy: 0.9375 Loss: 0.004902560263872147\n",
      "Iteration: 10040 Training Accuracy: 0.84375 Loss: 0.007880419492721558\n",
      "Iteration: 10050 Training Accuracy: 0.90625 Loss: 0.0046457331627607346\n",
      "Iteration: 10060 Training Accuracy: 0.828125 Loss: 0.011237865313887596\n",
      "Iteration: 10070 Training Accuracy: 0.921875 Loss: 0.004402584861963987\n",
      "Iteration: 10080 Training Accuracy: 0.859375 Loss: 0.008291835896670818\n",
      "Iteration: 10090 Training Accuracy: 0.84375 Loss: 0.0063318354077637196\n",
      "Iteration: 10100 Training Accuracy: 0.859375 Loss: 0.008051851764321327\n",
      "Iteration: 10110 Training Accuracy: 0.8125 Loss: 0.009465284645557404\n",
      "Iteration: 10120 Training Accuracy: 0.828125 Loss: 0.008580922149121761\n",
      "Iteration: 10130 Training Accuracy: 0.84375 Loss: 0.00800906028598547\n",
      "Iteration: 10140 Training Accuracy: 0.828125 Loss: 0.009851472452282906\n",
      "Iteration: 10150 Training Accuracy: 0.859375 Loss: 0.00788925215601921\n",
      "Iteration: 10160 Training Accuracy: 0.953125 Loss: 0.005214888136833906\n",
      "Iteration: 10170 Training Accuracy: 0.875 Loss: 0.006450262852013111\n",
      "Iteration: 10180 Training Accuracy: 0.796875 Loss: 0.009938953444361687\n",
      "Iteration: 10190 Training Accuracy: 0.8125 Loss: 0.00925663486123085\n",
      "Iteration: 10200 Training Accuracy: 0.90625 Loss: 0.0053509557619690895\n",
      "Iteration: 10210 Training Accuracy: 0.859375 Loss: 0.0073716603219509125\n",
      "Iteration: 10220 Training Accuracy: 0.828125 Loss: 0.0064409757032990456\n",
      "Iteration: 10230 Training Accuracy: 0.90625 Loss: 0.007741062436252832\n",
      "Iteration: 10240 Training Accuracy: 0.9375 Loss: 0.003762408159673214\n",
      "Iteration: 10250 Training Accuracy: 0.828125 Loss: 0.006861476693302393\n",
      "Iteration: 10260 Training Accuracy: 0.875 Loss: 0.00530935637652874\n",
      "Iteration: 10270 Training Accuracy: 0.859375 Loss: 0.006990797817707062\n",
      "Iteration: 10280 Training Accuracy: 0.859375 Loss: 0.006287276279181242\n",
      "Iteration: 10290 Training Accuracy: 0.859375 Loss: 0.007455585990101099\n",
      "Iteration: 10300 Training Accuracy: 0.875 Loss: 0.0063301390036940575\n",
      "Iteration: 10310 Training Accuracy: 0.890625 Loss: 0.0051537128165364265\n",
      "Training Accuracy = 0.78125\n",
      "Validation Accuracy = 0\n",
      "epoch: 11\n",
      "Iteration: 10320 Training Accuracy: 0.8125 Loss: 0.011236792430281639\n",
      "Iteration: 10330 Training Accuracy: 0.8125 Loss: 0.010975637473165989\n",
      "Iteration: 10340 Training Accuracy: 0.84375 Loss: 0.006490660831332207\n",
      "Iteration: 10350 Training Accuracy: 0.953125 Loss: 0.003996244631707668\n",
      "Iteration: 10360 Training Accuracy: 0.90625 Loss: 0.0035441352520138025\n",
      "Iteration: 10370 Training Accuracy: 0.890625 Loss: 0.004585782065987587\n",
      "Iteration: 10380 Training Accuracy: 0.9375 Loss: 0.00493388157337904\n",
      "Iteration: 10390 Training Accuracy: 0.921875 Loss: 0.004128646105527878\n",
      "Iteration: 10400 Training Accuracy: 0.921875 Loss: 0.004526722244918346\n",
      "Iteration: 10410 Training Accuracy: 0.828125 Loss: 0.00859192293137312\n",
      "Iteration: 10420 Training Accuracy: 0.796875 Loss: 0.008227788843214512\n",
      "Iteration: 10430 Training Accuracy: 0.890625 Loss: 0.005005311220884323\n",
      "Iteration: 10440 Training Accuracy: 0.875 Loss: 0.0067206695675849915\n",
      "Iteration: 10450 Training Accuracy: 0.796875 Loss: 0.008852416649460793\n",
      "Iteration: 10460 Training Accuracy: 0.875 Loss: 0.006569434888660908\n",
      "Iteration: 10470 Training Accuracy: 0.828125 Loss: 0.00795785803347826\n",
      "Iteration: 10480 Training Accuracy: 0.78125 Loss: 0.009260628372430801\n",
      "Iteration: 10490 Training Accuracy: 0.859375 Loss: 0.008520379662513733\n",
      "Iteration: 10500 Training Accuracy: 0.890625 Loss: 0.005154414102435112\n",
      "Iteration: 10510 Training Accuracy: 0.875 Loss: 0.006829092279076576\n",
      "Iteration: 10520 Training Accuracy: 0.921875 Loss: 0.004681710619479418\n",
      "Iteration: 10530 Training Accuracy: 0.84375 Loss: 0.007305001839995384\n",
      "Iteration: 10540 Training Accuracy: 0.828125 Loss: 0.00836917944252491\n",
      "Iteration: 10550 Training Accuracy: 0.890625 Loss: 0.005636350251734257\n",
      "Iteration: 10560 Training Accuracy: 0.90625 Loss: 0.004985477309674025\n",
      "Iteration: 10570 Training Accuracy: 0.828125 Loss: 0.011390268802642822\n",
      "Iteration: 10580 Training Accuracy: 0.859375 Loss: 0.007047565653920174\n",
      "Iteration: 10590 Training Accuracy: 0.796875 Loss: 0.008659197948873043\n",
      "Iteration: 10600 Training Accuracy: 0.84375 Loss: 0.007135211955755949\n",
      "Iteration: 10610 Training Accuracy: 0.859375 Loss: 0.007008467335253954\n",
      "Iteration: 10620 Training Accuracy: 0.859375 Loss: 0.006502293515950441\n",
      "Iteration: 10630 Training Accuracy: 0.8125 Loss: 0.010032491758465767\n",
      "Iteration: 10640 Training Accuracy: 0.828125 Loss: 0.010742375627160072\n",
      "Iteration: 10650 Training Accuracy: 0.859375 Loss: 0.005816817283630371\n",
      "Iteration: 10660 Training Accuracy: 0.890625 Loss: 0.004895990714430809\n",
      "Iteration: 10670 Training Accuracy: 0.8125 Loss: 0.008111931383609772\n",
      "Iteration: 10680 Training Accuracy: 0.890625 Loss: 0.00467744842171669\n",
      "Iteration: 10690 Training Accuracy: 0.78125 Loss: 0.008264725096523762\n",
      "Iteration: 10700 Training Accuracy: 0.8125 Loss: 0.009118925780057907\n",
      "Iteration: 10710 Training Accuracy: 0.828125 Loss: 0.006721158511936665\n",
      "Iteration: 10720 Training Accuracy: 0.828125 Loss: 0.008389660157263279\n",
      "Iteration: 10730 Training Accuracy: 0.796875 Loss: 0.009807445108890533\n",
      "Iteration: 10740 Training Accuracy: 0.796875 Loss: 0.010667035356163979\n",
      "Iteration: 10750 Training Accuracy: 0.8125 Loss: 0.008832309395074844\n",
      "Iteration: 10760 Training Accuracy: 0.875 Loss: 0.0049097249284386635\n",
      "Iteration: 10770 Training Accuracy: 0.8125 Loss: 0.01018550992012024\n",
      "Iteration: 10780 Training Accuracy: 0.875 Loss: 0.007298724725842476\n",
      "Iteration: 10790 Training Accuracy: 0.890625 Loss: 0.004993056878447533\n",
      "Iteration: 10800 Training Accuracy: 0.84375 Loss: 0.007874603383243084\n",
      "Iteration: 10810 Training Accuracy: 0.90625 Loss: 0.004352414049208164\n",
      "Iteration: 10820 Training Accuracy: 0.828125 Loss: 0.008022943511605263\n",
      "Iteration: 10830 Training Accuracy: 0.78125 Loss: 0.008566563948988914\n",
      "Iteration: 10840 Training Accuracy: 0.8125 Loss: 0.007745082024484873\n",
      "Iteration: 10850 Training Accuracy: 0.84375 Loss: 0.005351223982870579\n",
      "Iteration: 10860 Training Accuracy: 0.8125 Loss: 0.008545685559511185\n",
      "Iteration: 10870 Training Accuracy: 0.8125 Loss: 0.009237581863999367\n",
      "Iteration: 10880 Training Accuracy: 0.90625 Loss: 0.003942043520510197\n",
      "Iteration: 10890 Training Accuracy: 0.859375 Loss: 0.007296344731003046\n",
      "Iteration: 10900 Training Accuracy: 0.875 Loss: 0.004599311389029026\n",
      "Iteration: 10910 Training Accuracy: 0.84375 Loss: 0.006417448632419109\n",
      "Iteration: 10920 Training Accuracy: 0.828125 Loss: 0.006445582024753094\n",
      "Iteration: 10930 Training Accuracy: 0.796875 Loss: 0.009025818668305874\n",
      "Iteration: 10940 Training Accuracy: 0.8125 Loss: 0.008215563371777534\n",
      "Iteration: 10950 Training Accuracy: 0.90625 Loss: 0.004107923246920109\n",
      "Iteration: 10960 Training Accuracy: 0.84375 Loss: 0.0076055689714848995\n",
      "Iteration: 10970 Training Accuracy: 0.859375 Loss: 0.0050241900607943535\n",
      "Iteration: 10980 Training Accuracy: 0.8125 Loss: 0.010377934202551842\n",
      "Iteration: 10990 Training Accuracy: 0.828125 Loss: 0.007880938239395618\n",
      "Iteration: 11000 Training Accuracy: 0.875 Loss: 0.0063002631068229675\n",
      "Iteration: 11010 Training Accuracy: 0.796875 Loss: 0.009289337322115898\n",
      "Iteration: 11020 Training Accuracy: 0.84375 Loss: 0.007337189745157957\n",
      "Iteration: 11030 Training Accuracy: 0.890625 Loss: 0.006213782355189323\n",
      "Iteration: 11040 Training Accuracy: 0.921875 Loss: 0.004611460492014885\n",
      "Iteration: 11050 Training Accuracy: 0.734375 Loss: 0.010892098769545555\n",
      "Iteration: 11060 Training Accuracy: 0.9375 Loss: 0.0036309976130723953\n",
      "Iteration: 11070 Training Accuracy: 0.921875 Loss: 0.005502346903085709\n",
      "Iteration: 11080 Training Accuracy: 0.84375 Loss: 0.0087373536080122\n",
      "Iteration: 11090 Training Accuracy: 0.84375 Loss: 0.007144037634134293\n",
      "Iteration: 11100 Training Accuracy: 0.84375 Loss: 0.007418796420097351\n",
      "Iteration: 11110 Training Accuracy: 0.84375 Loss: 0.00741215655580163\n",
      "Iteration: 11120 Training Accuracy: 0.8125 Loss: 0.006764343939721584\n",
      "Iteration: 11130 Training Accuracy: 0.859375 Loss: 0.006447616033256054\n",
      "Iteration: 11140 Training Accuracy: 0.8125 Loss: 0.007356113754212856\n",
      "Iteration: 11150 Training Accuracy: 0.84375 Loss: 0.008191713131964207\n",
      "Iteration: 11160 Training Accuracy: 0.859375 Loss: 0.007447281386703253\n",
      "Iteration: 11170 Training Accuracy: 0.859375 Loss: 0.009315636940300465\n",
      "Iteration: 11180 Training Accuracy: 0.875 Loss: 0.006114213727414608\n",
      "Iteration: 11190 Training Accuracy: 0.921875 Loss: 0.005118226166814566\n",
      "Iteration: 11200 Training Accuracy: 0.78125 Loss: 0.007890325039625168\n",
      "Iteration: 11210 Training Accuracy: 0.890625 Loss: 0.0071007548831403255\n",
      "Iteration: 11220 Training Accuracy: 0.890625 Loss: 0.0065975007601082325\n",
      "Iteration: 11230 Training Accuracy: 0.796875 Loss: 0.011134030297398567\n",
      "Iteration: 11240 Training Accuracy: 0.90625 Loss: 0.004595181439071894\n",
      "Iteration: 11250 Training Accuracy: 0.75 Loss: 0.011328371241688728\n",
      "Training Accuracy = 0.84375\n",
      "Validation Accuracy = 0\n",
      "epoch: 12\n",
      "Iteration: 11260 Training Accuracy: 0.828125 Loss: 0.008286474272608757\n",
      "Iteration: 11270 Training Accuracy: 0.828125 Loss: 0.007697813678532839\n",
      "Iteration: 11280 Training Accuracy: 0.90625 Loss: 0.005525623448193073\n",
      "Iteration: 11290 Training Accuracy: 0.8125 Loss: 0.008192799054086208\n",
      "Iteration: 11300 Training Accuracy: 0.875 Loss: 0.006492163520306349\n",
      "Iteration: 11310 Training Accuracy: 0.84375 Loss: 0.008232666179537773\n",
      "Iteration: 11320 Training Accuracy: 0.8125 Loss: 0.01105864904820919\n",
      "Iteration: 11330 Training Accuracy: 0.8125 Loss: 0.00919925794005394\n",
      "Iteration: 11340 Training Accuracy: 0.84375 Loss: 0.006429914850741625\n",
      "Iteration: 11350 Training Accuracy: 0.796875 Loss: 0.009485894814133644\n",
      "Iteration: 11360 Training Accuracy: 0.859375 Loss: 0.005977922584861517\n",
      "Iteration: 11370 Training Accuracy: 0.90625 Loss: 0.0058278292417526245\n",
      "Iteration: 11380 Training Accuracy: 0.8125 Loss: 0.00647636828944087\n",
      "Iteration: 11390 Training Accuracy: 0.890625 Loss: 0.006760038435459137\n",
      "Iteration: 11400 Training Accuracy: 0.921875 Loss: 0.00398405222222209\n",
      "Iteration: 11410 Training Accuracy: 0.859375 Loss: 0.006528070196509361\n",
      "Iteration: 11420 Training Accuracy: 0.890625 Loss: 0.006915473844856024\n",
      "Iteration: 11430 Training Accuracy: 0.859375 Loss: 0.006314355880022049\n",
      "Iteration: 11440 Training Accuracy: 0.828125 Loss: 0.008986934088170528\n",
      "Iteration: 11450 Training Accuracy: 0.875 Loss: 0.0082265455275774\n",
      "Iteration: 11460 Training Accuracy: 0.953125 Loss: 0.0033272947184741497\n",
      "Iteration: 11470 Training Accuracy: 0.90625 Loss: 0.005255792755633593\n",
      "Iteration: 11480 Training Accuracy: 0.8125 Loss: 0.008415344171226025\n",
      "Iteration: 11490 Training Accuracy: 0.84375 Loss: 0.009011603891849518\n",
      "Iteration: 11500 Training Accuracy: 0.78125 Loss: 0.009890774264931679\n",
      "Iteration: 11510 Training Accuracy: 0.90625 Loss: 0.0046266778372228146\n",
      "Iteration: 11520 Training Accuracy: 0.9375 Loss: 0.0040147751569747925\n",
      "Iteration: 11530 Training Accuracy: 0.78125 Loss: 0.0069709643721580505\n",
      "Iteration: 11540 Training Accuracy: 0.890625 Loss: 0.00652482733130455\n",
      "Iteration: 11550 Training Accuracy: 0.828125 Loss: 0.008442258462309837\n",
      "Iteration: 11560 Training Accuracy: 0.890625 Loss: 0.005287538282573223\n",
      "Iteration: 11570 Training Accuracy: 0.859375 Loss: 0.005956253036856651\n",
      "Iteration: 11580 Training Accuracy: 0.859375 Loss: 0.0070107728242874146\n",
      "Iteration: 11590 Training Accuracy: 0.828125 Loss: 0.009397203102707863\n",
      "Iteration: 11600 Training Accuracy: 0.859375 Loss: 0.008021996356546879\n",
      "Iteration: 11610 Training Accuracy: 0.953125 Loss: 0.0033613904379308224\n",
      "Iteration: 11620 Training Accuracy: 0.828125 Loss: 0.009326251223683357\n",
      "Iteration: 11630 Training Accuracy: 0.890625 Loss: 0.00518685020506382\n",
      "Iteration: 11640 Training Accuracy: 0.78125 Loss: 0.009426027536392212\n",
      "Iteration: 11650 Training Accuracy: 0.859375 Loss: 0.00863115955144167\n",
      "Iteration: 11660 Training Accuracy: 0.859375 Loss: 0.006138393189758062\n",
      "Iteration: 11670 Training Accuracy: 0.875 Loss: 0.006237651687115431\n",
      "Iteration: 11680 Training Accuracy: 0.828125 Loss: 0.008359252475202084\n",
      "Iteration: 11690 Training Accuracy: 0.90625 Loss: 0.004777287133038044\n",
      "Iteration: 11700 Training Accuracy: 0.828125 Loss: 0.008067088201642036\n",
      "Iteration: 11710 Training Accuracy: 0.84375 Loss: 0.007669825106859207\n",
      "Iteration: 11720 Training Accuracy: 0.828125 Loss: 0.007612859830260277\n",
      "Iteration: 11730 Training Accuracy: 0.9375 Loss: 0.0047194636426866055\n",
      "Iteration: 11740 Training Accuracy: 0.859375 Loss: 0.006902662105858326\n",
      "Iteration: 11750 Training Accuracy: 0.890625 Loss: 0.005532595328986645\n",
      "Iteration: 11760 Training Accuracy: 0.875 Loss: 0.009602610021829605\n",
      "Iteration: 11770 Training Accuracy: 0.875 Loss: 0.008218804374337196\n",
      "Iteration: 11780 Training Accuracy: 0.84375 Loss: 0.01023297943174839\n",
      "Iteration: 11790 Training Accuracy: 0.796875 Loss: 0.007601846940815449\n",
      "Iteration: 11800 Training Accuracy: 0.84375 Loss: 0.006588242948055267\n",
      "Iteration: 11810 Training Accuracy: 0.890625 Loss: 0.0060967449098825455\n",
      "Iteration: 11820 Training Accuracy: 0.90625 Loss: 0.0038989284075796604\n",
      "Iteration: 11830 Training Accuracy: 0.890625 Loss: 0.005977865308523178\n",
      "Iteration: 11840 Training Accuracy: 0.9375 Loss: 0.003991617821156979\n",
      "Iteration: 11850 Training Accuracy: 0.890625 Loss: 0.006194971036165953\n",
      "Iteration: 11860 Training Accuracy: 0.78125 Loss: 0.010084192268550396\n",
      "Iteration: 11870 Training Accuracy: 0.90625 Loss: 0.0037829300854355097\n",
      "Iteration: 11880 Training Accuracy: 0.84375 Loss: 0.009985070675611496\n",
      "Iteration: 11890 Training Accuracy: 0.859375 Loss: 0.006757223978638649\n",
      "Iteration: 11900 Training Accuracy: 0.859375 Loss: 0.007281293161213398\n",
      "Iteration: 11910 Training Accuracy: 0.84375 Loss: 0.01050412468612194\n",
      "Iteration: 11920 Training Accuracy: 0.875 Loss: 0.006919607985764742\n",
      "Iteration: 11930 Training Accuracy: 0.796875 Loss: 0.007161357440054417\n",
      "Iteration: 11940 Training Accuracy: 0.859375 Loss: 0.006774209439754486\n",
      "Iteration: 11950 Training Accuracy: 0.875 Loss: 0.006218016147613525\n",
      "Iteration: 11960 Training Accuracy: 0.84375 Loss: 0.006981981452554464\n",
      "Iteration: 11970 Training Accuracy: 0.9375 Loss: 0.0032680300064384937\n",
      "Iteration: 11980 Training Accuracy: 0.859375 Loss: 0.006110766902565956\n",
      "Iteration: 11990 Training Accuracy: 0.796875 Loss: 0.008243372663855553\n",
      "Iteration: 12000 Training Accuracy: 0.796875 Loss: 0.010720443911850452\n",
      "Iteration: 12010 Training Accuracy: 0.859375 Loss: 0.006468702107667923\n",
      "Iteration: 12020 Training Accuracy: 0.71875 Loss: 0.012607402168214321\n",
      "Iteration: 12030 Training Accuracy: 0.875 Loss: 0.007803056389093399\n",
      "Iteration: 12040 Training Accuracy: 0.75 Loss: 0.011458415538072586\n",
      "Iteration: 12050 Training Accuracy: 0.90625 Loss: 0.00469084782525897\n",
      "Iteration: 12060 Training Accuracy: 0.765625 Loss: 0.013451099395751953\n",
      "Iteration: 12070 Training Accuracy: 0.875 Loss: 0.005892700515687466\n",
      "Iteration: 12080 Training Accuracy: 0.875 Loss: 0.0050547169521451\n",
      "Iteration: 12090 Training Accuracy: 0.859375 Loss: 0.007092964835464954\n",
      "Iteration: 12100 Training Accuracy: 0.859375 Loss: 0.005464853718876839\n",
      "Iteration: 12110 Training Accuracy: 0.8125 Loss: 0.00965646281838417\n",
      "Iteration: 12120 Training Accuracy: 0.875 Loss: 0.007532735820859671\n",
      "Iteration: 12130 Training Accuracy: 0.875 Loss: 0.006933812517672777\n",
      "Iteration: 12140 Training Accuracy: 0.90625 Loss: 0.004896635189652443\n",
      "Iteration: 12150 Training Accuracy: 0.828125 Loss: 0.01046702265739441\n",
      "Iteration: 12160 Training Accuracy: 0.90625 Loss: 0.005406314041465521\n",
      "Iteration: 12170 Training Accuracy: 0.890625 Loss: 0.005246160086244345\n",
      "Iteration: 12180 Training Accuracy: 0.796875 Loss: 0.010006792843341827\n",
      "Iteration: 12190 Training Accuracy: 0.875 Loss: 0.00602013198658824\n",
      "Training Accuracy = 0.875\n",
      "Validation Accuracy = 0\n",
      "epoch: 13\n",
      "Iteration: 12200 Training Accuracy: 0.859375 Loss: 0.00584397092461586\n",
      "Iteration: 12210 Training Accuracy: 0.859375 Loss: 0.007822483777999878\n",
      "Iteration: 12220 Training Accuracy: 0.921875 Loss: 0.004442681558430195\n",
      "Iteration: 12230 Training Accuracy: 0.890625 Loss: 0.006679217796772718\n",
      "Iteration: 12240 Training Accuracy: 0.78125 Loss: 0.009404229000210762\n",
      "Iteration: 12250 Training Accuracy: 0.859375 Loss: 0.006023217923939228\n",
      "Iteration: 12260 Training Accuracy: 0.84375 Loss: 0.00857565924525261\n",
      "Iteration: 12270 Training Accuracy: 0.796875 Loss: 0.007365433033555746\n",
      "Iteration: 12280 Training Accuracy: 0.859375 Loss: 0.006427804008126259\n",
      "Iteration: 12290 Training Accuracy: 0.84375 Loss: 0.0064374106004834175\n",
      "Iteration: 12300 Training Accuracy: 0.8125 Loss: 0.008448770269751549\n",
      "Iteration: 12310 Training Accuracy: 0.875 Loss: 0.005723572801798582\n",
      "Iteration: 12320 Training Accuracy: 0.859375 Loss: 0.00514637865126133\n",
      "Iteration: 12330 Training Accuracy: 0.875 Loss: 0.005369963124394417\n",
      "Iteration: 12340 Training Accuracy: 0.84375 Loss: 0.006901314482092857\n",
      "Iteration: 12350 Training Accuracy: 0.828125 Loss: 0.009687596932053566\n",
      "Iteration: 12360 Training Accuracy: 0.84375 Loss: 0.006823308765888214\n",
      "Iteration: 12370 Training Accuracy: 0.890625 Loss: 0.005618845112621784\n",
      "Iteration: 12380 Training Accuracy: 0.890625 Loss: 0.006034858524799347\n",
      "Iteration: 12390 Training Accuracy: 0.859375 Loss: 0.006292914506047964\n",
      "Iteration: 12400 Training Accuracy: 0.90625 Loss: 0.004655233584344387\n",
      "Iteration: 12410 Training Accuracy: 0.875 Loss: 0.006066457834094763\n",
      "Iteration: 12420 Training Accuracy: 0.921875 Loss: 0.004435622598975897\n",
      "Iteration: 12430 Training Accuracy: 0.90625 Loss: 0.003738713450729847\n",
      "Iteration: 12440 Training Accuracy: 0.890625 Loss: 0.0067048147320747375\n",
      "Iteration: 12450 Training Accuracy: 0.875 Loss: 0.006433460861444473\n",
      "Iteration: 12460 Training Accuracy: 0.859375 Loss: 0.009340177290141582\n",
      "Iteration: 12470 Training Accuracy: 0.875 Loss: 0.00725167989730835\n",
      "Iteration: 12480 Training Accuracy: 0.859375 Loss: 0.0065445867367088795\n",
      "Iteration: 12490 Training Accuracy: 0.875 Loss: 0.007150236051529646\n",
      "Iteration: 12500 Training Accuracy: 0.890625 Loss: 0.006623213179409504\n",
      "Iteration: 12510 Training Accuracy: 0.828125 Loss: 0.008613504469394684\n",
      "Iteration: 12520 Training Accuracy: 0.953125 Loss: 0.0041817957535386086\n",
      "Iteration: 12530 Training Accuracy: 0.765625 Loss: 0.009814083576202393\n",
      "Iteration: 12540 Training Accuracy: 0.890625 Loss: 0.005450070835649967\n",
      "Iteration: 12550 Training Accuracy: 0.890625 Loss: 0.00626726821064949\n",
      "Iteration: 12560 Training Accuracy: 0.90625 Loss: 0.0044223894365131855\n",
      "Iteration: 12570 Training Accuracy: 0.875 Loss: 0.005145387724041939\n",
      "Iteration: 12580 Training Accuracy: 0.796875 Loss: 0.009471760131418705\n",
      "Iteration: 12590 Training Accuracy: 0.828125 Loss: 0.00793861597776413\n",
      "Iteration: 12600 Training Accuracy: 0.921875 Loss: 0.004628063179552555\n",
      "Iteration: 12610 Training Accuracy: 0.859375 Loss: 0.006734649650752544\n",
      "Iteration: 12620 Training Accuracy: 0.890625 Loss: 0.004327364265918732\n",
      "Iteration: 12630 Training Accuracy: 0.859375 Loss: 0.006839759647846222\n",
      "Iteration: 12640 Training Accuracy: 0.796875 Loss: 0.00863965880125761\n",
      "Iteration: 12650 Training Accuracy: 0.859375 Loss: 0.007031810469925404\n",
      "Iteration: 12660 Training Accuracy: 0.8125 Loss: 0.007173200603574514\n",
      "Iteration: 12670 Training Accuracy: 0.796875 Loss: 0.009358817711472511\n",
      "Iteration: 12680 Training Accuracy: 0.765625 Loss: 0.009255331009626389\n",
      "Iteration: 12690 Training Accuracy: 0.859375 Loss: 0.005678186193108559\n",
      "Iteration: 12700 Training Accuracy: 0.921875 Loss: 0.004132564645260572\n",
      "Iteration: 12710 Training Accuracy: 0.875 Loss: 0.005408097058534622\n",
      "Iteration: 12720 Training Accuracy: 0.890625 Loss: 0.006725277751684189\n",
      "Iteration: 12730 Training Accuracy: 0.859375 Loss: 0.006016779690980911\n",
      "Iteration: 12740 Training Accuracy: 0.8125 Loss: 0.01056654378771782\n",
      "Iteration: 12750 Training Accuracy: 0.84375 Loss: 0.006341688334941864\n",
      "Iteration: 12760 Training Accuracy: 0.921875 Loss: 0.004525927826762199\n",
      "Iteration: 12770 Training Accuracy: 0.859375 Loss: 0.0060053481720387936\n",
      "Iteration: 12780 Training Accuracy: 0.875 Loss: 0.005920318886637688\n",
      "Iteration: 12790 Training Accuracy: 0.859375 Loss: 0.007345660589635372\n",
      "Iteration: 12800 Training Accuracy: 0.890625 Loss: 0.004527583252638578\n",
      "Iteration: 12810 Training Accuracy: 0.859375 Loss: 0.00599968945607543\n",
      "Iteration: 12820 Training Accuracy: 0.90625 Loss: 0.005585609469562769\n",
      "Iteration: 12830 Training Accuracy: 0.890625 Loss: 0.005780109204351902\n",
      "Iteration: 12840 Training Accuracy: 0.84375 Loss: 0.010266836732625961\n",
      "Iteration: 12850 Training Accuracy: 0.890625 Loss: 0.005698942579329014\n",
      "Iteration: 12860 Training Accuracy: 0.875 Loss: 0.0048582591116428375\n",
      "Iteration: 12870 Training Accuracy: 0.859375 Loss: 0.006848805118352175\n",
      "Iteration: 12880 Training Accuracy: 0.859375 Loss: 0.00750837754458189\n",
      "Iteration: 12890 Training Accuracy: 0.859375 Loss: 0.007181393448263407\n",
      "Iteration: 12900 Training Accuracy: 0.890625 Loss: 0.0047010211274027824\n",
      "Iteration: 12910 Training Accuracy: 0.8125 Loss: 0.00910753384232521\n",
      "Iteration: 12920 Training Accuracy: 0.90625 Loss: 0.005508699454367161\n",
      "Iteration: 12930 Training Accuracy: 0.828125 Loss: 0.008619008585810661\n",
      "Iteration: 12940 Training Accuracy: 0.828125 Loss: 0.007887011393904686\n",
      "Iteration: 12950 Training Accuracy: 0.859375 Loss: 0.008702283725142479\n",
      "Iteration: 12960 Training Accuracy: 0.875 Loss: 0.005680010188370943\n",
      "Iteration: 12970 Training Accuracy: 0.890625 Loss: 0.005237127188593149\n",
      "Iteration: 12980 Training Accuracy: 0.84375 Loss: 0.00674035819247365\n",
      "Iteration: 12990 Training Accuracy: 0.90625 Loss: 0.006467120256274939\n",
      "Iteration: 13000 Training Accuracy: 0.890625 Loss: 0.004520546179264784\n",
      "Iteration: 13010 Training Accuracy: 0.84375 Loss: 0.007035184185951948\n",
      "Iteration: 13020 Training Accuracy: 0.828125 Loss: 0.007657254580408335\n",
      "Iteration: 13030 Training Accuracy: 0.890625 Loss: 0.005027634557336569\n",
      "Iteration: 13040 Training Accuracy: 0.859375 Loss: 0.00922122597694397\n",
      "Iteration: 13050 Training Accuracy: 0.796875 Loss: 0.009230249561369419\n",
      "Iteration: 13060 Training Accuracy: 0.921875 Loss: 0.004182349890470505\n",
      "Iteration: 13070 Training Accuracy: 0.859375 Loss: 0.006848996505141258\n",
      "Iteration: 13080 Training Accuracy: 0.71875 Loss: 0.011317206546664238\n",
      "Iteration: 13090 Training Accuracy: 0.921875 Loss: 0.00397277669981122\n",
      "Iteration: 13100 Training Accuracy: 0.828125 Loss: 0.008961455896496773\n",
      "Iteration: 13110 Training Accuracy: 0.828125 Loss: 0.009267325513064861\n",
      "Iteration: 13120 Training Accuracy: 0.859375 Loss: 0.006021252833306789\n",
      "Iteration: 13130 Training Accuracy: 0.859375 Loss: 0.006236100569367409\n",
      "Training Accuracy = 0.9375\n",
      "Validation Accuracy = 0\n",
      "epoch: 14\n",
      "Iteration: 13140 Training Accuracy: 0.921875 Loss: 0.00513731874525547\n",
      "Iteration: 13150 Training Accuracy: 0.84375 Loss: 0.006801149342209101\n",
      "Iteration: 13160 Training Accuracy: 0.875 Loss: 0.005666255950927734\n",
      "Iteration: 13170 Training Accuracy: 0.890625 Loss: 0.005536404438316822\n",
      "Iteration: 13180 Training Accuracy: 0.875 Loss: 0.007816417142748833\n",
      "Iteration: 13190 Training Accuracy: 0.859375 Loss: 0.006444534286856651\n",
      "Iteration: 13200 Training Accuracy: 0.765625 Loss: 0.009283168241381645\n",
      "Iteration: 13210 Training Accuracy: 0.8125 Loss: 0.010356668382883072\n",
      "Iteration: 13220 Training Accuracy: 0.828125 Loss: 0.0058795372024178505\n",
      "Iteration: 13230 Training Accuracy: 0.9375 Loss: 0.004778885282576084\n",
      "Iteration: 13240 Training Accuracy: 0.90625 Loss: 0.005918936338275671\n",
      "Iteration: 13250 Training Accuracy: 0.859375 Loss: 0.0077516064047813416\n",
      "Iteration: 13260 Training Accuracy: 0.890625 Loss: 0.004510756582021713\n",
      "Iteration: 13270 Training Accuracy: 0.890625 Loss: 0.004974866285920143\n",
      "Iteration: 13280 Training Accuracy: 0.859375 Loss: 0.005831245798617601\n",
      "Iteration: 13290 Training Accuracy: 0.859375 Loss: 0.00679912231862545\n",
      "Iteration: 13300 Training Accuracy: 0.890625 Loss: 0.005391082260757685\n",
      "Iteration: 13310 Training Accuracy: 0.921875 Loss: 0.0041532195173203945\n",
      "Iteration: 13320 Training Accuracy: 0.875 Loss: 0.006234468426555395\n",
      "Iteration: 13330 Training Accuracy: 0.8125 Loss: 0.007426049560308456\n",
      "Iteration: 13340 Training Accuracy: 0.875 Loss: 0.00565006397664547\n",
      "Iteration: 13350 Training Accuracy: 0.828125 Loss: 0.007653840817511082\n",
      "Iteration: 13360 Training Accuracy: 0.84375 Loss: 0.0065893977880477905\n",
      "Iteration: 13370 Training Accuracy: 0.859375 Loss: 0.005000868812203407\n",
      "Iteration: 13380 Training Accuracy: 0.859375 Loss: 0.004807169549167156\n",
      "Iteration: 13390 Training Accuracy: 0.828125 Loss: 0.008487215265631676\n",
      "Iteration: 13400 Training Accuracy: 0.859375 Loss: 0.00961049273610115\n",
      "Iteration: 13410 Training Accuracy: 0.84375 Loss: 0.006808493752032518\n",
      "Iteration: 13420 Training Accuracy: 0.921875 Loss: 0.005382176488637924\n",
      "Iteration: 13430 Training Accuracy: 0.84375 Loss: 0.005855719093233347\n",
      "Iteration: 13440 Training Accuracy: 0.890625 Loss: 0.006387766916304827\n",
      "Iteration: 13450 Training Accuracy: 0.75 Loss: 0.009150602854788303\n",
      "Iteration: 13460 Training Accuracy: 0.9375 Loss: 0.0033595392014831305\n",
      "Iteration: 13470 Training Accuracy: 0.90625 Loss: 0.004089327994734049\n",
      "Iteration: 13480 Training Accuracy: 0.9375 Loss: 0.0037011918611824512\n",
      "Iteration: 13490 Training Accuracy: 0.890625 Loss: 0.003940739668905735\n",
      "Iteration: 13500 Training Accuracy: 0.8125 Loss: 0.008572415448725224\n",
      "Iteration: 13510 Training Accuracy: 0.859375 Loss: 0.007684533949941397\n",
      "Iteration: 13520 Training Accuracy: 0.859375 Loss: 0.006394821684807539\n",
      "Iteration: 13530 Training Accuracy: 0.8125 Loss: 0.008860151283442974\n",
      "Iteration: 13540 Training Accuracy: 0.921875 Loss: 0.0036212357226759195\n",
      "Iteration: 13550 Training Accuracy: 0.921875 Loss: 0.0037248351145535707\n",
      "Iteration: 13560 Training Accuracy: 0.828125 Loss: 0.010266706347465515\n",
      "Iteration: 13570 Training Accuracy: 0.921875 Loss: 0.004440540913492441\n",
      "Iteration: 13580 Training Accuracy: 0.765625 Loss: 0.006650710944086313\n",
      "Iteration: 13590 Training Accuracy: 0.921875 Loss: 0.004436672665178776\n",
      "Iteration: 13600 Training Accuracy: 0.90625 Loss: 0.004211707040667534\n",
      "Iteration: 13610 Training Accuracy: 0.875 Loss: 0.00834130123257637\n",
      "Iteration: 13620 Training Accuracy: 0.828125 Loss: 0.008360418491065502\n",
      "Iteration: 13630 Training Accuracy: 0.90625 Loss: 0.004296750761568546\n",
      "Iteration: 13640 Training Accuracy: 0.875 Loss: 0.006620550528168678\n",
      "Iteration: 13650 Training Accuracy: 0.84375 Loss: 0.009054291993379593\n",
      "Iteration: 13660 Training Accuracy: 0.890625 Loss: 0.006591302342712879\n",
      "Iteration: 13670 Training Accuracy: 0.828125 Loss: 0.007974239066243172\n",
      "Iteration: 13680 Training Accuracy: 0.90625 Loss: 0.005143361631780863\n",
      "Iteration: 13690 Training Accuracy: 0.8125 Loss: 0.012487528845667839\n",
      "Iteration: 13700 Training Accuracy: 0.84375 Loss: 0.008552417159080505\n",
      "Iteration: 13710 Training Accuracy: 0.859375 Loss: 0.006416723132133484\n",
      "Iteration: 13720 Training Accuracy: 0.84375 Loss: 0.007961254566907883\n",
      "Iteration: 13730 Training Accuracy: 0.953125 Loss: 0.002823382383212447\n",
      "Iteration: 13740 Training Accuracy: 0.84375 Loss: 0.007641882169991732\n",
      "Iteration: 13750 Training Accuracy: 0.84375 Loss: 0.006613727658987045\n",
      "Iteration: 13760 Training Accuracy: 0.921875 Loss: 0.00520204845815897\n",
      "Iteration: 13770 Training Accuracy: 0.875 Loss: 0.0076134041883051395\n",
      "Iteration: 13780 Training Accuracy: 0.921875 Loss: 0.0043153539299964905\n",
      "Iteration: 13790 Training Accuracy: 0.765625 Loss: 0.008688051253557205\n",
      "Iteration: 13800 Training Accuracy: 0.8125 Loss: 0.008998392149806023\n",
      "Iteration: 13810 Training Accuracy: 0.84375 Loss: 0.007193639874458313\n",
      "Iteration: 13820 Training Accuracy: 0.84375 Loss: 0.006480017676949501\n",
      "Iteration: 13830 Training Accuracy: 0.921875 Loss: 0.004136888310313225\n",
      "Iteration: 13840 Training Accuracy: 0.953125 Loss: 0.0035935223568230867\n",
      "Iteration: 13850 Training Accuracy: 0.921875 Loss: 0.005166544578969479\n",
      "Iteration: 13860 Training Accuracy: 0.84375 Loss: 0.0069055682979524136\n",
      "Iteration: 13870 Training Accuracy: 0.828125 Loss: 0.008505967445671558\n",
      "Iteration: 13880 Training Accuracy: 0.921875 Loss: 0.005702565889805555\n",
      "Iteration: 13890 Training Accuracy: 0.828125 Loss: 0.007644863799214363\n",
      "Iteration: 13900 Training Accuracy: 0.875 Loss: 0.005846420302987099\n",
      "Iteration: 13910 Training Accuracy: 0.84375 Loss: 0.006639669183641672\n",
      "Iteration: 13920 Training Accuracy: 0.890625 Loss: 0.006375480443239212\n",
      "Iteration: 13930 Training Accuracy: 0.84375 Loss: 0.007366873323917389\n",
      "Iteration: 13940 Training Accuracy: 0.84375 Loss: 0.009487980045378208\n",
      "Iteration: 13950 Training Accuracy: 0.8125 Loss: 0.007079104892909527\n",
      "Iteration: 13960 Training Accuracy: 0.859375 Loss: 0.004794945940375328\n",
      "Iteration: 13970 Training Accuracy: 0.90625 Loss: 0.005220623686909676\n",
      "Iteration: 13980 Training Accuracy: 0.859375 Loss: 0.006464869249612093\n",
      "Iteration: 13990 Training Accuracy: 0.859375 Loss: 0.00864927377551794\n",
      "Iteration: 14000 Training Accuracy: 0.921875 Loss: 0.005688562989234924\n",
      "Iteration: 14010 Training Accuracy: 0.90625 Loss: 0.004859237000346184\n",
      "Iteration: 14020 Training Accuracy: 0.859375 Loss: 0.006927646696567535\n",
      "Iteration: 14030 Training Accuracy: 0.859375 Loss: 0.004582392517477274\n",
      "Iteration: 14040 Training Accuracy: 0.90625 Loss: 0.004341484047472477\n",
      "Iteration: 14050 Training Accuracy: 0.84375 Loss: 0.0066690025851130486\n",
      "Iteration: 14060 Training Accuracy: 0.859375 Loss: 0.006656869314610958\n",
      "Iteration: 14070 Training Accuracy: 0.78125 Loss: 0.006986957509070635\n",
      "Training Accuracy = 0.78125\n",
      "Validation Accuracy = 0\n",
      "epoch: 15\n",
      "Iteration: 14080 Training Accuracy: 0.8125 Loss: 0.010064302012324333\n",
      "Iteration: 14090 Training Accuracy: 0.9375 Loss: 0.00437841285020113\n",
      "Iteration: 14100 Training Accuracy: 0.875 Loss: 0.0052115218713879585\n",
      "Iteration: 14110 Training Accuracy: 0.796875 Loss: 0.009517564438283443\n",
      "Iteration: 14120 Training Accuracy: 0.765625 Loss: 0.009259916841983795\n",
      "Iteration: 14130 Training Accuracy: 0.859375 Loss: 0.006015436723828316\n",
      "Iteration: 14140 Training Accuracy: 0.890625 Loss: 0.00619433494284749\n",
      "Iteration: 14150 Training Accuracy: 0.859375 Loss: 0.007237537298351526\n",
      "Iteration: 14160 Training Accuracy: 0.921875 Loss: 0.004462026990950108\n",
      "Iteration: 14170 Training Accuracy: 0.84375 Loss: 0.008118831552565098\n",
      "Iteration: 14180 Training Accuracy: 0.859375 Loss: 0.006484068930149078\n",
      "Iteration: 14190 Training Accuracy: 0.921875 Loss: 0.004781080409884453\n",
      "Iteration: 14200 Training Accuracy: 0.796875 Loss: 0.00986071303486824\n",
      "Iteration: 14210 Training Accuracy: 0.78125 Loss: 0.010336392559111118\n",
      "Iteration: 14220 Training Accuracy: 0.765625 Loss: 0.009212572127580643\n",
      "Iteration: 14230 Training Accuracy: 0.84375 Loss: 0.007464123889803886\n",
      "Iteration: 14240 Training Accuracy: 0.828125 Loss: 0.009539356455206871\n",
      "Iteration: 14250 Training Accuracy: 0.84375 Loss: 0.008555266074836254\n",
      "Iteration: 14260 Training Accuracy: 0.828125 Loss: 0.0060125975869596004\n",
      "Iteration: 14270 Training Accuracy: 0.828125 Loss: 0.008831491693854332\n",
      "Iteration: 14280 Training Accuracy: 0.890625 Loss: 0.004895030055195093\n",
      "Iteration: 14290 Training Accuracy: 0.90625 Loss: 0.004635365679860115\n",
      "Iteration: 14300 Training Accuracy: 0.828125 Loss: 0.008662302047014236\n",
      "Iteration: 14310 Training Accuracy: 0.84375 Loss: 0.009132950566709042\n",
      "Iteration: 14320 Training Accuracy: 0.875 Loss: 0.006778756156563759\n",
      "Iteration: 14330 Training Accuracy: 0.90625 Loss: 0.004605449270457029\n",
      "Iteration: 14340 Training Accuracy: 0.9375 Loss: 0.003972696606069803\n",
      "Iteration: 14350 Training Accuracy: 0.859375 Loss: 0.0077837142162024975\n",
      "Iteration: 14360 Training Accuracy: 0.84375 Loss: 0.006202740129083395\n",
      "Iteration: 14370 Training Accuracy: 0.84375 Loss: 0.007449574768543243\n",
      "Iteration: 14380 Training Accuracy: 0.84375 Loss: 0.009242570027709007\n",
      "Iteration: 14390 Training Accuracy: 0.875 Loss: 0.00584676256403327\n",
      "Iteration: 14400 Training Accuracy: 0.84375 Loss: 0.005354339722543955\n",
      "Iteration: 14410 Training Accuracy: 0.921875 Loss: 0.0054570348002016544\n",
      "Iteration: 14420 Training Accuracy: 0.875 Loss: 0.007896935567259789\n",
      "Iteration: 14430 Training Accuracy: 0.734375 Loss: 0.00981216598302126\n",
      "Iteration: 14440 Training Accuracy: 0.8125 Loss: 0.007402507588267326\n",
      "Iteration: 14450 Training Accuracy: 0.8125 Loss: 0.00805162638425827\n",
      "Iteration: 14460 Training Accuracy: 0.78125 Loss: 0.01026176568120718\n",
      "Iteration: 14470 Training Accuracy: 0.8125 Loss: 0.006570125464349985\n",
      "Iteration: 14480 Training Accuracy: 0.828125 Loss: 0.007602598052471876\n",
      "Iteration: 14490 Training Accuracy: 0.859375 Loss: 0.0071648601442575455\n",
      "Iteration: 14500 Training Accuracy: 0.875 Loss: 0.007065105251967907\n",
      "Iteration: 14510 Training Accuracy: 0.90625 Loss: 0.004006938077509403\n",
      "Iteration: 14520 Training Accuracy: 0.90625 Loss: 0.005004301201552153\n",
      "Iteration: 14530 Training Accuracy: 0.984375 Loss: 0.0029063671827316284\n",
      "Iteration: 14540 Training Accuracy: 0.890625 Loss: 0.006327134557068348\n",
      "Iteration: 14550 Training Accuracy: 0.890625 Loss: 0.005406206473708153\n",
      "Iteration: 14560 Training Accuracy: 0.84375 Loss: 0.008342833258211613\n",
      "Iteration: 14570 Training Accuracy: 0.890625 Loss: 0.006604486610740423\n",
      "Iteration: 14580 Training Accuracy: 0.875 Loss: 0.005176082719117403\n",
      "Iteration: 14590 Training Accuracy: 0.875 Loss: 0.007640800904482603\n",
      "Iteration: 14600 Training Accuracy: 0.796875 Loss: 0.008080584928393364\n",
      "Iteration: 14610 Training Accuracy: 0.9375 Loss: 0.0036443567369133234\n",
      "Iteration: 14620 Training Accuracy: 0.859375 Loss: 0.005991365760564804\n",
      "Iteration: 14630 Training Accuracy: 0.84375 Loss: 0.0056311339139938354\n",
      "Iteration: 14640 Training Accuracy: 0.84375 Loss: 0.007851294241845608\n",
      "Iteration: 14650 Training Accuracy: 0.90625 Loss: 0.005660227499902248\n",
      "Iteration: 14660 Training Accuracy: 0.875 Loss: 0.00657262559980154\n",
      "Iteration: 14670 Training Accuracy: 0.890625 Loss: 0.006143205799162388\n",
      "Iteration: 14680 Training Accuracy: 0.90625 Loss: 0.004985139239579439\n",
      "Iteration: 14690 Training Accuracy: 0.765625 Loss: 0.007498914375901222\n",
      "Iteration: 14700 Training Accuracy: 0.859375 Loss: 0.0064653558656573296\n",
      "Iteration: 14710 Training Accuracy: 0.828125 Loss: 0.008342241868376732\n",
      "Iteration: 14720 Training Accuracy: 0.921875 Loss: 0.002907557412981987\n",
      "Iteration: 14730 Training Accuracy: 0.90625 Loss: 0.004062306135892868\n",
      "Iteration: 14740 Training Accuracy: 0.90625 Loss: 0.004343855194747448\n",
      "Iteration: 14750 Training Accuracy: 0.875 Loss: 0.008801409974694252\n",
      "Iteration: 14760 Training Accuracy: 0.96875 Loss: 0.002496662549674511\n",
      "Iteration: 14770 Training Accuracy: 0.90625 Loss: 0.0054372213780879974\n",
      "Iteration: 14780 Training Accuracy: 0.96875 Loss: 0.0021672712173312902\n",
      "Iteration: 14790 Training Accuracy: 0.859375 Loss: 0.008186452090740204\n",
      "Iteration: 14800 Training Accuracy: 0.84375 Loss: 0.008503368124365807\n",
      "Iteration: 14810 Training Accuracy: 0.875 Loss: 0.0063073644414544106\n",
      "Iteration: 14820 Training Accuracy: 0.921875 Loss: 0.0043546948581933975\n",
      "Iteration: 14830 Training Accuracy: 0.8125 Loss: 0.007828429341316223\n",
      "Iteration: 14840 Training Accuracy: 0.84375 Loss: 0.007335145026445389\n",
      "Iteration: 14850 Training Accuracy: 0.875 Loss: 0.003933236934244633\n",
      "Iteration: 14860 Training Accuracy: 0.90625 Loss: 0.0071539622731506824\n",
      "Iteration: 14870 Training Accuracy: 0.859375 Loss: 0.006564510054886341\n",
      "Iteration: 14880 Training Accuracy: 0.8125 Loss: 0.008355623111128807\n",
      "Iteration: 14890 Training Accuracy: 0.921875 Loss: 0.005001555662602186\n",
      "Iteration: 14900 Training Accuracy: 0.875 Loss: 0.008821122348308563\n",
      "Iteration: 14910 Training Accuracy: 0.890625 Loss: 0.004548120778053999\n",
      "Iteration: 14920 Training Accuracy: 0.90625 Loss: 0.006362195126712322\n",
      "Iteration: 14930 Training Accuracy: 0.9375 Loss: 0.0037303525023162365\n",
      "Iteration: 14940 Training Accuracy: 0.890625 Loss: 0.006431118119508028\n",
      "Iteration: 14950 Training Accuracy: 0.90625 Loss: 0.004114282317459583\n",
      "Iteration: 14960 Training Accuracy: 0.859375 Loss: 0.005960063077509403\n",
      "Iteration: 14970 Training Accuracy: 0.859375 Loss: 0.0068680522963404655\n",
      "Iteration: 14980 Training Accuracy: 0.84375 Loss: 0.007069002371281385\n",
      "Iteration: 14990 Training Accuracy: 0.921875 Loss: 0.006097742356359959\n",
      "Iteration: 15000 Training Accuracy: 0.859375 Loss: 0.007896894589066505\n",
      "Training Accuracy = 0.875\n",
      "Validation Accuracy = 0\n",
      "epoch: 16\n",
      "Iteration: 15010 Training Accuracy: 0.796875 Loss: 0.010878787375986576\n",
      "Iteration: 15020 Training Accuracy: 0.796875 Loss: 0.011686539277434349\n",
      "Iteration: 15030 Training Accuracy: 0.828125 Loss: 0.006909966003149748\n",
      "Iteration: 15040 Training Accuracy: 0.84375 Loss: 0.0051059415563941\n",
      "Iteration: 15050 Training Accuracy: 0.875 Loss: 0.006327224895358086\n",
      "Iteration: 15060 Training Accuracy: 0.859375 Loss: 0.005464417859911919\n",
      "Iteration: 15070 Training Accuracy: 0.9375 Loss: 0.0040184264071285725\n",
      "Iteration: 15080 Training Accuracy: 0.921875 Loss: 0.004225010983645916\n",
      "Iteration: 15090 Training Accuracy: 0.9375 Loss: 0.003140048822388053\n",
      "Iteration: 15100 Training Accuracy: 0.84375 Loss: 0.00903976522386074\n",
      "Iteration: 15110 Training Accuracy: 0.765625 Loss: 0.010578894056379795\n",
      "Iteration: 15120 Training Accuracy: 0.90625 Loss: 0.005248410627245903\n",
      "Iteration: 15130 Training Accuracy: 0.859375 Loss: 0.005768275819718838\n",
      "Iteration: 15140 Training Accuracy: 0.890625 Loss: 0.006459517404437065\n",
      "Iteration: 15150 Training Accuracy: 0.90625 Loss: 0.005684696137905121\n",
      "Iteration: 15160 Training Accuracy: 0.875 Loss: 0.0049083055928349495\n",
      "Iteration: 15170 Training Accuracy: 0.8125 Loss: 0.007497526239603758\n",
      "Iteration: 15180 Training Accuracy: 0.921875 Loss: 0.005095801316201687\n",
      "Iteration: 15190 Training Accuracy: 0.90625 Loss: 0.006323430687189102\n",
      "Iteration: 15200 Training Accuracy: 0.9375 Loss: 0.00344268255867064\n",
      "Iteration: 15210 Training Accuracy: 0.890625 Loss: 0.00606607086956501\n",
      "Iteration: 15220 Training Accuracy: 0.84375 Loss: 0.006196173839271069\n",
      "Iteration: 15230 Training Accuracy: 0.796875 Loss: 0.010092733427882195\n",
      "Iteration: 15240 Training Accuracy: 0.875 Loss: 0.0058317286893725395\n",
      "Iteration: 15250 Training Accuracy: 0.890625 Loss: 0.004894674755632877\n",
      "Iteration: 15260 Training Accuracy: 0.84375 Loss: 0.005245216656476259\n",
      "Iteration: 15270 Training Accuracy: 0.84375 Loss: 0.006722602061927319\n",
      "Iteration: 15280 Training Accuracy: 0.859375 Loss: 0.009221876040101051\n",
      "Iteration: 15290 Training Accuracy: 0.859375 Loss: 0.007913593202829361\n",
      "Iteration: 15300 Training Accuracy: 0.890625 Loss: 0.007137724198400974\n",
      "Iteration: 15310 Training Accuracy: 0.890625 Loss: 0.0044675590470433235\n",
      "Iteration: 15320 Training Accuracy: 0.828125 Loss: 0.007852618582546711\n",
      "Iteration: 15330 Training Accuracy: 0.859375 Loss: 0.007115542888641357\n",
      "Iteration: 15340 Training Accuracy: 0.96875 Loss: 0.0037327921018004417\n",
      "Iteration: 15350 Training Accuracy: 0.90625 Loss: 0.004448220133781433\n",
      "Iteration: 15360 Training Accuracy: 0.828125 Loss: 0.007996558211743832\n",
      "Iteration: 15370 Training Accuracy: 0.875 Loss: 0.0036099781282246113\n",
      "Iteration: 15380 Training Accuracy: 0.859375 Loss: 0.005869830958545208\n",
      "Iteration: 15390 Training Accuracy: 0.828125 Loss: 0.009601952508091927\n",
      "Iteration: 15400 Training Accuracy: 0.90625 Loss: 0.0055559538304805756\n",
      "Iteration: 15410 Training Accuracy: 0.84375 Loss: 0.008458117954432964\n",
      "Iteration: 15420 Training Accuracy: 0.859375 Loss: 0.007466708775609732\n",
      "Iteration: 15430 Training Accuracy: 0.78125 Loss: 0.00820495467633009\n",
      "Iteration: 15440 Training Accuracy: 0.859375 Loss: 0.007927819155156612\n",
      "Iteration: 15450 Training Accuracy: 0.8125 Loss: 0.008538943715393543\n",
      "Iteration: 15460 Training Accuracy: 0.828125 Loss: 0.00830506905913353\n",
      "Iteration: 15470 Training Accuracy: 0.890625 Loss: 0.004741865210235119\n",
      "Iteration: 15480 Training Accuracy: 0.921875 Loss: 0.0032273484393954277\n",
      "Iteration: 15490 Training Accuracy: 0.890625 Loss: 0.004909413866698742\n",
      "Iteration: 15500 Training Accuracy: 0.9375 Loss: 0.0040807235054671764\n",
      "Iteration: 15510 Training Accuracy: 0.890625 Loss: 0.0057043940760195255\n",
      "Iteration: 15520 Training Accuracy: 0.828125 Loss: 0.008677864447236061\n",
      "Iteration: 15530 Training Accuracy: 0.859375 Loss: 0.006305725779384375\n",
      "Iteration: 15540 Training Accuracy: 0.90625 Loss: 0.004491360858082771\n",
      "Iteration: 15550 Training Accuracy: 0.90625 Loss: 0.004667682107537985\n",
      "Iteration: 15560 Training Accuracy: 0.875 Loss: 0.008111467584967613\n",
      "Iteration: 15570 Training Accuracy: 0.90625 Loss: 0.003663998795673251\n",
      "Iteration: 15580 Training Accuracy: 0.859375 Loss: 0.00692395307123661\n",
      "Iteration: 15590 Training Accuracy: 0.921875 Loss: 0.005705187562853098\n",
      "Iteration: 15600 Training Accuracy: 0.875 Loss: 0.005272937938570976\n",
      "Iteration: 15610 Training Accuracy: 0.90625 Loss: 0.004642882850021124\n",
      "Iteration: 15620 Training Accuracy: 0.8125 Loss: 0.00867263413965702\n",
      "Iteration: 15630 Training Accuracy: 0.90625 Loss: 0.005477152299135923\n",
      "Iteration: 15640 Training Accuracy: 0.921875 Loss: 0.003957267850637436\n",
      "Iteration: 15650 Training Accuracy: 0.875 Loss: 0.006262803450226784\n",
      "Iteration: 15660 Training Accuracy: 0.859375 Loss: 0.006238984875380993\n",
      "Iteration: 15670 Training Accuracy: 0.90625 Loss: 0.00559031730517745\n",
      "Iteration: 15680 Training Accuracy: 0.875 Loss: 0.009545769542455673\n",
      "Iteration: 15690 Training Accuracy: 0.90625 Loss: 0.0055752890184521675\n",
      "Iteration: 15700 Training Accuracy: 0.84375 Loss: 0.006964698899537325\n",
      "Iteration: 15710 Training Accuracy: 0.828125 Loss: 0.008397001773118973\n",
      "Iteration: 15720 Training Accuracy: 0.875 Loss: 0.006378560326993465\n",
      "Iteration: 15730 Training Accuracy: 0.921875 Loss: 0.004070281982421875\n",
      "Iteration: 15740 Training Accuracy: 0.859375 Loss: 0.005765930749475956\n",
      "Iteration: 15750 Training Accuracy: 0.921875 Loss: 0.003755869111046195\n",
      "Iteration: 15760 Training Accuracy: 0.84375 Loss: 0.004970991518348455\n",
      "Iteration: 15770 Training Accuracy: 0.890625 Loss: 0.005142500624060631\n",
      "Iteration: 15780 Training Accuracy: 0.859375 Loss: 0.004016363061964512\n",
      "Iteration: 15790 Training Accuracy: 0.8125 Loss: 0.005969558376818895\n",
      "Iteration: 15800 Training Accuracy: 0.828125 Loss: 0.006718214601278305\n",
      "Iteration: 15810 Training Accuracy: 0.875 Loss: 0.0071817561984062195\n",
      "Iteration: 15820 Training Accuracy: 0.8125 Loss: 0.0061647468246519566\n",
      "Iteration: 15830 Training Accuracy: 0.9375 Loss: 0.0046270452439785\n",
      "Iteration: 15840 Training Accuracy: 0.890625 Loss: 0.005086733493953943\n",
      "Iteration: 15850 Training Accuracy: 0.875 Loss: 0.007146493066102266\n",
      "Iteration: 15860 Training Accuracy: 0.890625 Loss: 0.00637600664049387\n",
      "Iteration: 15870 Training Accuracy: 0.90625 Loss: 0.005393459461629391\n",
      "Iteration: 15880 Training Accuracy: 0.859375 Loss: 0.007303521037101746\n",
      "Iteration: 15890 Training Accuracy: 0.859375 Loss: 0.006953258067369461\n",
      "Iteration: 15900 Training Accuracy: 0.90625 Loss: 0.0040366994217038155\n",
      "Iteration: 15910 Training Accuracy: 0.9375 Loss: 0.0030543687753379345\n",
      "Iteration: 15920 Training Accuracy: 0.84375 Loss: 0.009746978990733624\n",
      "Iteration: 15930 Training Accuracy: 0.90625 Loss: 0.0038494872860610485\n",
      "Iteration: 15940 Training Accuracy: 0.90625 Loss: 0.005536122713238001\n",
      "Training Accuracy = 0.9375\n",
      "Validation Accuracy = 0\n",
      "epoch: 17\n",
      "Iteration: 15950 Training Accuracy: 0.875 Loss: 0.007383633870631456\n",
      "Iteration: 15960 Training Accuracy: 0.84375 Loss: 0.007848221808671951\n",
      "Iteration: 15970 Training Accuracy: 0.890625 Loss: 0.005410104990005493\n",
      "Iteration: 15980 Training Accuracy: 0.765625 Loss: 0.007476034574210644\n",
      "Iteration: 15990 Training Accuracy: 0.875 Loss: 0.005716462153941393\n",
      "Iteration: 16000 Training Accuracy: 0.828125 Loss: 0.006884397938847542\n",
      "Iteration: 16010 Training Accuracy: 0.921875 Loss: 0.0045660436153411865\n",
      "Iteration: 16020 Training Accuracy: 0.828125 Loss: 0.00622565159574151\n",
      "Iteration: 16030 Training Accuracy: 0.921875 Loss: 0.004155552014708519\n",
      "Iteration: 16040 Training Accuracy: 0.90625 Loss: 0.006393447518348694\n",
      "Iteration: 16050 Training Accuracy: 0.9375 Loss: 0.0047147213481366634\n",
      "Iteration: 16060 Training Accuracy: 0.890625 Loss: 0.004429805092513561\n",
      "Iteration: 16070 Training Accuracy: 0.890625 Loss: 0.005862957797944546\n",
      "Iteration: 16080 Training Accuracy: 0.859375 Loss: 0.006913147866725922\n",
      "Iteration: 16090 Training Accuracy: 0.921875 Loss: 0.0036932763177901506\n",
      "Iteration: 16100 Training Accuracy: 0.859375 Loss: 0.006489851046353579\n",
      "Iteration: 16110 Training Accuracy: 0.828125 Loss: 0.008192434906959534\n",
      "Iteration: 16120 Training Accuracy: 0.84375 Loss: 0.007002479396760464\n",
      "Iteration: 16130 Training Accuracy: 0.875 Loss: 0.006411092355847359\n",
      "Iteration: 16140 Training Accuracy: 0.859375 Loss: 0.007512036710977554\n",
      "Iteration: 16150 Training Accuracy: 0.953125 Loss: 0.002807983662933111\n",
      "Iteration: 16160 Training Accuracy: 0.921875 Loss: 0.0046898252330720425\n",
      "Iteration: 16170 Training Accuracy: 0.921875 Loss: 0.004513567313551903\n",
      "Iteration: 16180 Training Accuracy: 0.84375 Loss: 0.008906066417694092\n",
      "Iteration: 16190 Training Accuracy: 0.859375 Loss: 0.006154879927635193\n",
      "Iteration: 16200 Training Accuracy: 0.859375 Loss: 0.006566285155713558\n",
      "Iteration: 16210 Training Accuracy: 0.9375 Loss: 0.0030226914677768946\n",
      "Iteration: 16220 Training Accuracy: 0.921875 Loss: 0.005398528650403023\n",
      "Iteration: 16230 Training Accuracy: 0.796875 Loss: 0.010458956472575665\n",
      "Iteration: 16240 Training Accuracy: 0.859375 Loss: 0.0050496626645326614\n",
      "Iteration: 16250 Training Accuracy: 0.921875 Loss: 0.0039011426270008087\n",
      "Iteration: 16260 Training Accuracy: 0.890625 Loss: 0.0053737713024020195\n",
      "Iteration: 16270 Training Accuracy: 0.859375 Loss: 0.006470230408012867\n",
      "Iteration: 16280 Training Accuracy: 0.875 Loss: 0.005830761045217514\n",
      "Iteration: 16290 Training Accuracy: 0.8125 Loss: 0.006758606992661953\n",
      "Iteration: 16300 Training Accuracy: 0.890625 Loss: 0.004171736538410187\n",
      "Iteration: 16310 Training Accuracy: 0.90625 Loss: 0.004662356339395046\n",
      "Iteration: 16320 Training Accuracy: 0.875 Loss: 0.007356463931500912\n",
      "Iteration: 16330 Training Accuracy: 0.84375 Loss: 0.007972748950123787\n",
      "Iteration: 16340 Training Accuracy: 0.859375 Loss: 0.006835872773081064\n",
      "Iteration: 16350 Training Accuracy: 0.9375 Loss: 0.0047378018498420715\n",
      "Iteration: 16360 Training Accuracy: 0.90625 Loss: 0.00464265514165163\n",
      "Iteration: 16370 Training Accuracy: 0.890625 Loss: 0.0064934734255075455\n",
      "Iteration: 16380 Training Accuracy: 0.890625 Loss: 0.0038022641092538834\n",
      "Iteration: 16390 Training Accuracy: 0.828125 Loss: 0.006156119983643293\n",
      "Iteration: 16400 Training Accuracy: 0.859375 Loss: 0.007221082225441933\n",
      "Iteration: 16410 Training Accuracy: 0.828125 Loss: 0.00679303053766489\n",
      "Iteration: 16420 Training Accuracy: 0.84375 Loss: 0.009823521599173546\n",
      "Iteration: 16430 Training Accuracy: 0.859375 Loss: 0.006818460766226053\n",
      "Iteration: 16440 Training Accuracy: 0.921875 Loss: 0.00315698329359293\n",
      "Iteration: 16450 Training Accuracy: 0.84375 Loss: 0.00800204649567604\n",
      "Iteration: 16460 Training Accuracy: 0.78125 Loss: 0.010082565248012543\n",
      "Iteration: 16470 Training Accuracy: 0.84375 Loss: 0.006840849295258522\n",
      "Iteration: 16480 Training Accuracy: 0.890625 Loss: 0.004979822784662247\n",
      "Iteration: 16490 Training Accuracy: 0.84375 Loss: 0.007943699136376381\n",
      "Iteration: 16500 Training Accuracy: 0.8125 Loss: 0.010743312537670135\n",
      "Iteration: 16510 Training Accuracy: 0.921875 Loss: 0.0058337729424238205\n",
      "Iteration: 16520 Training Accuracy: 0.90625 Loss: 0.004860071465373039\n",
      "Iteration: 16530 Training Accuracy: 0.9375 Loss: 0.0030676720198243856\n",
      "Iteration: 16540 Training Accuracy: 0.890625 Loss: 0.0056261708959937096\n",
      "Iteration: 16550 Training Accuracy: 0.875 Loss: 0.006781791336834431\n",
      "Iteration: 16560 Training Accuracy: 0.828125 Loss: 0.00611252011731267\n",
      "Iteration: 16570 Training Accuracy: 0.78125 Loss: 0.01310061290860176\n",
      "Iteration: 16580 Training Accuracy: 0.921875 Loss: 0.003962627612054348\n",
      "Iteration: 16590 Training Accuracy: 0.9375 Loss: 0.00328354025259614\n",
      "Iteration: 16600 Training Accuracy: 0.859375 Loss: 0.004982721991837025\n",
      "Iteration: 16610 Training Accuracy: 0.859375 Loss: 0.008384615182876587\n",
      "Iteration: 16620 Training Accuracy: 0.90625 Loss: 0.0047755674459040165\n",
      "Iteration: 16630 Training Accuracy: 0.875 Loss: 0.006010399200022221\n",
      "Iteration: 16640 Training Accuracy: 0.90625 Loss: 0.0044003636576235294\n",
      "Iteration: 16650 Training Accuracy: 0.796875 Loss: 0.009984249249100685\n",
      "Iteration: 16660 Training Accuracy: 0.90625 Loss: 0.005048750899732113\n",
      "Iteration: 16670 Training Accuracy: 0.78125 Loss: 0.008264755830168724\n",
      "Iteration: 16680 Training Accuracy: 0.890625 Loss: 0.005175136029720306\n",
      "Iteration: 16690 Training Accuracy: 0.875 Loss: 0.006551684811711311\n",
      "Iteration: 16700 Training Accuracy: 0.875 Loss: 0.00582316517829895\n",
      "Iteration: 16710 Training Accuracy: 0.84375 Loss: 0.00849105417728424\n",
      "Iteration: 16720 Training Accuracy: 0.8125 Loss: 0.007219862658530474\n",
      "Iteration: 16730 Training Accuracy: 0.84375 Loss: 0.008442875929176807\n",
      "Iteration: 16740 Training Accuracy: 0.90625 Loss: 0.004754361696541309\n",
      "Iteration: 16750 Training Accuracy: 0.84375 Loss: 0.007948659360408783\n",
      "Iteration: 16760 Training Accuracy: 0.875 Loss: 0.006071326322853565\n",
      "Iteration: 16770 Training Accuracy: 0.9375 Loss: 0.004303619731217623\n",
      "Iteration: 16780 Training Accuracy: 0.890625 Loss: 0.006557919085025787\n",
      "Iteration: 16790 Training Accuracy: 0.859375 Loss: 0.006165315397083759\n",
      "Iteration: 16800 Training Accuracy: 0.796875 Loss: 0.007154947146773338\n",
      "Iteration: 16810 Training Accuracy: 0.890625 Loss: 0.003949047531932592\n",
      "Iteration: 16820 Training Accuracy: 0.890625 Loss: 0.005631650798022747\n",
      "Iteration: 16830 Training Accuracy: 0.828125 Loss: 0.006735994480550289\n",
      "Iteration: 16840 Training Accuracy: 0.890625 Loss: 0.007039953023195267\n",
      "Iteration: 16850 Training Accuracy: 0.890625 Loss: 0.005219096317887306\n",
      "Iteration: 16860 Training Accuracy: 0.890625 Loss: 0.003809511661529541\n",
      "Iteration: 16870 Training Accuracy: 0.828125 Loss: 0.010197361931204796\n",
      "Iteration: 16880 Training Accuracy: 0.875 Loss: 0.00486966036260128\n",
      "Training Accuracy = 0.84375\n",
      "Validation Accuracy = 0\n",
      "epoch: 18\n",
      "Iteration: 16890 Training Accuracy: 0.90625 Loss: 0.004116348922252655\n",
      "Iteration: 16900 Training Accuracy: 0.875 Loss: 0.005564966704696417\n",
      "Iteration: 16910 Training Accuracy: 0.875 Loss: 0.004390306770801544\n",
      "Iteration: 16920 Training Accuracy: 0.921875 Loss: 0.004717105068266392\n",
      "Iteration: 16930 Training Accuracy: 0.84375 Loss: 0.006812752224504948\n",
      "Iteration: 16940 Training Accuracy: 0.875 Loss: 0.006003221031278372\n",
      "Iteration: 16950 Training Accuracy: 0.84375 Loss: 0.007411513943225145\n",
      "Iteration: 16960 Training Accuracy: 0.90625 Loss: 0.0038972801994532347\n",
      "Iteration: 16970 Training Accuracy: 0.8125 Loss: 0.009669468738138676\n",
      "Iteration: 16980 Training Accuracy: 0.90625 Loss: 0.006236670538783073\n",
      "Iteration: 16990 Training Accuracy: 0.8125 Loss: 0.00834277831017971\n",
      "Iteration: 17000 Training Accuracy: 0.890625 Loss: 0.004657762125134468\n",
      "Iteration: 17010 Training Accuracy: 0.890625 Loss: 0.006643414497375488\n",
      "Iteration: 17020 Training Accuracy: 0.90625 Loss: 0.004391576163470745\n",
      "Iteration: 17030 Training Accuracy: 0.9375 Loss: 0.00479242904111743\n",
      "Iteration: 17040 Training Accuracy: 0.828125 Loss: 0.008541738614439964\n",
      "Iteration: 17050 Training Accuracy: 0.90625 Loss: 0.004672403912991285\n",
      "Iteration: 17060 Training Accuracy: 0.890625 Loss: 0.005978320725262165\n",
      "Iteration: 17070 Training Accuracy: 0.84375 Loss: 0.006365921813994646\n",
      "Iteration: 17080 Training Accuracy: 0.890625 Loss: 0.004489402752369642\n",
      "Iteration: 17090 Training Accuracy: 0.90625 Loss: 0.005462350323796272\n",
      "Iteration: 17100 Training Accuracy: 0.859375 Loss: 0.00889725610613823\n",
      "Iteration: 17110 Training Accuracy: 0.875 Loss: 0.006102991756051779\n",
      "Iteration: 17120 Training Accuracy: 0.90625 Loss: 0.0047995178028941154\n",
      "Iteration: 17130 Training Accuracy: 0.84375 Loss: 0.005288435146212578\n",
      "Iteration: 17140 Training Accuracy: 0.859375 Loss: 0.0071089486591517925\n",
      "Iteration: 17150 Training Accuracy: 0.859375 Loss: 0.008105820044875145\n",
      "Iteration: 17160 Training Accuracy: 0.875 Loss: 0.006028603762388229\n",
      "Iteration: 17170 Training Accuracy: 0.921875 Loss: 0.0038297646678984165\n",
      "Iteration: 17180 Training Accuracy: 0.84375 Loss: 0.006874958053231239\n",
      "Iteration: 17190 Training Accuracy: 0.875 Loss: 0.005454643629491329\n",
      "Iteration: 17200 Training Accuracy: 0.875 Loss: 0.005179659463465214\n",
      "Iteration: 17210 Training Accuracy: 0.953125 Loss: 0.0044970763847231865\n",
      "Iteration: 17220 Training Accuracy: 0.828125 Loss: 0.008228967897593975\n",
      "Iteration: 17230 Training Accuracy: 0.90625 Loss: 0.005164780654013157\n",
      "Iteration: 17240 Training Accuracy: 0.78125 Loss: 0.008930834010243416\n",
      "Iteration: 17250 Training Accuracy: 0.890625 Loss: 0.004115833900868893\n",
      "Iteration: 17260 Training Accuracy: 0.84375 Loss: 0.006898039020597935\n",
      "Iteration: 17270 Training Accuracy: 0.8125 Loss: 0.008629863150417805\n",
      "Iteration: 17280 Training Accuracy: 0.921875 Loss: 0.003979114815592766\n",
      "Iteration: 17290 Training Accuracy: 0.921875 Loss: 0.006034305319190025\n",
      "Iteration: 17300 Training Accuracy: 0.859375 Loss: 0.005908755585551262\n",
      "Iteration: 17310 Training Accuracy: 0.890625 Loss: 0.005166896618902683\n",
      "Iteration: 17320 Training Accuracy: 0.875 Loss: 0.006090951152145863\n",
      "Iteration: 17330 Training Accuracy: 0.90625 Loss: 0.005000861827284098\n",
      "Iteration: 17340 Training Accuracy: 0.84375 Loss: 0.007248695939779282\n",
      "Iteration: 17350 Training Accuracy: 0.84375 Loss: 0.008827040903270245\n",
      "Iteration: 17360 Training Accuracy: 0.84375 Loss: 0.005578186362981796\n",
      "Iteration: 17370 Training Accuracy: 0.890625 Loss: 0.006080172024667263\n",
      "Iteration: 17380 Training Accuracy: 0.921875 Loss: 0.004129963461309671\n",
      "Iteration: 17390 Training Accuracy: 0.890625 Loss: 0.0067950100637972355\n",
      "Iteration: 17400 Training Accuracy: 0.84375 Loss: 0.006988671608269215\n",
      "Iteration: 17410 Training Accuracy: 0.859375 Loss: 0.008695868775248528\n",
      "Iteration: 17420 Training Accuracy: 0.875 Loss: 0.006230154540389776\n",
      "Iteration: 17430 Training Accuracy: 0.9375 Loss: 0.006329601164907217\n",
      "Iteration: 17440 Training Accuracy: 0.90625 Loss: 0.004173589870333672\n",
      "Iteration: 17450 Training Accuracy: 0.875 Loss: 0.005258454475551844\n",
      "Iteration: 17460 Training Accuracy: 0.9375 Loss: 0.003432168625295162\n",
      "Iteration: 17470 Training Accuracy: 0.90625 Loss: 0.0037537876050919294\n",
      "Iteration: 17480 Training Accuracy: 0.78125 Loss: 0.007113513071089983\n",
      "Iteration: 17490 Training Accuracy: 0.90625 Loss: 0.005544258747249842\n",
      "Iteration: 17500 Training Accuracy: 0.859375 Loss: 0.004810687154531479\n",
      "Iteration: 17510 Training Accuracy: 0.953125 Loss: 0.004560681991279125\n",
      "Iteration: 17520 Training Accuracy: 0.90625 Loss: 0.004522295203059912\n",
      "Iteration: 17530 Training Accuracy: 0.828125 Loss: 0.008852166123688221\n",
      "Iteration: 17540 Training Accuracy: 0.859375 Loss: 0.006999725475907326\n",
      "Iteration: 17550 Training Accuracy: 0.90625 Loss: 0.006186244077980518\n",
      "Iteration: 17560 Training Accuracy: 0.875 Loss: 0.00636441120877862\n",
      "Iteration: 17570 Training Accuracy: 0.84375 Loss: 0.00856156274676323\n",
      "Iteration: 17580 Training Accuracy: 0.84375 Loss: 0.006823864299803972\n",
      "Iteration: 17590 Training Accuracy: 0.921875 Loss: 0.0048377495259046555\n",
      "Iteration: 17600 Training Accuracy: 0.875 Loss: 0.007551679387688637\n",
      "Iteration: 17610 Training Accuracy: 0.921875 Loss: 0.005229541100561619\n",
      "Iteration: 17620 Training Accuracy: 0.828125 Loss: 0.010526895523071289\n",
      "Iteration: 17630 Training Accuracy: 0.859375 Loss: 0.005261828191578388\n",
      "Iteration: 17640 Training Accuracy: 0.875 Loss: 0.00575845455750823\n",
      "Iteration: 17650 Training Accuracy: 0.828125 Loss: 0.0069584655575454235\n",
      "Iteration: 17660 Training Accuracy: 0.84375 Loss: 0.006184880156069994\n",
      "Iteration: 17670 Training Accuracy: 0.890625 Loss: 0.005409812554717064\n",
      "Iteration: 17680 Training Accuracy: 0.796875 Loss: 0.00846103299409151\n",
      "Iteration: 17690 Training Accuracy: 0.921875 Loss: 0.004164431244134903\n",
      "Iteration: 17700 Training Accuracy: 0.890625 Loss: 0.0056581683456897736\n",
      "Iteration: 17710 Training Accuracy: 0.890625 Loss: 0.00835452787578106\n",
      "Iteration: 17720 Training Accuracy: 0.84375 Loss: 0.0069761257618665695\n",
      "Iteration: 17730 Training Accuracy: 0.921875 Loss: 0.004469733219593763\n",
      "Iteration: 17740 Training Accuracy: 0.859375 Loss: 0.007581727579236031\n",
      "Iteration: 17750 Training Accuracy: 0.90625 Loss: 0.00506575359031558\n",
      "Iteration: 17760 Training Accuracy: 0.828125 Loss: 0.007385417819023132\n",
      "Iteration: 17770 Training Accuracy: 0.8125 Loss: 0.010009747929871082\n",
      "Iteration: 17780 Training Accuracy: 0.90625 Loss: 0.004812413826584816\n",
      "Iteration: 17790 Training Accuracy: 0.859375 Loss: 0.005114610306918621\n",
      "Iteration: 17800 Training Accuracy: 0.890625 Loss: 0.008692465722560883\n",
      "Iteration: 17810 Training Accuracy: 0.859375 Loss: 0.007096484303474426\n",
      "Iteration: 17820 Training Accuracy: 0.859375 Loss: 0.006082294508814812\n",
      "Training Accuracy = 0.875\n",
      "Validation Accuracy = 0\n",
      "epoch: 19\n",
      "Iteration: 17830 Training Accuracy: 0.859375 Loss: 0.006434557493776083\n",
      "Iteration: 17840 Training Accuracy: 0.84375 Loss: 0.005476999096572399\n",
      "Iteration: 17850 Training Accuracy: 0.859375 Loss: 0.007179254200309515\n",
      "Iteration: 17860 Training Accuracy: 0.859375 Loss: 0.00755208358168602\n",
      "Iteration: 17870 Training Accuracy: 0.90625 Loss: 0.006661318242549896\n",
      "Iteration: 17880 Training Accuracy: 0.90625 Loss: 0.004448171705007553\n",
      "Iteration: 17890 Training Accuracy: 0.84375 Loss: 0.009374662302434444\n",
      "Iteration: 17900 Training Accuracy: 0.875 Loss: 0.008263329975306988\n",
      "Iteration: 17910 Training Accuracy: 0.859375 Loss: 0.006970221176743507\n",
      "Iteration: 17920 Training Accuracy: 0.8125 Loss: 0.006383505649864674\n",
      "Iteration: 17930 Training Accuracy: 0.890625 Loss: 0.004980969708412886\n",
      "Iteration: 17940 Training Accuracy: 0.84375 Loss: 0.007125594653189182\n",
      "Iteration: 17950 Training Accuracy: 0.859375 Loss: 0.005816621705889702\n",
      "Iteration: 17960 Training Accuracy: 0.859375 Loss: 0.008419904857873917\n",
      "Iteration: 17970 Training Accuracy: 0.921875 Loss: 0.005259769968688488\n",
      "Iteration: 17980 Training Accuracy: 0.84375 Loss: 0.005944880656898022\n",
      "Iteration: 17990 Training Accuracy: 0.90625 Loss: 0.004735125228762627\n",
      "Iteration: 18000 Training Accuracy: 0.953125 Loss: 0.0029781621415168047\n",
      "Iteration: 18010 Training Accuracy: 0.859375 Loss: 0.0056789955124258995\n",
      "Iteration: 18020 Training Accuracy: 0.890625 Loss: 0.0065705981105566025\n",
      "Iteration: 18030 Training Accuracy: 0.921875 Loss: 0.0038033502642065287\n",
      "Iteration: 18040 Training Accuracy: 0.890625 Loss: 0.00470655458047986\n",
      "Iteration: 18050 Training Accuracy: 0.84375 Loss: 0.006317182444036007\n",
      "Iteration: 18060 Training Accuracy: 0.890625 Loss: 0.004954323638230562\n",
      "Iteration: 18070 Training Accuracy: 0.890625 Loss: 0.004377285484224558\n",
      "Iteration: 18080 Training Accuracy: 0.890625 Loss: 0.006225794553756714\n",
      "Iteration: 18090 Training Accuracy: 0.84375 Loss: 0.008001044392585754\n",
      "Iteration: 18100 Training Accuracy: 0.921875 Loss: 0.005545986816287041\n",
      "Iteration: 18110 Training Accuracy: 0.90625 Loss: 0.00436934269964695\n",
      "Iteration: 18120 Training Accuracy: 0.828125 Loss: 0.008577952161431313\n",
      "Iteration: 18130 Training Accuracy: 0.8125 Loss: 0.008008383214473724\n",
      "Iteration: 18140 Training Accuracy: 0.828125 Loss: 0.008749512955546379\n",
      "Iteration: 18150 Training Accuracy: 0.875 Loss: 0.003930648788809776\n",
      "Iteration: 18160 Training Accuracy: 0.828125 Loss: 0.008228517137467861\n",
      "Iteration: 18170 Training Accuracy: 0.921875 Loss: 0.005707026459276676\n",
      "Iteration: 18180 Training Accuracy: 0.90625 Loss: 0.004478731192648411\n",
      "Iteration: 18190 Training Accuracy: 0.890625 Loss: 0.005258091725409031\n",
      "Iteration: 18200 Training Accuracy: 0.84375 Loss: 0.008499490097165108\n",
      "Iteration: 18210 Training Accuracy: 0.859375 Loss: 0.007577852346003056\n",
      "Iteration: 18220 Training Accuracy: 0.875 Loss: 0.006375525146722794\n",
      "Iteration: 18230 Training Accuracy: 0.90625 Loss: 0.004575353115797043\n",
      "Iteration: 18240 Training Accuracy: 0.890625 Loss: 0.005384653806686401\n",
      "Iteration: 18250 Training Accuracy: 0.875 Loss: 0.00613211328163743\n",
      "Iteration: 18260 Training Accuracy: 0.890625 Loss: 0.005163016263395548\n",
      "Iteration: 18270 Training Accuracy: 0.84375 Loss: 0.006983745843172073\n",
      "Iteration: 18280 Training Accuracy: 0.96875 Loss: 0.00297518540173769\n",
      "Iteration: 18290 Training Accuracy: 0.890625 Loss: 0.005378258414566517\n",
      "Iteration: 18300 Training Accuracy: 0.84375 Loss: 0.008225192315876484\n",
      "Iteration: 18310 Training Accuracy: 0.78125 Loss: 0.009514905512332916\n",
      "Iteration: 18320 Training Accuracy: 0.875 Loss: 0.005316200666129589\n",
      "Iteration: 18330 Training Accuracy: 0.90625 Loss: 0.004616931080818176\n",
      "Iteration: 18340 Training Accuracy: 0.890625 Loss: 0.008505563251674175\n",
      "Iteration: 18350 Training Accuracy: 0.84375 Loss: 0.007775522768497467\n",
      "Iteration: 18360 Training Accuracy: 0.890625 Loss: 0.005580767057836056\n",
      "Iteration: 18370 Training Accuracy: 0.90625 Loss: 0.005664103664457798\n",
      "Iteration: 18380 Training Accuracy: 0.828125 Loss: 0.010566171258687973\n",
      "Iteration: 18390 Training Accuracy: 0.875 Loss: 0.006815087981522083\n",
      "Iteration: 18400 Training Accuracy: 0.875 Loss: 0.004751039668917656\n",
      "Iteration: 18410 Training Accuracy: 0.875 Loss: 0.006100816186517477\n",
      "Iteration: 18420 Training Accuracy: 0.90625 Loss: 0.004402494989335537\n",
      "Iteration: 18430 Training Accuracy: 0.953125 Loss: 0.003753922414034605\n",
      "Iteration: 18440 Training Accuracy: 0.828125 Loss: 0.005913402885198593\n",
      "Iteration: 18450 Training Accuracy: 0.875 Loss: 0.004148736596107483\n",
      "Iteration: 18460 Training Accuracy: 0.859375 Loss: 0.007528819143772125\n",
      "Iteration: 18470 Training Accuracy: 0.90625 Loss: 0.0027935157995671034\n",
      "Iteration: 18480 Training Accuracy: 0.859375 Loss: 0.006866311654448509\n",
      "Iteration: 18490 Training Accuracy: 0.84375 Loss: 0.008217979222536087\n",
      "Iteration: 18500 Training Accuracy: 0.890625 Loss: 0.006090574897825718\n",
      "Iteration: 18510 Training Accuracy: 0.859375 Loss: 0.007110023871064186\n",
      "Iteration: 18520 Training Accuracy: 0.953125 Loss: 0.0036813179031014442\n",
      "Iteration: 18530 Training Accuracy: 0.9375 Loss: 0.0038541199173778296\n",
      "Iteration: 18540 Training Accuracy: 0.875 Loss: 0.005564199294894934\n",
      "Iteration: 18550 Training Accuracy: 0.875 Loss: 0.006130150519311428\n",
      "Iteration: 18560 Training Accuracy: 0.859375 Loss: 0.007254474796354771\n",
      "Iteration: 18570 Training Accuracy: 0.890625 Loss: 0.0057332441210746765\n",
      "Iteration: 18580 Training Accuracy: 0.90625 Loss: 0.004599431529641151\n",
      "Iteration: 18590 Training Accuracy: 0.859375 Loss: 0.007840494625270367\n",
      "Iteration: 18600 Training Accuracy: 0.828125 Loss: 0.00792731810361147\n",
      "Iteration: 18610 Training Accuracy: 0.890625 Loss: 0.00621831277385354\n",
      "Iteration: 18620 Training Accuracy: 0.84375 Loss: 0.006330221891403198\n",
      "Iteration: 18630 Training Accuracy: 0.84375 Loss: 0.007673957385122776\n",
      "Iteration: 18640 Training Accuracy: 0.859375 Loss: 0.006043615750968456\n",
      "Iteration: 18650 Training Accuracy: 0.890625 Loss: 0.004350878763943911\n",
      "Iteration: 18660 Training Accuracy: 0.90625 Loss: 0.0037309059407562017\n",
      "Iteration: 18670 Training Accuracy: 0.859375 Loss: 0.005015004891902208\n",
      "Iteration: 18680 Training Accuracy: 0.84375 Loss: 0.007192216347903013\n",
      "Iteration: 18690 Training Accuracy: 0.921875 Loss: 0.0037960615009069443\n",
      "Iteration: 18700 Training Accuracy: 0.90625 Loss: 0.004250733647495508\n",
      "Iteration: 18710 Training Accuracy: 0.828125 Loss: 0.006993704941123724\n",
      "Iteration: 18720 Training Accuracy: 0.90625 Loss: 0.005853018723428249\n",
      "Iteration: 18730 Training Accuracy: 0.84375 Loss: 0.006969410926103592\n",
      "Iteration: 18740 Training Accuracy: 0.8125 Loss: 0.006878085434436798\n",
      "Iteration: 18750 Training Accuracy: 0.921875 Loss: 0.004355788230895996\n",
      "Iteration: 18760 Training Accuracy: 0.84375 Loss: 0.00837679672986269\n",
      "Training Accuracy = 0.84375\n",
      "Validation Accuracy = 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.8809333333333333\n",
      "Validation Accuracy = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RESNET18(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use res net \n",
    "model = RESNET18()\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.953125 Loss: 0.002456019865348935\n",
      "Iteration: 20 Training Accuracy: 0.96875 Loss: 0.0015930567169561982\n",
      "Iteration: 30 Training Accuracy: 0.953125 Loss: 0.0026750778779387474\n",
      "Iteration: 40 Training Accuracy: 0.890625 Loss: 0.004841750022023916\n",
      "Iteration: 50 Training Accuracy: 0.953125 Loss: 0.0018685010727494955\n",
      "Iteration: 60 Training Accuracy: 0.90625 Loss: 0.0038833809085190296\n",
      "Iteration: 70 Training Accuracy: 0.90625 Loss: 0.004598974250257015\n",
      "Iteration: 80 Training Accuracy: 0.96875 Loss: 0.0024193914141505957\n",
      "Iteration: 90 Training Accuracy: 0.859375 Loss: 0.0059529077261686325\n",
      "Iteration: 100 Training Accuracy: 0.984375 Loss: 0.0014757458120584488\n",
      "Iteration: 110 Training Accuracy: 0.953125 Loss: 0.0035659342538565397\n",
      "Iteration: 120 Training Accuracy: 0.96875 Loss: 0.001166941481642425\n",
      "Iteration: 130 Training Accuracy: 0.921875 Loss: 0.004291274584829807\n",
      "Iteration: 140 Training Accuracy: 0.90625 Loss: 0.00576234795153141\n",
      "Iteration: 150 Training Accuracy: 0.921875 Loss: 0.0032476289197802544\n",
      "Iteration: 160 Training Accuracy: 0.921875 Loss: 0.004114571493119001\n",
      "Iteration: 170 Training Accuracy: 0.921875 Loss: 0.004257319029420614\n",
      "Iteration: 180 Training Accuracy: 0.9375 Loss: 0.004392520524561405\n",
      "Iteration: 190 Training Accuracy: 0.921875 Loss: 0.0044859182089567184\n",
      "Iteration: 200 Training Accuracy: 0.921875 Loss: 0.003090551123023033\n",
      "Iteration: 210 Training Accuracy: 0.90625 Loss: 0.0031058716122061014\n",
      "Iteration: 220 Training Accuracy: 0.9375 Loss: 0.0025774892419576645\n",
      "Iteration: 230 Training Accuracy: 0.9375 Loss: 0.0033538085408508778\n",
      "Iteration: 240 Training Accuracy: 0.890625 Loss: 0.005011153407394886\n",
      "Iteration: 250 Training Accuracy: 0.984375 Loss: 0.0019870796240866184\n",
      "Iteration: 260 Training Accuracy: 0.953125 Loss: 0.002618672326207161\n",
      "Iteration: 270 Training Accuracy: 0.9375 Loss: 0.0026534688659012318\n",
      "Iteration: 280 Training Accuracy: 0.90625 Loss: 0.003654780564829707\n",
      "Iteration: 290 Training Accuracy: 0.921875 Loss: 0.003462345339357853\n",
      "Iteration: 300 Training Accuracy: 0.90625 Loss: 0.004024750553071499\n",
      "Iteration: 310 Training Accuracy: 0.890625 Loss: 0.00671965628862381\n",
      "Iteration: 320 Training Accuracy: 0.890625 Loss: 0.0035581421107053757\n",
      "Iteration: 330 Training Accuracy: 0.9375 Loss: 0.002829363802447915\n",
      "Iteration: 340 Training Accuracy: 0.953125 Loss: 0.0024634678848087788\n",
      "Iteration: 350 Training Accuracy: 0.9375 Loss: 0.0018601918127387762\n",
      "Iteration: 360 Training Accuracy: 0.9375 Loss: 0.0024022592697292566\n",
      "Iteration: 370 Training Accuracy: 0.921875 Loss: 0.0032264618203043938\n",
      "Iteration: 380 Training Accuracy: 0.90625 Loss: 0.005775604397058487\n",
      "Iteration: 390 Training Accuracy: 0.921875 Loss: 0.0036954754032194614\n",
      "Iteration: 400 Training Accuracy: 0.9375 Loss: 0.002680005971342325\n",
      "Iteration: 410 Training Accuracy: 0.953125 Loss: 0.0028497781604528427\n",
      "Iteration: 420 Training Accuracy: 0.890625 Loss: 0.0037886020727455616\n",
      "Iteration: 430 Training Accuracy: 0.953125 Loss: 0.0027047947514802217\n",
      "Iteration: 440 Training Accuracy: 0.953125 Loss: 0.0026583303697407246\n",
      "Iteration: 450 Training Accuracy: 0.9375 Loss: 0.004262453876435757\n",
      "Iteration: 460 Training Accuracy: 0.9375 Loss: 0.003354161512106657\n",
      "Iteration: 470 Training Accuracy: 0.953125 Loss: 0.0019031611736863852\n",
      "Iteration: 480 Training Accuracy: 0.96875 Loss: 0.001959743909537792\n",
      "Iteration: 490 Training Accuracy: 0.921875 Loss: 0.003420707304030657\n",
      "Iteration: 500 Training Accuracy: 0.9375 Loss: 0.0030336829368025064\n",
      "Iteration: 510 Training Accuracy: 0.953125 Loss: 0.003058929927647114\n",
      "Iteration: 520 Training Accuracy: 0.96875 Loss: 0.00223341747187078\n",
      "Iteration: 530 Training Accuracy: 0.90625 Loss: 0.0037874900735914707\n",
      "Iteration: 540 Training Accuracy: 0.96875 Loss: 0.001964235445484519\n",
      "Iteration: 550 Training Accuracy: 0.953125 Loss: 0.002400258556008339\n",
      "Iteration: 560 Training Accuracy: 0.921875 Loss: 0.002465250436216593\n",
      "Iteration: 570 Training Accuracy: 0.9375 Loss: 0.003262866288423538\n",
      "Iteration: 580 Training Accuracy: 0.9375 Loss: 0.004267099313437939\n",
      "Iteration: 590 Training Accuracy: 0.859375 Loss: 0.005480253137648106\n",
      "Iteration: 600 Training Accuracy: 0.984375 Loss: 0.0014317622408270836\n",
      "Iteration: 610 Training Accuracy: 0.921875 Loss: 0.002698745345696807\n",
      "Iteration: 620 Training Accuracy: 0.9375 Loss: 0.002740554278716445\n",
      "Iteration: 630 Training Accuracy: 0.921875 Loss: 0.0024737417697906494\n",
      "Iteration: 640 Training Accuracy: 0.953125 Loss: 0.0033684177324175835\n",
      "Iteration: 650 Training Accuracy: 1.0 Loss: 0.0008575451793149114\n",
      "Iteration: 660 Training Accuracy: 0.9375 Loss: 0.002409108681604266\n",
      "Iteration: 670 Training Accuracy: 0.921875 Loss: 0.002377308439463377\n",
      "Iteration: 680 Training Accuracy: 0.96875 Loss: 0.0029200948774814606\n",
      "Iteration: 690 Training Accuracy: 0.921875 Loss: 0.00386149063706398\n",
      "Iteration: 700 Training Accuracy: 0.9375 Loss: 0.002904854714870453\n",
      "Iteration: 710 Training Accuracy: 0.984375 Loss: 0.0009845518507063389\n",
      "Iteration: 720 Training Accuracy: 0.890625 Loss: 0.004366964101791382\n",
      "Iteration: 730 Training Accuracy: 0.96875 Loss: 0.0021400938276201487\n",
      "Iteration: 740 Training Accuracy: 0.9375 Loss: 0.0029354700818657875\n",
      "Iteration: 750 Training Accuracy: 0.9375 Loss: 0.0032211344223469496\n",
      "Iteration: 760 Training Accuracy: 0.9375 Loss: 0.003977116197347641\n",
      "Iteration: 770 Training Accuracy: 0.875 Loss: 0.004474559333175421\n",
      "Iteration: 780 Training Accuracy: 0.953125 Loss: 0.0023084329441189766\n",
      "Iteration: 790 Training Accuracy: 0.953125 Loss: 0.0026034063193947077\n",
      "Iteration: 800 Training Accuracy: 0.875 Loss: 0.004256783053278923\n",
      "Iteration: 810 Training Accuracy: 0.90625 Loss: 0.002970675239339471\n",
      "Iteration: 820 Training Accuracy: 1.0 Loss: 0.0014405492693185806\n",
      "Iteration: 830 Training Accuracy: 0.890625 Loss: 0.00423240615054965\n",
      "Iteration: 840 Training Accuracy: 0.9375 Loss: 0.002904750406742096\n",
      "Iteration: 850 Training Accuracy: 0.96875 Loss: 0.002679205499589443\n",
      "Iteration: 860 Training Accuracy: 0.953125 Loss: 0.0015392632922157645\n",
      "Iteration: 870 Training Accuracy: 0.984375 Loss: 0.001099064596928656\n",
      "Iteration: 880 Training Accuracy: 0.9375 Loss: 0.003355159657076001\n",
      "Iteration: 890 Training Accuracy: 0.953125 Loss: 0.0031343584414571524\n",
      "Iteration: 900 Training Accuracy: 0.90625 Loss: 0.004015778191387653\n",
      "Iteration: 910 Training Accuracy: 0.9375 Loss: 0.0029422128573060036\n",
      "Iteration: 920 Training Accuracy: 0.90625 Loss: 0.0037697735242545605\n",
      "Iteration: 930 Training Accuracy: 0.890625 Loss: 0.004366178996860981\n",
      "Training Accuracy = 0.90625\n",
      "Validation Accuracy = 0.9246666666666666\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 1.0 Loss: 0.001625566277652979\n",
      "Iteration: 950 Training Accuracy: 0.984375 Loss: 0.0015041580190882087\n",
      "Iteration: 960 Training Accuracy: 0.96875 Loss: 0.0010619538370519876\n",
      "Iteration: 970 Training Accuracy: 0.96875 Loss: 0.0013655865332111716\n",
      "Iteration: 980 Training Accuracy: 0.96875 Loss: 0.0010577497305348516\n",
      "Iteration: 990 Training Accuracy: 0.96875 Loss: 0.0018254125025123358\n",
      "Iteration: 1000 Training Accuracy: 1.0 Loss: 0.0001130344535340555\n",
      "Iteration: 1010 Training Accuracy: 1.0 Loss: 0.00043526251101866364\n",
      "Iteration: 1020 Training Accuracy: 1.0 Loss: 0.00017673027468845248\n",
      "Iteration: 1030 Training Accuracy: 0.984375 Loss: 0.0010395090794190764\n",
      "Iteration: 1040 Training Accuracy: 0.96875 Loss: 0.0013988562859594822\n",
      "Iteration: 1050 Training Accuracy: 0.984375 Loss: 0.0006005377508699894\n",
      "Iteration: 1060 Training Accuracy: 0.984375 Loss: 0.0004724147147499025\n",
      "Iteration: 1070 Training Accuracy: 0.96875 Loss: 0.001562460558488965\n",
      "Iteration: 1080 Training Accuracy: 0.984375 Loss: 0.0006769713945686817\n",
      "Iteration: 1090 Training Accuracy: 1.0 Loss: 0.0005177357234060764\n",
      "Iteration: 1100 Training Accuracy: 0.984375 Loss: 0.0007795446435920894\n",
      "Iteration: 1110 Training Accuracy: 1.0 Loss: 0.00048760412028059363\n",
      "Iteration: 1120 Training Accuracy: 0.984375 Loss: 0.001121240551583469\n",
      "Iteration: 1130 Training Accuracy: 1.0 Loss: 0.000561379361897707\n",
      "Iteration: 1140 Training Accuracy: 0.984375 Loss: 0.0007323105237446725\n",
      "Iteration: 1150 Training Accuracy: 1.0 Loss: 0.00037592643639072776\n",
      "Iteration: 1160 Training Accuracy: 0.96875 Loss: 0.001239122124388814\n",
      "Iteration: 1170 Training Accuracy: 0.96875 Loss: 0.0009369052713736892\n",
      "Iteration: 1180 Training Accuracy: 1.0 Loss: 0.0002873910707421601\n",
      "Iteration: 1190 Training Accuracy: 1.0 Loss: 0.0007292925729416311\n",
      "Iteration: 1200 Training Accuracy: 1.0 Loss: 0.0004694680101238191\n",
      "Iteration: 1210 Training Accuracy: 0.96875 Loss: 0.0019461162155494094\n",
      "Iteration: 1220 Training Accuracy: 1.0 Loss: 0.00099467346444726\n",
      "Iteration: 1230 Training Accuracy: 1.0 Loss: 0.00023749039974063635\n",
      "Iteration: 1240 Training Accuracy: 1.0 Loss: 0.0003683191316667944\n",
      "Iteration: 1250 Training Accuracy: 0.984375 Loss: 0.0009255007607862353\n",
      "Iteration: 1260 Training Accuracy: 0.96875 Loss: 0.0015571103431284428\n",
      "Iteration: 1270 Training Accuracy: 0.984375 Loss: 0.00042595984996296465\n",
      "Iteration: 1280 Training Accuracy: 1.0 Loss: 0.00024262495571747422\n",
      "Iteration: 1290 Training Accuracy: 0.984375 Loss: 0.0009035352850332856\n",
      "Iteration: 1300 Training Accuracy: 0.984375 Loss: 0.0006580573390237987\n",
      "Iteration: 1310 Training Accuracy: 1.0 Loss: 0.0005262353806756437\n",
      "Iteration: 1320 Training Accuracy: 0.984375 Loss: 0.001133446116000414\n",
      "Iteration: 1330 Training Accuracy: 0.953125 Loss: 0.001350866979919374\n",
      "Iteration: 1340 Training Accuracy: 0.984375 Loss: 0.0007627946906723082\n",
      "Iteration: 1350 Training Accuracy: 0.9375 Loss: 0.002111864509060979\n",
      "Iteration: 1360 Training Accuracy: 0.984375 Loss: 0.001075577107258141\n",
      "Iteration: 1370 Training Accuracy: 0.96875 Loss: 0.0008272847044281662\n",
      "Iteration: 1380 Training Accuracy: 1.0 Loss: 0.00015859461564105004\n",
      "Iteration: 1390 Training Accuracy: 1.0 Loss: 0.0007360399467870593\n",
      "Iteration: 1400 Training Accuracy: 1.0 Loss: 0.0003182661021128297\n",
      "Iteration: 1410 Training Accuracy: 1.0 Loss: 0.00032227602787315845\n",
      "Iteration: 1420 Training Accuracy: 0.96875 Loss: 0.0007684578304179013\n",
      "Iteration: 1430 Training Accuracy: 0.984375 Loss: 0.0008363166707567871\n",
      "Iteration: 1440 Training Accuracy: 0.96875 Loss: 0.0009865417378023267\n",
      "Iteration: 1450 Training Accuracy: 0.96875 Loss: 0.0009409988997504115\n",
      "Iteration: 1460 Training Accuracy: 0.96875 Loss: 0.0012690209550783038\n",
      "Iteration: 1470 Training Accuracy: 0.984375 Loss: 0.0014667956857010722\n",
      "Iteration: 1480 Training Accuracy: 1.0 Loss: 0.0006405371823348105\n",
      "Iteration: 1490 Training Accuracy: 0.96875 Loss: 0.0011409127619117498\n",
      "Iteration: 1500 Training Accuracy: 1.0 Loss: 0.00013590209709946066\n",
      "Iteration: 1510 Training Accuracy: 0.96875 Loss: 0.0011695371940732002\n",
      "Iteration: 1520 Training Accuracy: 1.0 Loss: 0.00033707605325616896\n",
      "Iteration: 1530 Training Accuracy: 0.984375 Loss: 0.00044695596443489194\n",
      "Iteration: 1540 Training Accuracy: 1.0 Loss: 0.000970221939496696\n",
      "Iteration: 1550 Training Accuracy: 1.0 Loss: 0.00047856158926151693\n",
      "Iteration: 1560 Training Accuracy: 1.0 Loss: 0.0006695477059110999\n",
      "Iteration: 1570 Training Accuracy: 0.984375 Loss: 0.000875690602697432\n",
      "Iteration: 1580 Training Accuracy: 0.984375 Loss: 0.00044759249431081116\n",
      "Iteration: 1590 Training Accuracy: 0.96875 Loss: 0.000807605276349932\n",
      "Iteration: 1600 Training Accuracy: 0.984375 Loss: 0.0005994053790345788\n",
      "Iteration: 1610 Training Accuracy: 1.0 Loss: 0.0003341275150887668\n",
      "Iteration: 1620 Training Accuracy: 0.953125 Loss: 0.0018829734763130546\n",
      "Iteration: 1630 Training Accuracy: 0.984375 Loss: 0.0006374804070219398\n",
      "Iteration: 1640 Training Accuracy: 0.984375 Loss: 0.0007581210811622441\n",
      "Iteration: 1650 Training Accuracy: 0.953125 Loss: 0.0011856681667268276\n",
      "Iteration: 1660 Training Accuracy: 1.0 Loss: 0.00038349872920662165\n",
      "Iteration: 1670 Training Accuracy: 0.984375 Loss: 0.0005996330874040723\n",
      "Iteration: 1680 Training Accuracy: 1.0 Loss: 8.32476798677817e-05\n",
      "Iteration: 1690 Training Accuracy: 1.0 Loss: 0.0004297846753615886\n",
      "Iteration: 1700 Training Accuracy: 1.0 Loss: 0.00037810654612258077\n",
      "Iteration: 1710 Training Accuracy: 1.0 Loss: 0.000734141911379993\n",
      "Iteration: 1720 Training Accuracy: 1.0 Loss: 0.00032364914659410715\n",
      "Iteration: 1730 Training Accuracy: 0.984375 Loss: 0.0010344525799155235\n",
      "Iteration: 1740 Training Accuracy: 0.984375 Loss: 0.0010495521128177643\n",
      "Iteration: 1750 Training Accuracy: 1.0 Loss: 0.0004783870535902679\n",
      "Iteration: 1760 Training Accuracy: 0.984375 Loss: 0.0008011026075109839\n",
      "Iteration: 1770 Training Accuracy: 0.984375 Loss: 0.0008569358033128083\n",
      "Iteration: 1780 Training Accuracy: 0.984375 Loss: 0.0011148761259391904\n",
      "Iteration: 1790 Training Accuracy: 0.96875 Loss: 0.0012243115343153477\n",
      "Iteration: 1800 Training Accuracy: 0.984375 Loss: 0.0005724175134673715\n",
      "Iteration: 1810 Training Accuracy: 1.0 Loss: 0.000294093566481024\n",
      "Iteration: 1820 Training Accuracy: 0.96875 Loss: 0.0015362267149612308\n",
      "Iteration: 1830 Training Accuracy: 1.0 Loss: 0.00037693866761401296\n",
      "Iteration: 1840 Training Accuracy: 1.0 Loss: 0.00031911360565572977\n",
      "Iteration: 1850 Training Accuracy: 0.953125 Loss: 0.002569712931290269\n",
      "Iteration: 1860 Training Accuracy: 0.984375 Loss: 0.0004811293911188841\n",
      "Iteration: 1870 Training Accuracy: 0.984375 Loss: 0.00040861708112061024\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9265\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 0.96875 Loss: 0.0012946071801707149\n",
      "Iteration: 1890 Training Accuracy: 0.984375 Loss: 0.0008463133126497269\n",
      "Iteration: 1900 Training Accuracy: 1.0 Loss: 6.554094579769298e-05\n",
      "Iteration: 1910 Training Accuracy: 1.0 Loss: 0.0005343588418327272\n",
      "Iteration: 1920 Training Accuracy: 1.0 Loss: 0.0003488222137093544\n",
      "Iteration: 1930 Training Accuracy: 0.953125 Loss: 0.0013379680458456278\n",
      "Iteration: 1940 Training Accuracy: 0.984375 Loss: 0.0004949307767674327\n",
      "Iteration: 1950 Training Accuracy: 1.0 Loss: 0.0009464363101869822\n",
      "Iteration: 1960 Training Accuracy: 1.0 Loss: 0.00027178286109119654\n",
      "Iteration: 1970 Training Accuracy: 0.953125 Loss: 0.0012947191717103124\n",
      "Iteration: 1980 Training Accuracy: 1.0 Loss: 0.000561792403459549\n",
      "Iteration: 1990 Training Accuracy: 0.984375 Loss: 0.000568216317333281\n",
      "Iteration: 2000 Training Accuracy: 0.96875 Loss: 0.0009823247091844678\n",
      "Iteration: 2010 Training Accuracy: 0.96875 Loss: 0.001673203893005848\n",
      "Iteration: 2020 Training Accuracy: 1.0 Loss: 0.0005636487621814013\n",
      "Iteration: 2030 Training Accuracy: 1.0 Loss: 0.0003575738810468465\n",
      "Iteration: 2040 Training Accuracy: 1.0 Loss: 0.0004241172573529184\n",
      "Iteration: 2050 Training Accuracy: 0.984375 Loss: 0.000979266595095396\n",
      "Iteration: 2060 Training Accuracy: 0.984375 Loss: 0.0008329096017405391\n",
      "Iteration: 2070 Training Accuracy: 0.96875 Loss: 0.0010916644241660833\n",
      "Iteration: 2080 Training Accuracy: 0.984375 Loss: 0.0003961518523283303\n",
      "Iteration: 2090 Training Accuracy: 0.96875 Loss: 0.0007273295777849853\n",
      "Iteration: 2100 Training Accuracy: 0.984375 Loss: 0.0003775042132474482\n",
      "Iteration: 2110 Training Accuracy: 0.984375 Loss: 0.0009224112145602703\n",
      "Iteration: 2120 Training Accuracy: 0.96875 Loss: 0.0012132618576288223\n",
      "Iteration: 2130 Training Accuracy: 0.96875 Loss: 0.000652165908832103\n",
      "Iteration: 2140 Training Accuracy: 0.984375 Loss: 0.000390048255212605\n",
      "Iteration: 2150 Training Accuracy: 1.0 Loss: 0.0002740012132562697\n",
      "Iteration: 2160 Training Accuracy: 0.984375 Loss: 0.0010995264165103436\n",
      "Iteration: 2170 Training Accuracy: 0.984375 Loss: 0.0006018804269842803\n",
      "Iteration: 2180 Training Accuracy: 0.984375 Loss: 0.0010497489711269736\n",
      "Iteration: 2190 Training Accuracy: 0.96875 Loss: 0.0016404534690082073\n",
      "Iteration: 2200 Training Accuracy: 1.0 Loss: 0.0006253735045902431\n",
      "Iteration: 2210 Training Accuracy: 0.984375 Loss: 0.0010192400077357888\n",
      "Iteration: 2220 Training Accuracy: 1.0 Loss: 0.000795631087385118\n",
      "Iteration: 2230 Training Accuracy: 1.0 Loss: 9.828801557887346e-05\n",
      "Iteration: 2240 Training Accuracy: 1.0 Loss: 0.0006080284947529435\n",
      "Iteration: 2250 Training Accuracy: 0.984375 Loss: 0.0003981717745773494\n",
      "Iteration: 2260 Training Accuracy: 0.984375 Loss: 0.0010434358846396208\n",
      "Iteration: 2270 Training Accuracy: 0.984375 Loss: 0.0010191516485065222\n",
      "Iteration: 2280 Training Accuracy: 1.0 Loss: 0.00017018790822476149\n",
      "Iteration: 2290 Training Accuracy: 1.0 Loss: 0.000520705187227577\n",
      "Iteration: 2300 Training Accuracy: 1.0 Loss: 0.00033723082742653787\n",
      "Iteration: 2310 Training Accuracy: 0.984375 Loss: 0.000649108609650284\n",
      "Iteration: 2320 Training Accuracy: 0.96875 Loss: 0.0015323441475629807\n",
      "Iteration: 2330 Training Accuracy: 1.0 Loss: 0.000343968189554289\n",
      "Iteration: 2340 Training Accuracy: 0.984375 Loss: 0.0009864617604762316\n",
      "Iteration: 2350 Training Accuracy: 0.984375 Loss: 0.0006965018110349774\n",
      "Iteration: 2360 Training Accuracy: 0.9375 Loss: 0.002141839824616909\n",
      "Iteration: 2370 Training Accuracy: 1.0 Loss: 0.00013853891869075596\n",
      "Iteration: 2380 Training Accuracy: 0.921875 Loss: 0.002810757840052247\n",
      "Iteration: 2390 Training Accuracy: 0.96875 Loss: 0.0010669409530237317\n",
      "Iteration: 2400 Training Accuracy: 1.0 Loss: 0.0005178026622161269\n",
      "Iteration: 2410 Training Accuracy: 0.984375 Loss: 0.0006453287787735462\n",
      "Iteration: 2420 Training Accuracy: 1.0 Loss: 0.0004053423763252795\n",
      "Iteration: 2430 Training Accuracy: 0.953125 Loss: 0.0015973495319485664\n",
      "Iteration: 2440 Training Accuracy: 1.0 Loss: 0.00032068954897113144\n",
      "Iteration: 2450 Training Accuracy: 1.0 Loss: 0.00031481822952628136\n",
      "Iteration: 2460 Training Accuracy: 1.0 Loss: 3.146735252812505e-05\n",
      "Iteration: 2470 Training Accuracy: 0.984375 Loss: 0.0009815467055886984\n",
      "Iteration: 2480 Training Accuracy: 1.0 Loss: 0.00034994512679986656\n",
      "Iteration: 2490 Training Accuracy: 0.984375 Loss: 0.0006165914237499237\n",
      "Iteration: 2500 Training Accuracy: 1.0 Loss: 0.0009017379488795996\n",
      "Iteration: 2510 Training Accuracy: 0.96875 Loss: 0.0014162808656692505\n",
      "Iteration: 2520 Training Accuracy: 0.984375 Loss: 0.000492798862978816\n",
      "Iteration: 2530 Training Accuracy: 1.0 Loss: 0.0005194614641368389\n",
      "Iteration: 2540 Training Accuracy: 0.984375 Loss: 0.0008667400688864291\n",
      "Iteration: 2550 Training Accuracy: 1.0 Loss: 0.00012587792298290879\n",
      "Iteration: 2560 Training Accuracy: 1.0 Loss: 0.00023618685372639447\n",
      "Iteration: 2570 Training Accuracy: 1.0 Loss: 0.0002128909109160304\n",
      "Iteration: 2580 Training Accuracy: 0.984375 Loss: 0.0008827673736959696\n",
      "Iteration: 2590 Training Accuracy: 0.984375 Loss: 0.0007940089562907815\n",
      "Iteration: 2600 Training Accuracy: 0.953125 Loss: 0.0016690061893314123\n",
      "Iteration: 2610 Training Accuracy: 1.0 Loss: 0.0007332984241656959\n",
      "Iteration: 2620 Training Accuracy: 1.0 Loss: 0.00039663404459133744\n",
      "Iteration: 2630 Training Accuracy: 0.96875 Loss: 0.0010071898577734828\n",
      "Iteration: 2640 Training Accuracy: 0.984375 Loss: 0.0010308929486200213\n",
      "Iteration: 2650 Training Accuracy: 0.984375 Loss: 0.0014073707861825824\n",
      "Iteration: 2660 Training Accuracy: 0.96875 Loss: 0.0012530687963590026\n",
      "Iteration: 2670 Training Accuracy: 0.984375 Loss: 0.0013858715537935495\n",
      "Iteration: 2680 Training Accuracy: 1.0 Loss: 0.00045754428720101714\n",
      "Iteration: 2690 Training Accuracy: 1.0 Loss: 0.0004001708293799311\n",
      "Iteration: 2700 Training Accuracy: 1.0 Loss: 0.00036020224797539413\n",
      "Iteration: 2710 Training Accuracy: 0.96875 Loss: 0.0008598383283242583\n",
      "Iteration: 2720 Training Accuracy: 0.984375 Loss: 0.0011907205916941166\n",
      "Iteration: 2730 Training Accuracy: 0.984375 Loss: 0.0008706559310667217\n",
      "Iteration: 2740 Training Accuracy: 1.0 Loss: 0.0005368636338971555\n",
      "Iteration: 2750 Training Accuracy: 0.96875 Loss: 0.0009209164418280125\n",
      "Iteration: 2760 Training Accuracy: 0.984375 Loss: 0.0007587263826280832\n",
      "Iteration: 2770 Training Accuracy: 0.96875 Loss: 0.0016700626583769917\n",
      "Iteration: 2780 Training Accuracy: 1.0 Loss: 0.00024870980996638536\n",
      "Iteration: 2790 Training Accuracy: 0.984375 Loss: 0.0010045859962701797\n",
      "Iteration: 2800 Training Accuracy: 0.96875 Loss: 0.0013920096680521965\n",
      "Iteration: 2810 Training Accuracy: 1.0 Loss: 0.0003649087157100439\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9263333333333333\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 1.0 Loss: 0.00034441292518749833\n",
      "Iteration: 2830 Training Accuracy: 1.0 Loss: 0.000343603256624192\n",
      "Iteration: 2840 Training Accuracy: 1.0 Loss: 0.00019780141883529723\n",
      "Iteration: 2850 Training Accuracy: 1.0 Loss: 0.0006357845850288868\n",
      "Iteration: 2860 Training Accuracy: 1.0 Loss: 0.0002858774969354272\n",
      "Iteration: 2870 Training Accuracy: 1.0 Loss: 0.00044313582475297153\n",
      "Iteration: 2880 Training Accuracy: 0.984375 Loss: 0.0014886199496686459\n",
      "Iteration: 2890 Training Accuracy: 0.984375 Loss: 0.000990303698927164\n",
      "Iteration: 2900 Training Accuracy: 0.984375 Loss: 0.0013696183450520039\n",
      "Iteration: 2910 Training Accuracy: 0.96875 Loss: 0.001110463053919375\n",
      "Iteration: 2920 Training Accuracy: 0.984375 Loss: 0.0011611991794779897\n",
      "Iteration: 2930 Training Accuracy: 1.0 Loss: 0.00038082333048805594\n",
      "Iteration: 2940 Training Accuracy: 0.96875 Loss: 0.0011168099008500576\n",
      "Iteration: 2950 Training Accuracy: 1.0 Loss: 0.0002419475349597633\n",
      "Iteration: 2960 Training Accuracy: 0.984375 Loss: 0.000661353231407702\n",
      "Iteration: 2970 Training Accuracy: 0.96875 Loss: 0.0014044351410120726\n",
      "Iteration: 2980 Training Accuracy: 1.0 Loss: 0.0005581419682130218\n",
      "Iteration: 2990 Training Accuracy: 0.96875 Loss: 0.0008171863737516105\n",
      "Iteration: 3000 Training Accuracy: 1.0 Loss: 0.0002260116016259417\n",
      "Iteration: 3010 Training Accuracy: 1.0 Loss: 0.0005001040990464389\n",
      "Iteration: 3020 Training Accuracy: 0.953125 Loss: 0.0010861854534596205\n",
      "Iteration: 3030 Training Accuracy: 0.984375 Loss: 0.0011284458450973034\n",
      "Iteration: 3040 Training Accuracy: 0.984375 Loss: 0.001009761355817318\n",
      "Iteration: 3050 Training Accuracy: 0.984375 Loss: 0.00028795021353289485\n",
      "Iteration: 3060 Training Accuracy: 0.96875 Loss: 0.0010547586716711521\n",
      "Iteration: 3070 Training Accuracy: 1.0 Loss: 0.0002137913106707856\n",
      "Iteration: 3080 Training Accuracy: 0.984375 Loss: 0.0005200108862482011\n",
      "Iteration: 3090 Training Accuracy: 0.984375 Loss: 0.0012460885336622596\n",
      "Iteration: 3100 Training Accuracy: 1.0 Loss: 0.00015709659783169627\n",
      "Iteration: 3110 Training Accuracy: 0.96875 Loss: 0.001052690902724862\n",
      "Iteration: 3120 Training Accuracy: 1.0 Loss: 0.00020909056183882058\n",
      "Iteration: 3130 Training Accuracy: 0.984375 Loss: 0.0008635410340502858\n",
      "Iteration: 3140 Training Accuracy: 1.0 Loss: 0.00029222737066447735\n",
      "Iteration: 3150 Training Accuracy: 0.984375 Loss: 0.0016074292361736298\n",
      "Iteration: 3160 Training Accuracy: 0.984375 Loss: 0.0005256792646832764\n",
      "Iteration: 3170 Training Accuracy: 0.984375 Loss: 0.00036328265559859574\n",
      "Iteration: 3180 Training Accuracy: 1.0 Loss: 0.00016680045519024134\n",
      "Iteration: 3190 Training Accuracy: 0.984375 Loss: 0.001008078921586275\n",
      "Iteration: 3200 Training Accuracy: 0.984375 Loss: 0.0013195404317229986\n",
      "Iteration: 3210 Training Accuracy: 1.0 Loss: 0.0002735976595431566\n",
      "Iteration: 3220 Training Accuracy: 1.0 Loss: 0.00036130170337855816\n",
      "Iteration: 3230 Training Accuracy: 0.96875 Loss: 0.0012901334557682276\n",
      "Iteration: 3240 Training Accuracy: 1.0 Loss: 0.00023522041738033295\n",
      "Iteration: 3250 Training Accuracy: 1.0 Loss: 0.0002442153636366129\n",
      "Iteration: 3260 Training Accuracy: 0.984375 Loss: 0.0007608518353663385\n",
      "Iteration: 3270 Training Accuracy: 1.0 Loss: 0.0003044070617761463\n",
      "Iteration: 3280 Training Accuracy: 0.984375 Loss: 0.0010188957676291466\n",
      "Iteration: 3290 Training Accuracy: 0.984375 Loss: 0.0007920770440250635\n",
      "Iteration: 3300 Training Accuracy: 1.0 Loss: 0.0003192354051861912\n",
      "Iteration: 3310 Training Accuracy: 0.984375 Loss: 0.0011875375639647245\n",
      "Iteration: 3320 Training Accuracy: 0.984375 Loss: 0.0008395731565542519\n",
      "Iteration: 3330 Training Accuracy: 0.984375 Loss: 0.0006241367664188147\n",
      "Iteration: 3340 Training Accuracy: 0.96875 Loss: 0.0007315851398743689\n",
      "Iteration: 3350 Training Accuracy: 1.0 Loss: 0.000575475103687495\n",
      "Iteration: 3360 Training Accuracy: 0.984375 Loss: 0.0014369450509548187\n",
      "Iteration: 3370 Training Accuracy: 1.0 Loss: 0.00018595816800370812\n",
      "Iteration: 3380 Training Accuracy: 1.0 Loss: 0.00014994974480941892\n",
      "Iteration: 3390 Training Accuracy: 0.984375 Loss: 0.0007620819960720837\n",
      "Iteration: 3400 Training Accuracy: 0.984375 Loss: 0.0010411364492028952\n",
      "Iteration: 3410 Training Accuracy: 1.0 Loss: 0.0005603438476100564\n",
      "Iteration: 3420 Training Accuracy: 1.0 Loss: 0.000247886375291273\n",
      "Iteration: 3430 Training Accuracy: 1.0 Loss: 0.0001646968157729134\n",
      "Iteration: 3440 Training Accuracy: 1.0 Loss: 0.0001431432319805026\n",
      "Iteration: 3450 Training Accuracy: 1.0 Loss: 0.00026538455858826637\n",
      "Iteration: 3460 Training Accuracy: 0.953125 Loss: 0.0011886763386428356\n",
      "Iteration: 3470 Training Accuracy: 0.984375 Loss: 0.0010002804920077324\n",
      "Iteration: 3480 Training Accuracy: 0.984375 Loss: 0.0004509644058998674\n",
      "Iteration: 3490 Training Accuracy: 1.0 Loss: 0.0003371546627022326\n",
      "Iteration: 3500 Training Accuracy: 1.0 Loss: 0.000715279602445662\n",
      "Iteration: 3510 Training Accuracy: 1.0 Loss: 0.0004746147315017879\n",
      "Iteration: 3520 Training Accuracy: 1.0 Loss: 9.382057760376483e-05\n",
      "Iteration: 3530 Training Accuracy: 0.984375 Loss: 0.0010487635154277086\n",
      "Iteration: 3540 Training Accuracy: 0.984375 Loss: 0.0009945750935003161\n",
      "Iteration: 3550 Training Accuracy: 0.96875 Loss: 0.0011284220963716507\n",
      "Iteration: 3560 Training Accuracy: 0.984375 Loss: 0.0010548413265496492\n",
      "Iteration: 3570 Training Accuracy: 1.0 Loss: 0.0004283432208467275\n",
      "Iteration: 3580 Training Accuracy: 0.96875 Loss: 0.001142309163697064\n",
      "Iteration: 3590 Training Accuracy: 1.0 Loss: 0.0002321251085959375\n",
      "Iteration: 3600 Training Accuracy: 1.0 Loss: 0.0003949304227717221\n",
      "Iteration: 3610 Training Accuracy: 1.0 Loss: 0.00032843052758835256\n",
      "Iteration: 3620 Training Accuracy: 0.984375 Loss: 0.00041282540769316256\n",
      "Iteration: 3630 Training Accuracy: 1.0 Loss: 0.0002477215602993965\n",
      "Iteration: 3640 Training Accuracy: 0.984375 Loss: 0.0010668195318430662\n",
      "Iteration: 3650 Training Accuracy: 0.984375 Loss: 0.0006970101385377347\n",
      "Iteration: 3660 Training Accuracy: 0.984375 Loss: 0.00107146380469203\n",
      "Iteration: 3670 Training Accuracy: 0.96875 Loss: 0.0016267402097582817\n",
      "Iteration: 3680 Training Accuracy: 0.984375 Loss: 0.00055851717479527\n",
      "Iteration: 3690 Training Accuracy: 1.0 Loss: 0.0003650513826869428\n",
      "Iteration: 3700 Training Accuracy: 0.96875 Loss: 0.0018848597537726164\n",
      "Iteration: 3710 Training Accuracy: 0.984375 Loss: 0.0004271249345038086\n",
      "Iteration: 3720 Training Accuracy: 0.984375 Loss: 0.0006009474745951593\n",
      "Iteration: 3730 Training Accuracy: 1.0 Loss: 0.0009167373646050692\n",
      "Iteration: 3740 Training Accuracy: 0.984375 Loss: 0.0007791786338202655\n",
      "Iteration: 3750 Training Accuracy: 0.96875 Loss: 0.0007647273596376181\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9275\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 1.0 Loss: 0.0002576896804384887\n",
      "Iteration: 3770 Training Accuracy: 0.984375 Loss: 0.0005055083893239498\n",
      "Iteration: 3780 Training Accuracy: 0.984375 Loss: 0.0008430929156020284\n",
      "Iteration: 3790 Training Accuracy: 1.0 Loss: 0.00039352785097435117\n",
      "Iteration: 3800 Training Accuracy: 1.0 Loss: 0.0004821314651053399\n",
      "Iteration: 3810 Training Accuracy: 1.0 Loss: 0.0003337514353916049\n",
      "Iteration: 3820 Training Accuracy: 0.953125 Loss: 0.0021090225782245398\n",
      "Iteration: 3830 Training Accuracy: 1.0 Loss: 0.0005683739436790347\n",
      "Iteration: 3840 Training Accuracy: 0.96875 Loss: 0.0014156533870846033\n",
      "Iteration: 3850 Training Accuracy: 1.0 Loss: 0.0003458194842096418\n",
      "Iteration: 3860 Training Accuracy: 1.0 Loss: 0.00024896179093047976\n",
      "Iteration: 3870 Training Accuracy: 0.984375 Loss: 0.0012231471482664347\n",
      "Iteration: 3880 Training Accuracy: 1.0 Loss: 0.0004680929414462298\n",
      "Iteration: 3890 Training Accuracy: 0.984375 Loss: 0.0009968153899535537\n",
      "Iteration: 3900 Training Accuracy: 0.984375 Loss: 0.001383262686431408\n",
      "Iteration: 3910 Training Accuracy: 1.0 Loss: 0.00019675487419590354\n",
      "Iteration: 3920 Training Accuracy: 1.0 Loss: 0.0006151814595796168\n",
      "Iteration: 3930 Training Accuracy: 1.0 Loss: 0.00037515367148444057\n",
      "Iteration: 3940 Training Accuracy: 1.0 Loss: 0.00011179393186466768\n",
      "Iteration: 3950 Training Accuracy: 0.984375 Loss: 0.0002552367513999343\n",
      "Iteration: 3960 Training Accuracy: 1.0 Loss: 3.8404890801757574e-05\n",
      "Iteration: 3970 Training Accuracy: 0.984375 Loss: 0.0004347127105575055\n",
      "Iteration: 3980 Training Accuracy: 0.984375 Loss: 0.0010310534853488207\n",
      "Iteration: 3990 Training Accuracy: 1.0 Loss: 0.0005541702266782522\n",
      "Iteration: 4000 Training Accuracy: 1.0 Loss: 0.00044989382149651647\n",
      "Iteration: 4010 Training Accuracy: 0.984375 Loss: 0.0006300734239630401\n",
      "Iteration: 4020 Training Accuracy: 0.96875 Loss: 0.0015423926524817944\n",
      "Iteration: 4030 Training Accuracy: 0.984375 Loss: 0.0009620409109629691\n",
      "Iteration: 4040 Training Accuracy: 1.0 Loss: 0.000408992258599028\n",
      "Iteration: 4050 Training Accuracy: 0.984375 Loss: 0.0009894338436424732\n",
      "Iteration: 4060 Training Accuracy: 1.0 Loss: 0.000432932807598263\n",
      "Iteration: 4070 Training Accuracy: 0.9375 Loss: 0.0016098188934847713\n",
      "Iteration: 4080 Training Accuracy: 1.0 Loss: 0.00014943086716812104\n",
      "Iteration: 4090 Training Accuracy: 1.0 Loss: 9.515681449556723e-05\n",
      "Iteration: 4100 Training Accuracy: 1.0 Loss: 0.0001284181489609182\n",
      "Iteration: 4110 Training Accuracy: 1.0 Loss: 0.00012962036998942494\n",
      "Iteration: 4120 Training Accuracy: 0.96875 Loss: 0.0014289942337200046\n",
      "Iteration: 4130 Training Accuracy: 0.984375 Loss: 0.0019459056202322245\n",
      "Iteration: 4140 Training Accuracy: 0.984375 Loss: 0.0011381504591554403\n",
      "Iteration: 4150 Training Accuracy: 0.984375 Loss: 0.0007926783873699605\n",
      "Iteration: 4160 Training Accuracy: 0.984375 Loss: 0.0006410547066479921\n",
      "Iteration: 4170 Training Accuracy: 1.0 Loss: 9.870808571577072e-05\n",
      "Iteration: 4180 Training Accuracy: 0.984375 Loss: 0.0008957852842286229\n",
      "Iteration: 4190 Training Accuracy: 1.0 Loss: 0.00023360060004051775\n",
      "Iteration: 4200 Training Accuracy: 1.0 Loss: 0.0006488538347184658\n",
      "Iteration: 4210 Training Accuracy: 1.0 Loss: 9.837228571996093e-05\n",
      "Iteration: 4220 Training Accuracy: 1.0 Loss: 0.0004288745403755456\n",
      "Iteration: 4230 Training Accuracy: 1.0 Loss: 0.0005849569570273161\n",
      "Iteration: 4240 Training Accuracy: 1.0 Loss: 0.0006097910227254033\n",
      "Iteration: 4250 Training Accuracy: 1.0 Loss: 0.0001816076983232051\n",
      "Iteration: 4260 Training Accuracy: 0.984375 Loss: 0.0005203426699154079\n",
      "Iteration: 4270 Training Accuracy: 1.0 Loss: 0.0005973700317554176\n",
      "Iteration: 4280 Training Accuracy: 1.0 Loss: 0.0006431522779166698\n",
      "Iteration: 4290 Training Accuracy: 0.96875 Loss: 0.001346882781945169\n",
      "Iteration: 4300 Training Accuracy: 0.96875 Loss: 0.0012412972282618284\n",
      "Iteration: 4310 Training Accuracy: 0.96875 Loss: 0.0010474236914888024\n",
      "Iteration: 4320 Training Accuracy: 1.0 Loss: 0.000388794403988868\n",
      "Iteration: 4330 Training Accuracy: 0.984375 Loss: 0.0010495034512132406\n",
      "Iteration: 4340 Training Accuracy: 0.984375 Loss: 0.0007266562897711992\n",
      "Iteration: 4350 Training Accuracy: 1.0 Loss: 0.00022199454542715102\n",
      "Iteration: 4360 Training Accuracy: 1.0 Loss: 0.00016842075274325907\n",
      "Iteration: 4370 Training Accuracy: 1.0 Loss: 0.00016563879034947604\n",
      "Iteration: 4380 Training Accuracy: 0.984375 Loss: 0.0006174332229420543\n",
      "Iteration: 4390 Training Accuracy: 1.0 Loss: 0.0004341225721873343\n",
      "Iteration: 4400 Training Accuracy: 1.0 Loss: 0.00024150317767634988\n",
      "Iteration: 4410 Training Accuracy: 0.984375 Loss: 0.0012503204634413123\n",
      "Iteration: 4420 Training Accuracy: 0.953125 Loss: 0.0025552031584084034\n",
      "Iteration: 4430 Training Accuracy: 1.0 Loss: 0.00017026177374646068\n",
      "Iteration: 4440 Training Accuracy: 0.96875 Loss: 0.0008517306414432824\n",
      "Iteration: 4450 Training Accuracy: 1.0 Loss: 0.0001960866793524474\n",
      "Iteration: 4460 Training Accuracy: 0.984375 Loss: 0.0010338990250602365\n",
      "Iteration: 4470 Training Accuracy: 0.96875 Loss: 0.0010982267558574677\n",
      "Iteration: 4480 Training Accuracy: 1.0 Loss: 0.0001466043759137392\n",
      "Iteration: 4490 Training Accuracy: 1.0 Loss: 0.0005622458411380649\n",
      "Iteration: 4500 Training Accuracy: 0.96875 Loss: 0.0008784469682723284\n",
      "Iteration: 4510 Training Accuracy: 0.984375 Loss: 0.0006795972003601491\n",
      "Iteration: 4520 Training Accuracy: 0.984375 Loss: 0.0005097533576190472\n",
      "Iteration: 4530 Training Accuracy: 0.96875 Loss: 0.0010486114770174026\n",
      "Iteration: 4540 Training Accuracy: 1.0 Loss: 0.0002462885167915374\n",
      "Iteration: 4550 Training Accuracy: 0.984375 Loss: 0.0007251003989949822\n",
      "Iteration: 4560 Training Accuracy: 0.984375 Loss: 0.0008510928018949926\n",
      "Iteration: 4570 Training Accuracy: 0.984375 Loss: 0.0009032045491039753\n",
      "Iteration: 4580 Training Accuracy: 1.0 Loss: 0.00017117589595727623\n",
      "Iteration: 4590 Training Accuracy: 1.0 Loss: 0.00019912171410396695\n",
      "Iteration: 4600 Training Accuracy: 1.0 Loss: 0.00030957511626183987\n",
      "Iteration: 4610 Training Accuracy: 1.0 Loss: 0.00041021069046109915\n",
      "Iteration: 4620 Training Accuracy: 0.984375 Loss: 0.0009684221004135907\n",
      "Iteration: 4630 Training Accuracy: 0.984375 Loss: 0.000595628865994513\n",
      "Iteration: 4640 Training Accuracy: 0.984375 Loss: 0.0007087297854013741\n",
      "Iteration: 4650 Training Accuracy: 1.0 Loss: 0.0002531067293602973\n",
      "Iteration: 4660 Training Accuracy: 0.96875 Loss: 0.0006234975880943239\n",
      "Iteration: 4670 Training Accuracy: 1.0 Loss: 0.00045229861279949546\n",
      "Iteration: 4680 Training Accuracy: 0.984375 Loss: 0.0009107838850468397\n",
      "Iteration: 4690 Training Accuracy: 1.0 Loss: 0.0002319733757758513\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9273333333333333\n",
      "epoch: 5\n",
      "Iteration: 4700 Training Accuracy: 0.96875 Loss: 0.0008616163395345211\n",
      "Iteration: 4710 Training Accuracy: 1.0 Loss: 0.0001484012755099684\n",
      "Iteration: 4720 Training Accuracy: 0.984375 Loss: 0.0007607787847518921\n",
      "Iteration: 4730 Training Accuracy: 0.984375 Loss: 0.0009413943625986576\n",
      "Iteration: 4740 Training Accuracy: 0.984375 Loss: 0.0007826226064935327\n",
      "Iteration: 4750 Training Accuracy: 0.984375 Loss: 0.0008017385844141245\n",
      "Iteration: 4760 Training Accuracy: 1.0 Loss: 0.0005531799979507923\n",
      "Iteration: 4770 Training Accuracy: 0.96875 Loss: 0.0012912715319544077\n",
      "Iteration: 4780 Training Accuracy: 0.984375 Loss: 0.0008111416827887297\n",
      "Iteration: 4790 Training Accuracy: 1.0 Loss: 0.00039110175566747785\n",
      "Iteration: 4800 Training Accuracy: 0.96875 Loss: 0.0009801313281059265\n",
      "Iteration: 4810 Training Accuracy: 1.0 Loss: 0.0002591785741969943\n",
      "Iteration: 4820 Training Accuracy: 0.984375 Loss: 0.0010353594552725554\n",
      "Iteration: 4830 Training Accuracy: 0.984375 Loss: 0.000919810205232352\n",
      "Iteration: 4840 Training Accuracy: 0.984375 Loss: 0.0006581722991541028\n",
      "Iteration: 4850 Training Accuracy: 1.0 Loss: 0.0002847823197953403\n",
      "Iteration: 4860 Training Accuracy: 0.984375 Loss: 0.0007434315048158169\n",
      "Iteration: 4870 Training Accuracy: 0.96875 Loss: 0.0015849638730287552\n",
      "Iteration: 4880 Training Accuracy: 1.0 Loss: 0.0003288569860160351\n",
      "Iteration: 4890 Training Accuracy: 0.984375 Loss: 0.0008034368511289358\n",
      "Iteration: 4900 Training Accuracy: 1.0 Loss: 0.0004954648320563138\n",
      "Iteration: 4910 Training Accuracy: 1.0 Loss: 0.00029017284396104515\n",
      "Iteration: 4920 Training Accuracy: 0.984375 Loss: 0.0010175117058679461\n",
      "Iteration: 4930 Training Accuracy: 0.984375 Loss: 0.0007957467460073531\n",
      "Iteration: 4940 Training Accuracy: 1.0 Loss: 0.00023141861311160028\n",
      "Iteration: 4950 Training Accuracy: 1.0 Loss: 0.0001749616931192577\n",
      "Iteration: 4960 Training Accuracy: 1.0 Loss: 0.00013954847236163914\n",
      "Iteration: 4970 Training Accuracy: 0.984375 Loss: 0.00071365648182109\n",
      "Iteration: 4980 Training Accuracy: 0.96875 Loss: 0.001046566292643547\n",
      "Iteration: 4990 Training Accuracy: 0.96875 Loss: 0.0008866652497090399\n",
      "Iteration: 5000 Training Accuracy: 0.96875 Loss: 0.0022064002696424723\n",
      "Iteration: 5010 Training Accuracy: 0.984375 Loss: 0.000814112660009414\n",
      "Iteration: 5020 Training Accuracy: 1.0 Loss: 0.00045287233660928905\n",
      "Iteration: 5030 Training Accuracy: 1.0 Loss: 7.836139411665499e-05\n",
      "Iteration: 5040 Training Accuracy: 0.984375 Loss: 0.00044321140740066767\n",
      "Iteration: 5050 Training Accuracy: 0.984375 Loss: 0.000589426257647574\n",
      "Iteration: 5060 Training Accuracy: 1.0 Loss: 0.0005096291424706578\n",
      "Iteration: 5070 Training Accuracy: 0.984375 Loss: 0.0012467722408473492\n",
      "Iteration: 5080 Training Accuracy: 0.984375 Loss: 0.0011392394080758095\n",
      "Iteration: 5090 Training Accuracy: 1.0 Loss: 0.00048481542035005987\n",
      "Iteration: 5100 Training Accuracy: 1.0 Loss: 0.00014419083890970796\n",
      "Iteration: 5110 Training Accuracy: 1.0 Loss: 0.0003932457766495645\n",
      "Iteration: 5120 Training Accuracy: 1.0 Loss: 0.0003265522245783359\n",
      "Iteration: 5130 Training Accuracy: 0.984375 Loss: 0.0003398197586648166\n",
      "Iteration: 5140 Training Accuracy: 0.984375 Loss: 0.0007077876944094896\n",
      "Iteration: 5150 Training Accuracy: 1.0 Loss: 0.0001904325617942959\n",
      "Iteration: 5160 Training Accuracy: 1.0 Loss: 0.00028586966800503433\n",
      "Iteration: 5170 Training Accuracy: 1.0 Loss: 0.00010385992209194228\n",
      "Iteration: 5180 Training Accuracy: 0.984375 Loss: 0.0004640142433345318\n",
      "Iteration: 5190 Training Accuracy: 0.984375 Loss: 0.0008585292380303144\n",
      "Iteration: 5200 Training Accuracy: 1.0 Loss: 0.0001950686564669013\n",
      "Iteration: 5210 Training Accuracy: 1.0 Loss: 0.0002711857086978853\n",
      "Iteration: 5220 Training Accuracy: 1.0 Loss: 0.0003832241636700928\n",
      "Iteration: 5230 Training Accuracy: 1.0 Loss: 0.00023722340120002627\n",
      "Iteration: 5240 Training Accuracy: 1.0 Loss: 0.00045586959458887577\n",
      "Iteration: 5250 Training Accuracy: 0.984375 Loss: 0.0005845602136105299\n",
      "Iteration: 5260 Training Accuracy: 0.96875 Loss: 0.0014241973403841257\n",
      "Iteration: 5270 Training Accuracy: 0.984375 Loss: 0.0004862880741711706\n",
      "Iteration: 5280 Training Accuracy: 1.0 Loss: 0.00016368365322705358\n",
      "Iteration: 5290 Training Accuracy: 0.984375 Loss: 0.0007684206357225776\n",
      "Iteration: 5300 Training Accuracy: 1.0 Loss: 9.40339668886736e-05\n",
      "Iteration: 5310 Training Accuracy: 0.96875 Loss: 0.0015317110810428858\n",
      "Iteration: 5320 Training Accuracy: 1.0 Loss: 0.0002512488281354308\n",
      "Iteration: 5330 Training Accuracy: 1.0 Loss: 0.0003872254746966064\n",
      "Iteration: 5340 Training Accuracy: 1.0 Loss: 0.00040125721716322005\n",
      "Iteration: 5350 Training Accuracy: 0.96875 Loss: 0.0009444572497159243\n",
      "Iteration: 5360 Training Accuracy: 1.0 Loss: 0.0001303554163314402\n",
      "Iteration: 5370 Training Accuracy: 0.984375 Loss: 0.0010146002750843763\n",
      "Iteration: 5380 Training Accuracy: 1.0 Loss: 0.00015567863010801375\n",
      "Iteration: 5390 Training Accuracy: 1.0 Loss: 0.0002662795886863023\n",
      "Iteration: 5400 Training Accuracy: 1.0 Loss: 0.0001813699200283736\n",
      "Iteration: 5410 Training Accuracy: 1.0 Loss: 0.0004214471555314958\n",
      "Iteration: 5420 Training Accuracy: 0.96875 Loss: 0.0012276831548660994\n",
      "Iteration: 5430 Training Accuracy: 1.0 Loss: 0.0004873797297477722\n",
      "Iteration: 5440 Training Accuracy: 0.96875 Loss: 0.0007355543784797192\n",
      "Iteration: 5450 Training Accuracy: 0.984375 Loss: 0.001064975280314684\n",
      "Iteration: 5460 Training Accuracy: 1.0 Loss: 0.0002359055361011997\n",
      "Iteration: 5470 Training Accuracy: 0.984375 Loss: 0.0010692104697227478\n",
      "Iteration: 5480 Training Accuracy: 1.0 Loss: 0.00029879866633564234\n",
      "Iteration: 5490 Training Accuracy: 0.984375 Loss: 0.0010892777936533093\n",
      "Iteration: 5500 Training Accuracy: 0.953125 Loss: 0.001932836719788611\n",
      "Iteration: 5510 Training Accuracy: 0.984375 Loss: 0.000599061488173902\n",
      "Iteration: 5520 Training Accuracy: 0.984375 Loss: 0.0006516225985251367\n",
      "Iteration: 5530 Training Accuracy: 0.96875 Loss: 0.0007522215018980205\n",
      "Iteration: 5540 Training Accuracy: 1.0 Loss: 0.00037427269853651524\n",
      "Iteration: 5550 Training Accuracy: 1.0 Loss: 0.0001324157346971333\n",
      "Iteration: 5560 Training Accuracy: 0.984375 Loss: 0.0004708819615188986\n",
      "Iteration: 5570 Training Accuracy: 1.0 Loss: 0.00044207784230820835\n",
      "Iteration: 5580 Training Accuracy: 1.0 Loss: 0.0005656817811541259\n",
      "Iteration: 5590 Training Accuracy: 0.984375 Loss: 0.0006420063436962664\n",
      "Iteration: 5600 Training Accuracy: 0.984375 Loss: 0.0010828516678884625\n",
      "Iteration: 5610 Training Accuracy: 1.0 Loss: 0.00027278816560283303\n",
      "Iteration: 5620 Training Accuracy: 1.0 Loss: 8.560925198253244e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9281666666666667\n",
      "epoch: 6\n",
      "Iteration: 5630 Training Accuracy: 1.0 Loss: 0.0007886236999183893\n",
      "Iteration: 5640 Training Accuracy: 0.96875 Loss: 0.001085158670321107\n",
      "Iteration: 5650 Training Accuracy: 0.96875 Loss: 0.0007987766293808818\n",
      "Iteration: 5660 Training Accuracy: 0.984375 Loss: 0.0015626995591446757\n",
      "Iteration: 5670 Training Accuracy: 0.96875 Loss: 0.000636712065897882\n",
      "Iteration: 5680 Training Accuracy: 0.96875 Loss: 0.001441283617168665\n",
      "Iteration: 5690 Training Accuracy: 1.0 Loss: 7.71514605730772e-05\n",
      "Iteration: 5700 Training Accuracy: 1.0 Loss: 0.00025041052140295506\n",
      "Iteration: 5710 Training Accuracy: 1.0 Loss: 9.687786223366857e-05\n",
      "Iteration: 5720 Training Accuracy: 0.984375 Loss: 0.0008087069145403802\n",
      "Iteration: 5730 Training Accuracy: 0.96875 Loss: 0.0010823141783475876\n",
      "Iteration: 5740 Training Accuracy: 0.984375 Loss: 0.0004694605013355613\n",
      "Iteration: 5750 Training Accuracy: 1.0 Loss: 0.00025456672301515937\n",
      "Iteration: 5760 Training Accuracy: 0.984375 Loss: 0.0012040823930874467\n",
      "Iteration: 5770 Training Accuracy: 0.984375 Loss: 0.0005559070268645883\n",
      "Iteration: 5780 Training Accuracy: 1.0 Loss: 0.0002747991820797324\n",
      "Iteration: 5790 Training Accuracy: 0.984375 Loss: 0.0005357540212571621\n",
      "Iteration: 5800 Training Accuracy: 1.0 Loss: 0.00020274045527912676\n",
      "Iteration: 5810 Training Accuracy: 0.984375 Loss: 0.0007066263351589441\n",
      "Iteration: 5820 Training Accuracy: 1.0 Loss: 0.0003728199517354369\n",
      "Iteration: 5830 Training Accuracy: 0.984375 Loss: 0.0007183307898230851\n",
      "Iteration: 5840 Training Accuracy: 1.0 Loss: 0.00018356784130446613\n",
      "Iteration: 5850 Training Accuracy: 1.0 Loss: 0.0006498938892036676\n",
      "Iteration: 5860 Training Accuracy: 0.984375 Loss: 0.0006457332638092339\n",
      "Iteration: 5870 Training Accuracy: 1.0 Loss: 0.00016625921125523746\n",
      "Iteration: 5880 Training Accuracy: 1.0 Loss: 0.0005381162045523524\n",
      "Iteration: 5890 Training Accuracy: 1.0 Loss: 0.00024484467576257885\n",
      "Iteration: 5900 Training Accuracy: 0.96875 Loss: 0.0012570999097079039\n",
      "Iteration: 5910 Training Accuracy: 1.0 Loss: 0.0005625723279081285\n",
      "Iteration: 5920 Training Accuracy: 1.0 Loss: 0.0001299516297876835\n",
      "Iteration: 5930 Training Accuracy: 1.0 Loss: 0.00028732247301377356\n",
      "Iteration: 5940 Training Accuracy: 0.984375 Loss: 0.0005653186817653477\n",
      "Iteration: 5950 Training Accuracy: 0.984375 Loss: 0.0011740113841369748\n",
      "Iteration: 5960 Training Accuracy: 1.0 Loss: 0.00028163366368971765\n",
      "Iteration: 5970 Training Accuracy: 1.0 Loss: 0.00015663111116737127\n",
      "Iteration: 5980 Training Accuracy: 1.0 Loss: 0.0005178567953407764\n",
      "Iteration: 5990 Training Accuracy: 0.984375 Loss: 0.0005815310869365931\n",
      "Iteration: 6000 Training Accuracy: 1.0 Loss: 0.0003261936653871089\n",
      "Iteration: 6010 Training Accuracy: 0.984375 Loss: 0.0008794968598522246\n",
      "Iteration: 6020 Training Accuracy: 0.953125 Loss: 0.0010177659569308162\n",
      "Iteration: 6030 Training Accuracy: 1.0 Loss: 0.00040120090125128627\n",
      "Iteration: 6040 Training Accuracy: 0.96875 Loss: 0.00158536690287292\n",
      "Iteration: 6050 Training Accuracy: 0.984375 Loss: 0.000927882909309119\n",
      "Iteration: 6060 Training Accuracy: 1.0 Loss: 0.00048083675210364163\n",
      "Iteration: 6070 Training Accuracy: 1.0 Loss: 9.649871208239347e-05\n",
      "Iteration: 6080 Training Accuracy: 1.0 Loss: 0.0004497716436162591\n",
      "Iteration: 6090 Training Accuracy: 1.0 Loss: 0.00024104138719849288\n",
      "Iteration: 6100 Training Accuracy: 1.0 Loss: 0.00021301893866620958\n",
      "Iteration: 6110 Training Accuracy: 1.0 Loss: 0.0004531469021458179\n",
      "Iteration: 6120 Training Accuracy: 1.0 Loss: 0.0006554171559400856\n",
      "Iteration: 6130 Training Accuracy: 0.96875 Loss: 0.0006268872530199587\n",
      "Iteration: 6140 Training Accuracy: 0.984375 Loss: 0.0007070924038998783\n",
      "Iteration: 6150 Training Accuracy: 0.984375 Loss: 0.0009790724143385887\n",
      "Iteration: 6160 Training Accuracy: 0.984375 Loss: 0.0010269144549965858\n",
      "Iteration: 6170 Training Accuracy: 1.0 Loss: 0.0005183215253055096\n",
      "Iteration: 6180 Training Accuracy: 1.0 Loss: 0.0006251727463677526\n",
      "Iteration: 6190 Training Accuracy: 1.0 Loss: 8.8146174675785e-05\n",
      "Iteration: 6200 Training Accuracy: 0.96875 Loss: 0.0006963682826608419\n",
      "Iteration: 6210 Training Accuracy: 1.0 Loss: 0.0002487913297954947\n",
      "Iteration: 6220 Training Accuracy: 0.984375 Loss: 0.00037804929888807237\n",
      "Iteration: 6230 Training Accuracy: 1.0 Loss: 0.0004870183183811605\n",
      "Iteration: 6240 Training Accuracy: 1.0 Loss: 0.00032214901875704527\n",
      "Iteration: 6250 Training Accuracy: 1.0 Loss: 0.0004030433192383498\n",
      "Iteration: 6260 Training Accuracy: 0.984375 Loss: 0.0005785488756373525\n",
      "Iteration: 6270 Training Accuracy: 0.984375 Loss: 0.000276881706668064\n",
      "Iteration: 6280 Training Accuracy: 1.0 Loss: 0.00044143988634459674\n",
      "Iteration: 6290 Training Accuracy: 0.984375 Loss: 0.00032655728864483535\n",
      "Iteration: 6300 Training Accuracy: 1.0 Loss: 0.0001582102559041232\n",
      "Iteration: 6310 Training Accuracy: 0.953125 Loss: 0.0015825744485482574\n",
      "Iteration: 6320 Training Accuracy: 1.0 Loss: 0.00041951442835852504\n",
      "Iteration: 6330 Training Accuracy: 1.0 Loss: 0.0004508965357672423\n",
      "Iteration: 6340 Training Accuracy: 0.96875 Loss: 0.0010137399658560753\n",
      "Iteration: 6350 Training Accuracy: 1.0 Loss: 0.0002629816881380975\n",
      "Iteration: 6360 Training Accuracy: 0.984375 Loss: 0.00044363393681123853\n",
      "Iteration: 6370 Training Accuracy: 1.0 Loss: 4.7271012590499595e-05\n",
      "Iteration: 6380 Training Accuracy: 1.0 Loss: 0.00030560570303350687\n",
      "Iteration: 6390 Training Accuracy: 1.0 Loss: 0.0002472914638929069\n",
      "Iteration: 6400 Training Accuracy: 1.0 Loss: 0.0004857888270635158\n",
      "Iteration: 6410 Training Accuracy: 1.0 Loss: 0.00023739160678815097\n",
      "Iteration: 6420 Training Accuracy: 0.984375 Loss: 0.0007928583072498441\n",
      "Iteration: 6430 Training Accuracy: 0.984375 Loss: 0.0007008736720308661\n",
      "Iteration: 6440 Training Accuracy: 1.0 Loss: 0.0003131900739390403\n",
      "Iteration: 6450 Training Accuracy: 0.984375 Loss: 0.0005248818779364228\n",
      "Iteration: 6460 Training Accuracy: 1.0 Loss: 0.0005057583912275732\n",
      "Iteration: 6470 Training Accuracy: 0.984375 Loss: 0.0009184501832351089\n",
      "Iteration: 6480 Training Accuracy: 0.984375 Loss: 0.0009522744221612811\n",
      "Iteration: 6490 Training Accuracy: 0.984375 Loss: 0.0005303728394210339\n",
      "Iteration: 6500 Training Accuracy: 1.0 Loss: 0.00018084865587297827\n",
      "Iteration: 6510 Training Accuracy: 0.984375 Loss: 0.0011111600324511528\n",
      "Iteration: 6520 Training Accuracy: 1.0 Loss: 0.00026251969393342733\n",
      "Iteration: 6530 Training Accuracy: 1.0 Loss: 0.00017576717073097825\n",
      "Iteration: 6540 Training Accuracy: 0.96875 Loss: 0.001970553770661354\n",
      "Iteration: 6550 Training Accuracy: 0.984375 Loss: 0.0004048969713039696\n",
      "Iteration: 6560 Training Accuracy: 1.0 Loss: 0.0002932177158072591\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9293333333333333\n",
      "epoch: 7\n",
      "Iteration: 6570 Training Accuracy: 1.0 Loss: 0.0007239342667162418\n",
      "Iteration: 6580 Training Accuracy: 0.984375 Loss: 0.0003938984009437263\n",
      "Iteration: 6590 Training Accuracy: 1.0 Loss: 4.190886102151126e-05\n",
      "Iteration: 6600 Training Accuracy: 1.0 Loss: 0.0003257330972701311\n",
      "Iteration: 6610 Training Accuracy: 1.0 Loss: 0.0002642776817083359\n",
      "Iteration: 6620 Training Accuracy: 0.96875 Loss: 0.001010228879749775\n",
      "Iteration: 6630 Training Accuracy: 1.0 Loss: 0.00027980131562799215\n",
      "Iteration: 6640 Training Accuracy: 1.0 Loss: 0.0005285359220579267\n",
      "Iteration: 6650 Training Accuracy: 1.0 Loss: 0.00021760624076705426\n",
      "Iteration: 6660 Training Accuracy: 0.984375 Loss: 0.000742825330235064\n",
      "Iteration: 6670 Training Accuracy: 1.0 Loss: 0.00034495891304686666\n",
      "Iteration: 6680 Training Accuracy: 1.0 Loss: 0.0003808049950748682\n",
      "Iteration: 6690 Training Accuracy: 1.0 Loss: 0.0006856792606413364\n",
      "Iteration: 6700 Training Accuracy: 0.96875 Loss: 0.0013084152014926076\n",
      "Iteration: 6710 Training Accuracy: 1.0 Loss: 0.00036852507037110627\n",
      "Iteration: 6720 Training Accuracy: 1.0 Loss: 0.0002741779899224639\n",
      "Iteration: 6730 Training Accuracy: 1.0 Loss: 0.0002738491748459637\n",
      "Iteration: 6740 Training Accuracy: 0.984375 Loss: 0.00045333514572121203\n",
      "Iteration: 6750 Training Accuracy: 1.0 Loss: 0.0005854966584593058\n",
      "Iteration: 6760 Training Accuracy: 0.984375 Loss: 0.0008502666023559868\n",
      "Iteration: 6770 Training Accuracy: 0.984375 Loss: 0.00032791629200801253\n",
      "Iteration: 6780 Training Accuracy: 1.0 Loss: 0.00045404594857245684\n",
      "Iteration: 6790 Training Accuracy: 0.984375 Loss: 0.0003615173336584121\n",
      "Iteration: 6800 Training Accuracy: 0.984375 Loss: 0.0006894383113831282\n",
      "Iteration: 6810 Training Accuracy: 0.96875 Loss: 0.000995453679934144\n",
      "Iteration: 6820 Training Accuracy: 1.0 Loss: 0.00035201109130866826\n",
      "Iteration: 6830 Training Accuracy: 1.0 Loss: 0.0002265315706608817\n",
      "Iteration: 6840 Training Accuracy: 1.0 Loss: 0.00015648003318347037\n",
      "Iteration: 6850 Training Accuracy: 0.984375 Loss: 0.0007752689998596907\n",
      "Iteration: 6860 Training Accuracy: 1.0 Loss: 0.0003649708232842386\n",
      "Iteration: 6870 Training Accuracy: 0.984375 Loss: 0.00055481813615188\n",
      "Iteration: 6880 Training Accuracy: 0.96875 Loss: 0.0011271077673882246\n",
      "Iteration: 6890 Training Accuracy: 1.0 Loss: 0.0004997146897949278\n",
      "Iteration: 6900 Training Accuracy: 0.984375 Loss: 0.0006892342935316265\n",
      "Iteration: 6910 Training Accuracy: 1.0 Loss: 0.0005617636488750577\n",
      "Iteration: 6920 Training Accuracy: 1.0 Loss: 3.8571721233893186e-05\n",
      "Iteration: 6930 Training Accuracy: 1.0 Loss: 0.0003771439951378852\n",
      "Iteration: 6940 Training Accuracy: 0.984375 Loss: 0.0003891999658662826\n",
      "Iteration: 6950 Training Accuracy: 0.984375 Loss: 0.0006499792216345668\n",
      "Iteration: 6960 Training Accuracy: 1.0 Loss: 0.000529209675733\n",
      "Iteration: 6970 Training Accuracy: 1.0 Loss: 0.00010270316124660894\n",
      "Iteration: 6980 Training Accuracy: 1.0 Loss: 0.000442651507910341\n",
      "Iteration: 6990 Training Accuracy: 1.0 Loss: 0.0002566436305642128\n",
      "Iteration: 7000 Training Accuracy: 0.984375 Loss: 0.0005601230077445507\n",
      "Iteration: 7010 Training Accuracy: 0.984375 Loss: 0.0013446529628708959\n",
      "Iteration: 7020 Training Accuracy: 1.0 Loss: 0.0002892466727644205\n",
      "Iteration: 7030 Training Accuracy: 0.984375 Loss: 0.000753682223148644\n",
      "Iteration: 7040 Training Accuracy: 0.984375 Loss: 0.000435555906733498\n",
      "Iteration: 7050 Training Accuracy: 0.953125 Loss: 0.0016809302615001798\n",
      "Iteration: 7060 Training Accuracy: 1.0 Loss: 8.184504986274987e-05\n",
      "Iteration: 7070 Training Accuracy: 0.921875 Loss: 0.002115207025781274\n",
      "Iteration: 7080 Training Accuracy: 0.96875 Loss: 0.00078390451380983\n",
      "Iteration: 7090 Training Accuracy: 1.0 Loss: 0.00034285092260688543\n",
      "Iteration: 7100 Training Accuracy: 1.0 Loss: 0.0005106693715788424\n",
      "Iteration: 7110 Training Accuracy: 1.0 Loss: 0.00023105776926968247\n",
      "Iteration: 7120 Training Accuracy: 0.984375 Loss: 0.0010483573423698545\n",
      "Iteration: 7130 Training Accuracy: 1.0 Loss: 0.0002603188913781196\n",
      "Iteration: 7140 Training Accuracy: 1.0 Loss: 0.0002060706028714776\n",
      "Iteration: 7150 Training Accuracy: 1.0 Loss: 2.2641203031525947e-05\n",
      "Iteration: 7160 Training Accuracy: 1.0 Loss: 0.0006054916884750128\n",
      "Iteration: 7170 Training Accuracy: 1.0 Loss: 0.00024035654496401548\n",
      "Iteration: 7180 Training Accuracy: 0.984375 Loss: 0.00047080672811716795\n",
      "Iteration: 7190 Training Accuracy: 1.0 Loss: 0.0005772234871983528\n",
      "Iteration: 7200 Training Accuracy: 0.984375 Loss: 0.0007818665471859276\n",
      "Iteration: 7210 Training Accuracy: 1.0 Loss: 0.00021511054364964366\n",
      "Iteration: 7220 Training Accuracy: 1.0 Loss: 0.0002848544390872121\n",
      "Iteration: 7230 Training Accuracy: 0.984375 Loss: 0.0005094409571029246\n",
      "Iteration: 7240 Training Accuracy: 1.0 Loss: 8.289650577353314e-05\n",
      "Iteration: 7250 Training Accuracy: 1.0 Loss: 0.00019715138478204608\n",
      "Iteration: 7260 Training Accuracy: 1.0 Loss: 0.00013403405318967998\n",
      "Iteration: 7270 Training Accuracy: 0.984375 Loss: 0.0006644776440225542\n",
      "Iteration: 7280 Training Accuracy: 0.984375 Loss: 0.0006333521450869739\n",
      "Iteration: 7290 Training Accuracy: 1.0 Loss: 0.000877470534760505\n",
      "Iteration: 7300 Training Accuracy: 1.0 Loss: 0.0004984764382243156\n",
      "Iteration: 7310 Training Accuracy: 1.0 Loss: 0.00026321347104385495\n",
      "Iteration: 7320 Training Accuracy: 0.984375 Loss: 0.0007607805309817195\n",
      "Iteration: 7330 Training Accuracy: 0.984375 Loss: 0.0006009975331835449\n",
      "Iteration: 7340 Training Accuracy: 0.984375 Loss: 0.0010738562559708953\n",
      "Iteration: 7350 Training Accuracy: 1.0 Loss: 0.0006334685022011399\n",
      "Iteration: 7360 Training Accuracy: 0.984375 Loss: 0.0011989572085440159\n",
      "Iteration: 7370 Training Accuracy: 1.0 Loss: 0.0002953146176878363\n",
      "Iteration: 7380 Training Accuracy: 1.0 Loss: 0.0002997922711074352\n",
      "Iteration: 7390 Training Accuracy: 1.0 Loss: 0.00022777463891543448\n",
      "Iteration: 7400 Training Accuracy: 0.984375 Loss: 0.0008216165006160736\n",
      "Iteration: 7410 Training Accuracy: 0.984375 Loss: 0.000873316079378128\n",
      "Iteration: 7420 Training Accuracy: 0.984375 Loss: 0.0005706760566681623\n",
      "Iteration: 7430 Training Accuracy: 1.0 Loss: 0.00026769223040901124\n",
      "Iteration: 7440 Training Accuracy: 0.96875 Loss: 0.0007013874128460884\n",
      "Iteration: 7450 Training Accuracy: 1.0 Loss: 0.00048822895041666925\n",
      "Iteration: 7460 Training Accuracy: 0.984375 Loss: 0.0010113712633028626\n",
      "Iteration: 7470 Training Accuracy: 1.0 Loss: 0.0001666517782723531\n",
      "Iteration: 7480 Training Accuracy: 1.0 Loss: 0.000623062252998352\n",
      "Iteration: 7490 Training Accuracy: 1.0 Loss: 0.000797001994214952\n",
      "Iteration: 7500 Training Accuracy: 1.0 Loss: 0.00024939526338130236\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.929\n",
      "epoch: 8\n",
      "Iteration: 7510 Training Accuracy: 1.0 Loss: 0.0002510125341359526\n",
      "Iteration: 7520 Training Accuracy: 1.0 Loss: 0.00023573536600451916\n",
      "Iteration: 7530 Training Accuracy: 1.0 Loss: 0.00012359007087070495\n",
      "Iteration: 7540 Training Accuracy: 1.0 Loss: 0.0003584630321711302\n",
      "Iteration: 7550 Training Accuracy: 1.0 Loss: 0.0001912056904984638\n",
      "Iteration: 7560 Training Accuracy: 1.0 Loss: 0.0002928348840214312\n",
      "Iteration: 7570 Training Accuracy: 0.984375 Loss: 0.0012233747402206063\n",
      "Iteration: 7580 Training Accuracy: 0.984375 Loss: 0.0008323548827320337\n",
      "Iteration: 7590 Training Accuracy: 0.984375 Loss: 0.0013102656230330467\n",
      "Iteration: 7600 Training Accuracy: 0.984375 Loss: 0.0007732227095402777\n",
      "Iteration: 7610 Training Accuracy: 0.96875 Loss: 0.001206288579851389\n",
      "Iteration: 7620 Training Accuracy: 1.0 Loss: 0.0002221788454335183\n",
      "Iteration: 7630 Training Accuracy: 0.96875 Loss: 0.0009953847620636225\n",
      "Iteration: 7640 Training Accuracy: 1.0 Loss: 0.0001707077317405492\n",
      "Iteration: 7650 Training Accuracy: 0.984375 Loss: 0.0005698591703549027\n",
      "Iteration: 7660 Training Accuracy: 0.984375 Loss: 0.0009861692087724805\n",
      "Iteration: 7670 Training Accuracy: 1.0 Loss: 0.00041899955249391496\n",
      "Iteration: 7680 Training Accuracy: 0.984375 Loss: 0.0005762948421761394\n",
      "Iteration: 7690 Training Accuracy: 1.0 Loss: 0.00017226295312866569\n",
      "Iteration: 7700 Training Accuracy: 1.0 Loss: 0.0003511867253109813\n",
      "Iteration: 7710 Training Accuracy: 0.984375 Loss: 0.0009044668986462057\n",
      "Iteration: 7720 Training Accuracy: 0.984375 Loss: 0.000879655359312892\n",
      "Iteration: 7730 Training Accuracy: 0.984375 Loss: 0.0009500547312200069\n",
      "Iteration: 7740 Training Accuracy: 0.984375 Loss: 0.00023943278938531876\n",
      "Iteration: 7750 Training Accuracy: 0.984375 Loss: 0.0006709242006763816\n",
      "Iteration: 7760 Training Accuracy: 1.0 Loss: 0.00015602714847773314\n",
      "Iteration: 7770 Training Accuracy: 0.984375 Loss: 0.0004885194939561188\n",
      "Iteration: 7780 Training Accuracy: 0.984375 Loss: 0.0010393731063231826\n",
      "Iteration: 7790 Training Accuracy: 1.0 Loss: 0.0001033648441080004\n",
      "Iteration: 7800 Training Accuracy: 0.96875 Loss: 0.0009001316502690315\n",
      "Iteration: 7810 Training Accuracy: 1.0 Loss: 0.0001864125079009682\n",
      "Iteration: 7820 Training Accuracy: 0.984375 Loss: 0.0005996926920488477\n",
      "Iteration: 7830 Training Accuracy: 1.0 Loss: 0.000192211038665846\n",
      "Iteration: 7840 Training Accuracy: 0.984375 Loss: 0.0011675228597596288\n",
      "Iteration: 7850 Training Accuracy: 0.984375 Loss: 0.00047047442058101296\n",
      "Iteration: 7860 Training Accuracy: 0.984375 Loss: 0.0002628105285111815\n",
      "Iteration: 7870 Training Accuracy: 1.0 Loss: 0.00013336734264157712\n",
      "Iteration: 7880 Training Accuracy: 0.984375 Loss: 0.0007906025857664645\n",
      "Iteration: 7890 Training Accuracy: 0.984375 Loss: 0.0009764152928255498\n",
      "Iteration: 7900 Training Accuracy: 1.0 Loss: 0.0002634348638821393\n",
      "Iteration: 7910 Training Accuracy: 1.0 Loss: 0.0002569831267464906\n",
      "Iteration: 7920 Training Accuracy: 1.0 Loss: 0.0009299415396526456\n",
      "Iteration: 7930 Training Accuracy: 1.0 Loss: 0.00014393283345270902\n",
      "Iteration: 7940 Training Accuracy: 1.0 Loss: 0.00020590958592947572\n",
      "Iteration: 7950 Training Accuracy: 0.984375 Loss: 0.0005745770176872611\n",
      "Iteration: 7960 Training Accuracy: 1.0 Loss: 0.00016849640815053135\n",
      "Iteration: 7970 Training Accuracy: 0.984375 Loss: 0.0006936450954526663\n",
      "Iteration: 7980 Training Accuracy: 0.984375 Loss: 0.0006012087105773389\n",
      "Iteration: 7990 Training Accuracy: 1.0 Loss: 0.0002116136602126062\n",
      "Iteration: 8000 Training Accuracy: 0.984375 Loss: 0.0010795996058732271\n",
      "Iteration: 8010 Training Accuracy: 0.984375 Loss: 0.0006785598234273493\n",
      "Iteration: 8020 Training Accuracy: 1.0 Loss: 0.00048110526404343545\n",
      "Iteration: 8030 Training Accuracy: 0.984375 Loss: 0.0005068667232990265\n",
      "Iteration: 8040 Training Accuracy: 1.0 Loss: 0.0003969079698435962\n",
      "Iteration: 8050 Training Accuracy: 0.984375 Loss: 0.0011925584403797984\n",
      "Iteration: 8060 Training Accuracy: 1.0 Loss: 0.00011568290210561827\n",
      "Iteration: 8070 Training Accuracy: 1.0 Loss: 0.0001065164033207111\n",
      "Iteration: 8080 Training Accuracy: 1.0 Loss: 0.0004997325595468283\n",
      "Iteration: 8090 Training Accuracy: 0.984375 Loss: 0.0006455588154494762\n",
      "Iteration: 8100 Training Accuracy: 1.0 Loss: 0.0004757895949296653\n",
      "Iteration: 8110 Training Accuracy: 1.0 Loss: 0.0001998605002881959\n",
      "Iteration: 8120 Training Accuracy: 1.0 Loss: 0.00011342235666234046\n",
      "Iteration: 8130 Training Accuracy: 1.0 Loss: 8.695972064742818e-05\n",
      "Iteration: 8140 Training Accuracy: 1.0 Loss: 0.00020655387197621167\n",
      "Iteration: 8150 Training Accuracy: 0.953125 Loss: 0.0009572298731654882\n",
      "Iteration: 8160 Training Accuracy: 1.0 Loss: 0.0006922349566593766\n",
      "Iteration: 8170 Training Accuracy: 1.0 Loss: 0.0003525642096064985\n",
      "Iteration: 8180 Training Accuracy: 1.0 Loss: 0.0002659356687217951\n",
      "Iteration: 8190 Training Accuracy: 1.0 Loss: 0.0005839656805619597\n",
      "Iteration: 8200 Training Accuracy: 1.0 Loss: 0.00031863260664977133\n",
      "Iteration: 8210 Training Accuracy: 1.0 Loss: 8.112784416880459e-05\n",
      "Iteration: 8220 Training Accuracy: 0.984375 Loss: 0.0007061385549604893\n",
      "Iteration: 8230 Training Accuracy: 0.984375 Loss: 0.0007375383866019547\n",
      "Iteration: 8240 Training Accuracy: 0.96875 Loss: 0.0009210266871377826\n",
      "Iteration: 8250 Training Accuracy: 0.984375 Loss: 0.0007828580564819276\n",
      "Iteration: 8260 Training Accuracy: 1.0 Loss: 0.00033312011510133743\n",
      "Iteration: 8270 Training Accuracy: 0.984375 Loss: 0.0009380497504025698\n",
      "Iteration: 8280 Training Accuracy: 1.0 Loss: 0.00017426974955014884\n",
      "Iteration: 8290 Training Accuracy: 1.0 Loss: 0.0003607148537412286\n",
      "Iteration: 8300 Training Accuracy: 1.0 Loss: 0.00028941070195287466\n",
      "Iteration: 8310 Training Accuracy: 0.984375 Loss: 0.0003181467473041266\n",
      "Iteration: 8320 Training Accuracy: 1.0 Loss: 0.00017621330334804952\n",
      "Iteration: 8330 Training Accuracy: 1.0 Loss: 0.0007218436221592128\n",
      "Iteration: 8340 Training Accuracy: 1.0 Loss: 0.0005244194762781262\n",
      "Iteration: 8350 Training Accuracy: 0.984375 Loss: 0.0007539567304775119\n",
      "Iteration: 8360 Training Accuracy: 0.96875 Loss: 0.0010782567551359534\n",
      "Iteration: 8370 Training Accuracy: 0.984375 Loss: 0.0005249881651252508\n",
      "Iteration: 8380 Training Accuracy: 1.0 Loss: 0.0002443946141283959\n",
      "Iteration: 8390 Training Accuracy: 0.96875 Loss: 0.001277560950256884\n",
      "Iteration: 8400 Training Accuracy: 1.0 Loss: 0.00024807441513985395\n",
      "Iteration: 8410 Training Accuracy: 1.0 Loss: 0.00039274603477679193\n",
      "Iteration: 8420 Training Accuracy: 1.0 Loss: 0.0004836313601117581\n",
      "Iteration: 8430 Training Accuracy: 0.984375 Loss: 0.0005984937306493521\n",
      "Iteration: 8440 Training Accuracy: 0.96875 Loss: 0.0006727975560352206\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.929\n",
      "epoch: 9\n",
      "Iteration: 8450 Training Accuracy: 1.0 Loss: 0.00017468705482315272\n",
      "Iteration: 8460 Training Accuracy: 0.984375 Loss: 0.0004444182268343866\n",
      "Iteration: 8470 Training Accuracy: 0.984375 Loss: 0.0007129892474040389\n",
      "Iteration: 8480 Training Accuracy: 1.0 Loss: 0.00029510309104807675\n",
      "Iteration: 8490 Training Accuracy: 1.0 Loss: 0.0003831423236988485\n",
      "Iteration: 8500 Training Accuracy: 1.0 Loss: 0.0002562925801612437\n",
      "Iteration: 8510 Training Accuracy: 0.953125 Loss: 0.0017523950664326549\n",
      "Iteration: 8520 Training Accuracy: 1.0 Loss: 0.00047087768325582147\n",
      "Iteration: 8530 Training Accuracy: 0.984375 Loss: 0.001064593088813126\n",
      "Iteration: 8540 Training Accuracy: 1.0 Loss: 0.000258175830822438\n",
      "Iteration: 8550 Training Accuracy: 1.0 Loss: 0.00022091339633334428\n",
      "Iteration: 8560 Training Accuracy: 0.984375 Loss: 0.0009451117948628962\n",
      "Iteration: 8570 Training Accuracy: 1.0 Loss: 0.00034447209327481687\n",
      "Iteration: 8580 Training Accuracy: 1.0 Loss: 0.0007097570924088359\n",
      "Iteration: 8590 Training Accuracy: 0.984375 Loss: 0.0010373239638283849\n",
      "Iteration: 8600 Training Accuracy: 1.0 Loss: 0.00016911924467422068\n",
      "Iteration: 8610 Training Accuracy: 1.0 Loss: 0.0005066271987743676\n",
      "Iteration: 8620 Training Accuracy: 1.0 Loss: 0.0003403120790608227\n",
      "Iteration: 8630 Training Accuracy: 1.0 Loss: 0.00010570030281087384\n",
      "Iteration: 8640 Training Accuracy: 1.0 Loss: 0.00019586001872085035\n",
      "Iteration: 8650 Training Accuracy: 1.0 Loss: 2.3548116587335244e-05\n",
      "Iteration: 8660 Training Accuracy: 1.0 Loss: 0.000401210505515337\n",
      "Iteration: 8670 Training Accuracy: 0.984375 Loss: 0.0008203136385418475\n",
      "Iteration: 8680 Training Accuracy: 1.0 Loss: 0.0003462011809460819\n",
      "Iteration: 8690 Training Accuracy: 1.0 Loss: 0.0003528231754899025\n",
      "Iteration: 8700 Training Accuracy: 0.984375 Loss: 0.0005089636542834342\n",
      "Iteration: 8710 Training Accuracy: 0.96875 Loss: 0.0014831272419542074\n",
      "Iteration: 8720 Training Accuracy: 0.984375 Loss: 0.0007603294216096401\n",
      "Iteration: 8730 Training Accuracy: 1.0 Loss: 0.00035374905564822257\n",
      "Iteration: 8740 Training Accuracy: 1.0 Loss: 0.0007156574283726513\n",
      "Iteration: 8750 Training Accuracy: 1.0 Loss: 0.00033496663672849536\n",
      "Iteration: 8760 Training Accuracy: 0.96875 Loss: 0.0012388646136969328\n",
      "Iteration: 8770 Training Accuracy: 1.0 Loss: 0.00011464100680314004\n",
      "Iteration: 8780 Training Accuracy: 1.0 Loss: 7.591214671265334e-05\n",
      "Iteration: 8790 Training Accuracy: 1.0 Loss: 8.892442565411329e-05\n",
      "Iteration: 8800 Training Accuracy: 1.0 Loss: 7.6207252277527e-05\n",
      "Iteration: 8810 Training Accuracy: 0.96875 Loss: 0.0012413342483341694\n",
      "Iteration: 8820 Training Accuracy: 0.984375 Loss: 0.0017454387852922082\n",
      "Iteration: 8830 Training Accuracy: 0.984375 Loss: 0.0010600524256005883\n",
      "Iteration: 8840 Training Accuracy: 0.984375 Loss: 0.0005805223481729627\n",
      "Iteration: 8850 Training Accuracy: 0.984375 Loss: 0.0005205346969887614\n",
      "Iteration: 8860 Training Accuracy: 1.0 Loss: 6.841958384029567e-05\n",
      "Iteration: 8870 Training Accuracy: 0.984375 Loss: 0.0007279377896338701\n",
      "Iteration: 8880 Training Accuracy: 1.0 Loss: 0.0001667672477196902\n",
      "Iteration: 8890 Training Accuracy: 1.0 Loss: 0.0005168253555893898\n",
      "Iteration: 8900 Training Accuracy: 1.0 Loss: 7.828413072274998e-05\n",
      "Iteration: 8910 Training Accuracy: 0.984375 Loss: 0.0004468679544515908\n",
      "Iteration: 8920 Training Accuracy: 1.0 Loss: 0.0005045264260843396\n",
      "Iteration: 8930 Training Accuracy: 1.0 Loss: 0.0004367590299807489\n",
      "Iteration: 8940 Training Accuracy: 1.0 Loss: 0.00014997032121755183\n",
      "Iteration: 8950 Training Accuracy: 0.984375 Loss: 0.00040576959145255387\n",
      "Iteration: 8960 Training Accuracy: 1.0 Loss: 0.0005274861468933523\n",
      "Iteration: 8970 Training Accuracy: 1.0 Loss: 0.0005201708991080523\n",
      "Iteration: 8980 Training Accuracy: 0.984375 Loss: 0.0011468986049294472\n",
      "Iteration: 8990 Training Accuracy: 0.96875 Loss: 0.000914987176656723\n",
      "Iteration: 9000 Training Accuracy: 0.984375 Loss: 0.0009868086781352758\n",
      "Iteration: 9010 Training Accuracy: 1.0 Loss: 0.0003420248394832015\n",
      "Iteration: 9020 Training Accuracy: 0.984375 Loss: 0.0007054914603941143\n",
      "Iteration: 9030 Training Accuracy: 0.984375 Loss: 0.00046954257413744926\n",
      "Iteration: 9040 Training Accuracy: 1.0 Loss: 0.0001984591654036194\n",
      "Iteration: 9050 Training Accuracy: 1.0 Loss: 0.00013925725943408906\n",
      "Iteration: 9060 Training Accuracy: 1.0 Loss: 0.0001401408517267555\n",
      "Iteration: 9070 Training Accuracy: 1.0 Loss: 0.0004322167078498751\n",
      "Iteration: 9080 Training Accuracy: 1.0 Loss: 0.0003442436864133924\n",
      "Iteration: 9090 Training Accuracy: 1.0 Loss: 0.00017303420463576913\n",
      "Iteration: 9100 Training Accuracy: 0.984375 Loss: 0.0009757532388903201\n",
      "Iteration: 9110 Training Accuracy: 0.9375 Loss: 0.0021140803582966328\n",
      "Iteration: 9120 Training Accuracy: 1.0 Loss: 0.0001377904845867306\n",
      "Iteration: 9130 Training Accuracy: 0.984375 Loss: 0.0006715066265314817\n",
      "Iteration: 9140 Training Accuracy: 1.0 Loss: 0.00011349261330906302\n",
      "Iteration: 9150 Training Accuracy: 0.984375 Loss: 0.000900170358363539\n",
      "Iteration: 9160 Training Accuracy: 0.96875 Loss: 0.0008660462917760015\n",
      "Iteration: 9170 Training Accuracy: 1.0 Loss: 0.0001286916813114658\n",
      "Iteration: 9180 Training Accuracy: 1.0 Loss: 0.0003802909341175109\n",
      "Iteration: 9190 Training Accuracy: 0.984375 Loss: 0.0007092230371199548\n",
      "Iteration: 9200 Training Accuracy: 0.984375 Loss: 0.0004814769490621984\n",
      "Iteration: 9210 Training Accuracy: 0.984375 Loss: 0.00046397687401622534\n",
      "Iteration: 9220 Training Accuracy: 0.96875 Loss: 0.0008397040073759854\n",
      "Iteration: 9230 Training Accuracy: 1.0 Loss: 0.0001671189966145903\n",
      "Iteration: 9240 Training Accuracy: 0.984375 Loss: 0.00057050003670156\n",
      "Iteration: 9250 Training Accuracy: 0.984375 Loss: 0.0005157151026651263\n",
      "Iteration: 9260 Training Accuracy: 0.984375 Loss: 0.0007370624807663262\n",
      "Iteration: 9270 Training Accuracy: 1.0 Loss: 0.00013814125850331038\n",
      "Iteration: 9280 Training Accuracy: 1.0 Loss: 0.00016525952378287911\n",
      "Iteration: 9290 Training Accuracy: 1.0 Loss: 0.0002443572739139199\n",
      "Iteration: 9300 Training Accuracy: 1.0 Loss: 0.0003469153889454901\n",
      "Iteration: 9310 Training Accuracy: 0.984375 Loss: 0.0005612981040030718\n",
      "Iteration: 9320 Training Accuracy: 0.984375 Loss: 0.0004820505273528397\n",
      "Iteration: 9330 Training Accuracy: 0.984375 Loss: 0.0005100802518427372\n",
      "Iteration: 9340 Training Accuracy: 1.0 Loss: 0.000192489183973521\n",
      "Iteration: 9350 Training Accuracy: 0.984375 Loss: 0.0004706181352958083\n",
      "Iteration: 9360 Training Accuracy: 1.0 Loss: 0.00036253384314477444\n",
      "Iteration: 9370 Training Accuracy: 1.0 Loss: 0.0006403859006240964\n",
      "Iteration: 9380 Training Accuracy: 1.0 Loss: 0.00019037861784454435\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9288333333333333\n",
      "epoch: 10\n",
      "Iteration: 9390 Training Accuracy: 0.984375 Loss: 0.0007897595642134547\n",
      "Iteration: 9400 Training Accuracy: 1.0 Loss: 0.00012282212264835835\n",
      "Iteration: 9410 Training Accuracy: 0.984375 Loss: 0.0006847218028269708\n",
      "Iteration: 9420 Training Accuracy: 0.984375 Loss: 0.0008487197337672114\n",
      "Iteration: 9430 Training Accuracy: 0.984375 Loss: 0.0007183024426922202\n",
      "Iteration: 9440 Training Accuracy: 0.984375 Loss: 0.0006769268657080829\n",
      "Iteration: 9450 Training Accuracy: 1.0 Loss: 0.000424789497628808\n",
      "Iteration: 9460 Training Accuracy: 0.984375 Loss: 0.001060195849277079\n",
      "Iteration: 9470 Training Accuracy: 0.984375 Loss: 0.0007825617794878781\n",
      "Iteration: 9480 Training Accuracy: 1.0 Loss: 0.0003277137584518641\n",
      "Iteration: 9490 Training Accuracy: 0.984375 Loss: 0.0007216366939246655\n",
      "Iteration: 9500 Training Accuracy: 1.0 Loss: 0.00022784789325669408\n",
      "Iteration: 9510 Training Accuracy: 0.984375 Loss: 0.0008447456639260054\n",
      "Iteration: 9520 Training Accuracy: 0.984375 Loss: 0.0008037315565161407\n",
      "Iteration: 9530 Training Accuracy: 0.984375 Loss: 0.0004743710742332041\n",
      "Iteration: 9540 Training Accuracy: 1.0 Loss: 0.0002624920743983239\n",
      "Iteration: 9550 Training Accuracy: 0.984375 Loss: 0.0006898418068885803\n",
      "Iteration: 9560 Training Accuracy: 0.96875 Loss: 0.0013542905217036605\n",
      "Iteration: 9570 Training Accuracy: 1.0 Loss: 0.000248525757342577\n",
      "Iteration: 9580 Training Accuracy: 0.984375 Loss: 0.0006633616285398602\n",
      "Iteration: 9590 Training Accuracy: 1.0 Loss: 0.0004916872712783515\n",
      "Iteration: 9600 Training Accuracy: 1.0 Loss: 0.00032565376022830606\n",
      "Iteration: 9610 Training Accuracy: 0.984375 Loss: 0.0007940629730001092\n",
      "Iteration: 9620 Training Accuracy: 0.984375 Loss: 0.0007296286057680845\n",
      "Iteration: 9630 Training Accuracy: 1.0 Loss: 0.0002081106649711728\n",
      "Iteration: 9640 Training Accuracy: 1.0 Loss: 0.00013275136006996036\n",
      "Iteration: 9650 Training Accuracy: 1.0 Loss: 0.0001042973599396646\n",
      "Iteration: 9660 Training Accuracy: 0.984375 Loss: 0.0006152901332825422\n",
      "Iteration: 9670 Training Accuracy: 0.953125 Loss: 0.0009813382057473063\n",
      "Iteration: 9680 Training Accuracy: 0.984375 Loss: 0.0007560522062703967\n",
      "Iteration: 9690 Training Accuracy: 0.96875 Loss: 0.0018631303682923317\n",
      "Iteration: 9700 Training Accuracy: 0.984375 Loss: 0.0007127095595933497\n",
      "Iteration: 9710 Training Accuracy: 1.0 Loss: 0.00032978737726807594\n",
      "Iteration: 9720 Training Accuracy: 1.0 Loss: 8.01708156359382e-05\n",
      "Iteration: 9730 Training Accuracy: 1.0 Loss: 0.0003352323255967349\n",
      "Iteration: 9740 Training Accuracy: 0.984375 Loss: 0.000542427587788552\n",
      "Iteration: 9750 Training Accuracy: 1.0 Loss: 0.0004376154101919383\n",
      "Iteration: 9760 Training Accuracy: 0.984375 Loss: 0.001178321661427617\n",
      "Iteration: 9770 Training Accuracy: 0.984375 Loss: 0.0008412452298216522\n",
      "Iteration: 9780 Training Accuracy: 1.0 Loss: 0.0003773827338591218\n",
      "Iteration: 9790 Training Accuracy: 1.0 Loss: 0.00011710389662766829\n",
      "Iteration: 9800 Training Accuracy: 1.0 Loss: 0.00031838714494369924\n",
      "Iteration: 9810 Training Accuracy: 1.0 Loss: 0.0002857563958968967\n",
      "Iteration: 9820 Training Accuracy: 0.984375 Loss: 0.00030674482695758343\n",
      "Iteration: 9830 Training Accuracy: 1.0 Loss: 0.00047915606410242617\n",
      "Iteration: 9840 Training Accuracy: 1.0 Loss: 0.00019233828061260283\n",
      "Iteration: 9850 Training Accuracy: 1.0 Loss: 0.00023070172755979002\n",
      "Iteration: 9860 Training Accuracy: 1.0 Loss: 9.586906526237726e-05\n",
      "Iteration: 9870 Training Accuracy: 0.984375 Loss: 0.0004146159044466913\n",
      "Iteration: 9880 Training Accuracy: 0.984375 Loss: 0.0007589351153001189\n",
      "Iteration: 9890 Training Accuracy: 1.0 Loss: 0.00016287443577311933\n",
      "Iteration: 9900 Training Accuracy: 1.0 Loss: 0.0002128593623638153\n",
      "Iteration: 9910 Training Accuracy: 1.0 Loss: 0.00024728040443733335\n",
      "Iteration: 9920 Training Accuracy: 1.0 Loss: 0.00019306175818201154\n",
      "Iteration: 9930 Training Accuracy: 1.0 Loss: 0.0004372963448986411\n",
      "Iteration: 9940 Training Accuracy: 1.0 Loss: 0.0004266392206773162\n",
      "Iteration: 9950 Training Accuracy: 0.96875 Loss: 0.0012294785119593143\n",
      "Iteration: 9960 Training Accuracy: 0.984375 Loss: 0.0004281062865629792\n",
      "Iteration: 9970 Training Accuracy: 1.0 Loss: 0.00015329410962294787\n",
      "Iteration: 9980 Training Accuracy: 0.984375 Loss: 0.0006019718130119145\n",
      "Iteration: 9990 Training Accuracy: 1.0 Loss: 8.061705739237368e-05\n",
      "Iteration: 10000 Training Accuracy: 0.984375 Loss: 0.0013653532369062304\n",
      "Iteration: 10010 Training Accuracy: 1.0 Loss: 0.00015944546612445265\n",
      "Iteration: 10020 Training Accuracy: 1.0 Loss: 0.00033971824450418353\n",
      "Iteration: 10030 Training Accuracy: 1.0 Loss: 0.0003552303824108094\n",
      "Iteration: 10040 Training Accuracy: 0.984375 Loss: 0.0007783275796100497\n",
      "Iteration: 10050 Training Accuracy: 1.0 Loss: 0.00012049788347212598\n",
      "Iteration: 10060 Training Accuracy: 0.984375 Loss: 0.0006517129950225353\n",
      "Iteration: 10070 Training Accuracy: 1.0 Loss: 0.00015766694559715688\n",
      "Iteration: 10080 Training Accuracy: 1.0 Loss: 0.0001990612508961931\n",
      "Iteration: 10090 Training Accuracy: 1.0 Loss: 0.00015948963118717074\n",
      "Iteration: 10100 Training Accuracy: 1.0 Loss: 0.0003665320109575987\n",
      "Iteration: 10110 Training Accuracy: 0.984375 Loss: 0.0010966190602630377\n",
      "Iteration: 10120 Training Accuracy: 1.0 Loss: 0.00041291143861599267\n",
      "Iteration: 10130 Training Accuracy: 0.96875 Loss: 0.0006314956117421389\n",
      "Iteration: 10140 Training Accuracy: 0.984375 Loss: 0.0007691021310165524\n",
      "Iteration: 10150 Training Accuracy: 1.0 Loss: 0.00020245998166501522\n",
      "Iteration: 10160 Training Accuracy: 0.984375 Loss: 0.0009949499508365989\n",
      "Iteration: 10170 Training Accuracy: 1.0 Loss: 0.0002544295566622168\n",
      "Iteration: 10180 Training Accuracy: 0.984375 Loss: 0.0008889832533895969\n",
      "Iteration: 10190 Training Accuracy: 0.96875 Loss: 0.0014396872138604522\n",
      "Iteration: 10200 Training Accuracy: 0.984375 Loss: 0.0005305677186697721\n",
      "Iteration: 10210 Training Accuracy: 0.984375 Loss: 0.0006836546817794442\n",
      "Iteration: 10220 Training Accuracy: 0.984375 Loss: 0.0006621448555961251\n",
      "Iteration: 10230 Training Accuracy: 1.0 Loss: 0.0003323140845168382\n",
      "Iteration: 10240 Training Accuracy: 1.0 Loss: 0.0001145010901382193\n",
      "Iteration: 10250 Training Accuracy: 0.984375 Loss: 0.0004621701955329627\n",
      "Iteration: 10260 Training Accuracy: 1.0 Loss: 0.0003723187546711415\n",
      "Iteration: 10270 Training Accuracy: 0.984375 Loss: 0.0005172713426873088\n",
      "Iteration: 10280 Training Accuracy: 0.984375 Loss: 0.0005144430324435234\n",
      "Iteration: 10290 Training Accuracy: 0.984375 Loss: 0.0010088508715853095\n",
      "Iteration: 10300 Training Accuracy: 1.0 Loss: 0.00023290712852030993\n",
      "Iteration: 10310 Training Accuracy: 1.0 Loss: 7.713375089224428e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 11\n",
      "Iteration: 10320 Training Accuracy: 1.0 Loss: 0.0006713744369335473\n",
      "Iteration: 10330 Training Accuracy: 0.984375 Loss: 0.0009281787788495421\n",
      "Iteration: 10340 Training Accuracy: 0.96875 Loss: 0.0007588359294459224\n",
      "Iteration: 10350 Training Accuracy: 0.984375 Loss: 0.0014968601753935218\n",
      "Iteration: 10360 Training Accuracy: 1.0 Loss: 0.00045669093378819525\n",
      "Iteration: 10370 Training Accuracy: 0.96875 Loss: 0.0013316181721165776\n",
      "Iteration: 10380 Training Accuracy: 1.0 Loss: 5.634506305796094e-05\n",
      "Iteration: 10390 Training Accuracy: 1.0 Loss: 0.0002406105923000723\n",
      "Iteration: 10400 Training Accuracy: 1.0 Loss: 9.059793956112117e-05\n",
      "Iteration: 10410 Training Accuracy: 0.984375 Loss: 0.0006542859482578933\n",
      "Iteration: 10420 Training Accuracy: 0.96875 Loss: 0.001016553957015276\n",
      "Iteration: 10430 Training Accuracy: 0.984375 Loss: 0.00044836552115157247\n",
      "Iteration: 10440 Training Accuracy: 1.0 Loss: 0.00023860296641942114\n",
      "Iteration: 10450 Training Accuracy: 0.984375 Loss: 0.001243388862349093\n",
      "Iteration: 10460 Training Accuracy: 0.984375 Loss: 0.0004793664556927979\n",
      "Iteration: 10470 Training Accuracy: 1.0 Loss: 0.00025664098211564124\n",
      "Iteration: 10480 Training Accuracy: 1.0 Loss: 0.0005068632308393717\n",
      "Iteration: 10490 Training Accuracy: 1.0 Loss: 0.0001949433353729546\n",
      "Iteration: 10500 Training Accuracy: 0.984375 Loss: 0.0005531967035494745\n",
      "Iteration: 10510 Training Accuracy: 1.0 Loss: 0.0003449362120591104\n",
      "Iteration: 10520 Training Accuracy: 0.984375 Loss: 0.0006565783405676484\n",
      "Iteration: 10530 Training Accuracy: 1.0 Loss: 0.00015530831296928227\n",
      "Iteration: 10540 Training Accuracy: 1.0 Loss: 0.0005606276681646705\n",
      "Iteration: 10550 Training Accuracy: 0.96875 Loss: 0.0006143206264823675\n",
      "Iteration: 10560 Training Accuracy: 1.0 Loss: 0.0001617421512492001\n",
      "Iteration: 10570 Training Accuracy: 1.0 Loss: 0.0005087273311801255\n",
      "Iteration: 10580 Training Accuracy: 1.0 Loss: 0.0002188525249948725\n",
      "Iteration: 10590 Training Accuracy: 0.984375 Loss: 0.0010677354875952005\n",
      "Iteration: 10600 Training Accuracy: 1.0 Loss: 0.0004925002576783299\n",
      "Iteration: 10610 Training Accuracy: 1.0 Loss: 0.0001222582213813439\n",
      "Iteration: 10620 Training Accuracy: 1.0 Loss: 0.00025812280364334583\n",
      "Iteration: 10630 Training Accuracy: 0.984375 Loss: 0.0004978448268957436\n",
      "Iteration: 10640 Training Accuracy: 0.984375 Loss: 0.0009980523027479649\n",
      "Iteration: 10650 Training Accuracy: 1.0 Loss: 0.00026758419699035585\n",
      "Iteration: 10660 Training Accuracy: 1.0 Loss: 0.00010698169353418052\n",
      "Iteration: 10670 Training Accuracy: 1.0 Loss: 0.0004526481206994504\n",
      "Iteration: 10680 Training Accuracy: 0.984375 Loss: 0.0005141433794051409\n",
      "Iteration: 10690 Training Accuracy: 1.0 Loss: 0.0002472509804647416\n",
      "Iteration: 10700 Training Accuracy: 0.984375 Loss: 0.0007862751372158527\n",
      "Iteration: 10710 Training Accuracy: 0.96875 Loss: 0.00088857626542449\n",
      "Iteration: 10720 Training Accuracy: 1.0 Loss: 0.00032676811679266393\n",
      "Iteration: 10730 Training Accuracy: 0.96875 Loss: 0.0014300650218501687\n",
      "Iteration: 10740 Training Accuracy: 0.984375 Loss: 0.0008235068526118994\n",
      "Iteration: 10750 Training Accuracy: 1.0 Loss: 0.00030860336846672\n",
      "Iteration: 10760 Training Accuracy: 1.0 Loss: 0.00011905620340257883\n",
      "Iteration: 10770 Training Accuracy: 1.0 Loss: 0.0004531673912424594\n",
      "Iteration: 10780 Training Accuracy: 1.0 Loss: 0.00023189044441096485\n",
      "Iteration: 10790 Training Accuracy: 1.0 Loss: 0.00021460895368363708\n",
      "Iteration: 10800 Training Accuracy: 1.0 Loss: 0.0003668055578600615\n",
      "Iteration: 10810 Training Accuracy: 1.0 Loss: 0.0006256502238102257\n",
      "Iteration: 10820 Training Accuracy: 0.96875 Loss: 0.0005471898475661874\n",
      "Iteration: 10830 Training Accuracy: 0.984375 Loss: 0.0006186220562085509\n",
      "Iteration: 10840 Training Accuracy: 0.984375 Loss: 0.0008758380427025259\n",
      "Iteration: 10850 Training Accuracy: 0.984375 Loss: 0.0009137302404269576\n",
      "Iteration: 10860 Training Accuracy: 1.0 Loss: 0.0004109939618501812\n",
      "Iteration: 10870 Training Accuracy: 1.0 Loss: 0.000552356184925884\n",
      "Iteration: 10880 Training Accuracy: 1.0 Loss: 8.25498573249206e-05\n",
      "Iteration: 10890 Training Accuracy: 1.0 Loss: 0.0006013457896187901\n",
      "Iteration: 10900 Training Accuracy: 1.0 Loss: 0.00024659070186316967\n",
      "Iteration: 10910 Training Accuracy: 0.984375 Loss: 0.00030129493097774684\n",
      "Iteration: 10920 Training Accuracy: 1.0 Loss: 0.00040255673229694366\n",
      "Iteration: 10930 Training Accuracy: 1.0 Loss: 0.0002753293374553323\n",
      "Iteration: 10940 Training Accuracy: 1.0 Loss: 0.0003755674697458744\n",
      "Iteration: 10950 Training Accuracy: 0.984375 Loss: 0.0005783204105682671\n",
      "Iteration: 10960 Training Accuracy: 0.984375 Loss: 0.0002280819317093119\n",
      "Iteration: 10970 Training Accuracy: 1.0 Loss: 0.0004070908762514591\n",
      "Iteration: 10980 Training Accuracy: 0.984375 Loss: 0.00028408027719706297\n",
      "Iteration: 10990 Training Accuracy: 1.0 Loss: 0.00016187180881388485\n",
      "Iteration: 11000 Training Accuracy: 0.96875 Loss: 0.001371398102492094\n",
      "Iteration: 11010 Training Accuracy: 1.0 Loss: 0.00034316349774599075\n",
      "Iteration: 11020 Training Accuracy: 1.0 Loss: 0.000446020916569978\n",
      "Iteration: 11030 Training Accuracy: 0.96875 Loss: 0.0009149173856712878\n",
      "Iteration: 11040 Training Accuracy: 1.0 Loss: 0.00025382122839801013\n",
      "Iteration: 11050 Training Accuracy: 0.984375 Loss: 0.0003163171641062945\n",
      "Iteration: 11060 Training Accuracy: 1.0 Loss: 4.186081059742719e-05\n",
      "Iteration: 11070 Training Accuracy: 1.0 Loss: 0.00028090531122870743\n",
      "Iteration: 11080 Training Accuracy: 1.0 Loss: 0.00020087481243535876\n",
      "Iteration: 11090 Training Accuracy: 1.0 Loss: 0.00044074893230572343\n",
      "Iteration: 11100 Training Accuracy: 1.0 Loss: 0.0002362672530580312\n",
      "Iteration: 11110 Training Accuracy: 0.984375 Loss: 0.0006835953099653125\n",
      "Iteration: 11120 Training Accuracy: 0.984375 Loss: 0.0006617981707677245\n",
      "Iteration: 11130 Training Accuracy: 0.984375 Loss: 0.000286639085970819\n",
      "Iteration: 11140 Training Accuracy: 0.984375 Loss: 0.0005107243196107447\n",
      "Iteration: 11150 Training Accuracy: 1.0 Loss: 0.0003986658120993525\n",
      "Iteration: 11160 Training Accuracy: 0.984375 Loss: 0.0007692340295761824\n",
      "Iteration: 11170 Training Accuracy: 0.984375 Loss: 0.0008452917682006955\n",
      "Iteration: 11180 Training Accuracy: 0.984375 Loss: 0.0005189740913920105\n",
      "Iteration: 11190 Training Accuracy: 1.0 Loss: 0.00015510716184508055\n",
      "Iteration: 11200 Training Accuracy: 0.984375 Loss: 0.0009714576881378889\n",
      "Iteration: 11210 Training Accuracy: 1.0 Loss: 0.0002477333473507315\n",
      "Iteration: 11220 Training Accuracy: 1.0 Loss: 0.0001661048154346645\n",
      "Iteration: 11230 Training Accuracy: 0.96875 Loss: 0.0018065038602799177\n",
      "Iteration: 11240 Training Accuracy: 0.984375 Loss: 0.00036897091194987297\n",
      "Iteration: 11250 Training Accuracy: 1.0 Loss: 0.0002343825763091445\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 12\n",
      "Iteration: 11260 Training Accuracy: 1.0 Loss: 0.0006035012193024158\n",
      "Iteration: 11270 Training Accuracy: 1.0 Loss: 0.0003383141302037984\n",
      "Iteration: 11280 Training Accuracy: 1.0 Loss: 3.758055390790105e-05\n",
      "Iteration: 11290 Training Accuracy: 1.0 Loss: 0.00030067621264606714\n",
      "Iteration: 11300 Training Accuracy: 1.0 Loss: 0.00024304377438966185\n",
      "Iteration: 11310 Training Accuracy: 0.96875 Loss: 0.0009694636100903153\n",
      "Iteration: 11320 Training Accuracy: 1.0 Loss: 0.0002629439113661647\n",
      "Iteration: 11330 Training Accuracy: 1.0 Loss: 0.00046397524420171976\n",
      "Iteration: 11340 Training Accuracy: 1.0 Loss: 0.00020742152992170304\n",
      "Iteration: 11350 Training Accuracy: 0.984375 Loss: 0.0006914593977853656\n",
      "Iteration: 11360 Training Accuracy: 1.0 Loss: 0.00031114413286559284\n",
      "Iteration: 11370 Training Accuracy: 1.0 Loss: 0.0003772845957428217\n",
      "Iteration: 11380 Training Accuracy: 1.0 Loss: 0.0006797542446292937\n",
      "Iteration: 11390 Training Accuracy: 0.96875 Loss: 0.0012363115092739463\n",
      "Iteration: 11400 Training Accuracy: 1.0 Loss: 0.00034436213900335133\n",
      "Iteration: 11410 Training Accuracy: 1.0 Loss: 0.00023333283024840057\n",
      "Iteration: 11420 Training Accuracy: 1.0 Loss: 0.0002753046865109354\n",
      "Iteration: 11430 Training Accuracy: 0.984375 Loss: 0.00040800959686748683\n",
      "Iteration: 11440 Training Accuracy: 1.0 Loss: 0.0005526568274945021\n",
      "Iteration: 11450 Training Accuracy: 0.984375 Loss: 0.0008228417718783021\n",
      "Iteration: 11460 Training Accuracy: 0.984375 Loss: 0.0003228716377634555\n",
      "Iteration: 11470 Training Accuracy: 1.0 Loss: 0.0003847855841740966\n",
      "Iteration: 11480 Training Accuracy: 0.984375 Loss: 0.000302229163935408\n",
      "Iteration: 11490 Training Accuracy: 0.984375 Loss: 0.0006428591441363096\n",
      "Iteration: 11500 Training Accuracy: 0.96875 Loss: 0.0009382240241393447\n",
      "Iteration: 11510 Training Accuracy: 1.0 Loss: 0.0003749486058950424\n",
      "Iteration: 11520 Training Accuracy: 1.0 Loss: 0.00022709312906954437\n",
      "Iteration: 11530 Training Accuracy: 1.0 Loss: 0.00013825368660036474\n",
      "Iteration: 11540 Training Accuracy: 0.984375 Loss: 0.0007885480881668627\n",
      "Iteration: 11550 Training Accuracy: 1.0 Loss: 0.0003260960220359266\n",
      "Iteration: 11560 Training Accuracy: 0.984375 Loss: 0.0005107614560984075\n",
      "Iteration: 11570 Training Accuracy: 0.96875 Loss: 0.0010963375680148602\n",
      "Iteration: 11580 Training Accuracy: 1.0 Loss: 0.0004455872986000031\n",
      "Iteration: 11590 Training Accuracy: 0.984375 Loss: 0.0005684976349584758\n",
      "Iteration: 11600 Training Accuracy: 1.0 Loss: 0.0005188476643525064\n",
      "Iteration: 11610 Training Accuracy: 1.0 Loss: 3.927618308807723e-05\n",
      "Iteration: 11620 Training Accuracy: 1.0 Loss: 0.0003664321848191321\n",
      "Iteration: 11630 Training Accuracy: 1.0 Loss: 0.00034933516872115433\n",
      "Iteration: 11640 Training Accuracy: 0.984375 Loss: 0.0005988807533867657\n",
      "Iteration: 11650 Training Accuracy: 1.0 Loss: 0.0005668856902047992\n",
      "Iteration: 11660 Training Accuracy: 1.0 Loss: 0.00010563730029389262\n",
      "Iteration: 11670 Training Accuracy: 1.0 Loss: 0.00040269835153594613\n",
      "Iteration: 11680 Training Accuracy: 1.0 Loss: 0.00026833784068003297\n",
      "Iteration: 11690 Training Accuracy: 0.984375 Loss: 0.0005509329494088888\n",
      "Iteration: 11700 Training Accuracy: 0.984375 Loss: 0.0013673135545104742\n",
      "Iteration: 11710 Training Accuracy: 1.0 Loss: 0.00027669547125697136\n",
      "Iteration: 11720 Training Accuracy: 0.984375 Loss: 0.0007118522771634161\n",
      "Iteration: 11730 Training Accuracy: 1.0 Loss: 0.00037954008439555764\n",
      "Iteration: 11740 Training Accuracy: 0.953125 Loss: 0.001610774896107614\n",
      "Iteration: 11750 Training Accuracy: 1.0 Loss: 7.799368177074939e-05\n",
      "Iteration: 11760 Training Accuracy: 0.921875 Loss: 0.0020610473584383726\n",
      "Iteration: 11770 Training Accuracy: 0.96875 Loss: 0.0007691553328186274\n",
      "Iteration: 11780 Training Accuracy: 1.0 Loss: 0.00031753932125866413\n",
      "Iteration: 11790 Training Accuracy: 1.0 Loss: 0.0004739005526062101\n",
      "Iteration: 11800 Training Accuracy: 1.0 Loss: 0.0002333087322767824\n",
      "Iteration: 11810 Training Accuracy: 0.984375 Loss: 0.0009462118614464998\n",
      "Iteration: 11820 Training Accuracy: 1.0 Loss: 0.00025712596834637225\n",
      "Iteration: 11830 Training Accuracy: 1.0 Loss: 0.00017915188800543547\n",
      "Iteration: 11840 Training Accuracy: 1.0 Loss: 2.0256700736354105e-05\n",
      "Iteration: 11850 Training Accuracy: 0.984375 Loss: 0.0006139952456578612\n",
      "Iteration: 11860 Training Accuracy: 1.0 Loss: 0.00022607856953982264\n",
      "Iteration: 11870 Training Accuracy: 0.984375 Loss: 0.00046836782712489367\n",
      "Iteration: 11880 Training Accuracy: 1.0 Loss: 0.0005567316547967494\n",
      "Iteration: 11890 Training Accuracy: 0.984375 Loss: 0.0008122447761707008\n",
      "Iteration: 11900 Training Accuracy: 1.0 Loss: 0.00018546565843280405\n",
      "Iteration: 11910 Training Accuracy: 1.0 Loss: 0.00027504732133820653\n",
      "Iteration: 11920 Training Accuracy: 0.984375 Loss: 0.00047777622239664197\n",
      "Iteration: 11930 Training Accuracy: 1.0 Loss: 7.605107384733856e-05\n",
      "Iteration: 11940 Training Accuracy: 1.0 Loss: 0.00020242207392584532\n",
      "Iteration: 11950 Training Accuracy: 1.0 Loss: 0.00012051461089868098\n",
      "Iteration: 11960 Training Accuracy: 0.984375 Loss: 0.0006279451190494001\n",
      "Iteration: 11970 Training Accuracy: 0.984375 Loss: 0.0006631116266362369\n",
      "Iteration: 11980 Training Accuracy: 1.0 Loss: 0.0008790899300947785\n",
      "Iteration: 11990 Training Accuracy: 1.0 Loss: 0.00047925207763910294\n",
      "Iteration: 12000 Training Accuracy: 1.0 Loss: 0.00026783155044540763\n",
      "Iteration: 12010 Training Accuracy: 0.984375 Loss: 0.000706988968886435\n",
      "Iteration: 12020 Training Accuracy: 0.984375 Loss: 0.0005485095316544175\n",
      "Iteration: 12030 Training Accuracy: 0.984375 Loss: 0.0010584983974695206\n",
      "Iteration: 12040 Training Accuracy: 1.0 Loss: 0.0006049085059203207\n",
      "Iteration: 12050 Training Accuracy: 0.984375 Loss: 0.0012191570131108165\n",
      "Iteration: 12060 Training Accuracy: 1.0 Loss: 0.00029307298245839775\n",
      "Iteration: 12070 Training Accuracy: 1.0 Loss: 0.00029547495068982244\n",
      "Iteration: 12080 Training Accuracy: 1.0 Loss: 0.0002205902710556984\n",
      "Iteration: 12090 Training Accuracy: 0.96875 Loss: 0.0008311038836836815\n",
      "Iteration: 12100 Training Accuracy: 0.984375 Loss: 0.0008573948289267719\n",
      "Iteration: 12110 Training Accuracy: 0.984375 Loss: 0.0005865051061846316\n",
      "Iteration: 12120 Training Accuracy: 1.0 Loss: 0.00027694692835211754\n",
      "Iteration: 12130 Training Accuracy: 0.96875 Loss: 0.0007060192292556167\n",
      "Iteration: 12140 Training Accuracy: 1.0 Loss: 0.0004554447950795293\n",
      "Iteration: 12150 Training Accuracy: 0.984375 Loss: 0.0010539100039750338\n",
      "Iteration: 12160 Training Accuracy: 1.0 Loss: 0.0001639591937419027\n",
      "Iteration: 12170 Training Accuracy: 1.0 Loss: 0.0006639966159127653\n",
      "Iteration: 12180 Training Accuracy: 1.0 Loss: 0.0007995989872142673\n",
      "Iteration: 12190 Training Accuracy: 1.0 Loss: 0.0002240526519017294\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 13\n",
      "Iteration: 12200 Training Accuracy: 1.0 Loss: 0.00022986001567915082\n",
      "Iteration: 12210 Training Accuracy: 1.0 Loss: 0.00021540610759984702\n",
      "Iteration: 12220 Training Accuracy: 1.0 Loss: 0.00011537652608240023\n",
      "Iteration: 12230 Training Accuracy: 1.0 Loss: 0.0003300352254882455\n",
      "Iteration: 12240 Training Accuracy: 1.0 Loss: 0.0001979441149160266\n",
      "Iteration: 12250 Training Accuracy: 1.0 Loss: 0.00025582080706954\n",
      "Iteration: 12260 Training Accuracy: 0.984375 Loss: 0.0010907936375588179\n",
      "Iteration: 12270 Training Accuracy: 0.984375 Loss: 0.0008216979331336915\n",
      "Iteration: 12280 Training Accuracy: 0.984375 Loss: 0.001305379206314683\n",
      "Iteration: 12290 Training Accuracy: 0.984375 Loss: 0.0007205904694274068\n",
      "Iteration: 12300 Training Accuracy: 0.984375 Loss: 0.0011373523157089949\n",
      "Iteration: 12310 Training Accuracy: 1.0 Loss: 0.000194944761460647\n",
      "Iteration: 12320 Training Accuracy: 0.96875 Loss: 0.0009977128356695175\n",
      "Iteration: 12330 Training Accuracy: 1.0 Loss: 0.00016731861978769302\n",
      "Iteration: 12340 Training Accuracy: 0.984375 Loss: 0.0005405075498856604\n",
      "Iteration: 12350 Training Accuracy: 0.984375 Loss: 0.0009756940416991711\n",
      "Iteration: 12360 Training Accuracy: 1.0 Loss: 0.0004105765838176012\n",
      "Iteration: 12370 Training Accuracy: 0.984375 Loss: 0.0005498507525771856\n",
      "Iteration: 12380 Training Accuracy: 1.0 Loss: 0.0001502785598859191\n",
      "Iteration: 12390 Training Accuracy: 1.0 Loss: 0.0003532144764903933\n",
      "Iteration: 12400 Training Accuracy: 0.984375 Loss: 0.0008425135165452957\n",
      "Iteration: 12410 Training Accuracy: 0.984375 Loss: 0.0008489305037073791\n",
      "Iteration: 12420 Training Accuracy: 0.984375 Loss: 0.0009141035261563957\n",
      "Iteration: 12430 Training Accuracy: 1.0 Loss: 0.00020510636386461556\n",
      "Iteration: 12440 Training Accuracy: 0.984375 Loss: 0.0006869016797281802\n",
      "Iteration: 12450 Training Accuracy: 1.0 Loss: 0.00014187293709255755\n",
      "Iteration: 12460 Training Accuracy: 0.984375 Loss: 0.0004911392461508512\n",
      "Iteration: 12470 Training Accuracy: 0.984375 Loss: 0.0009305691346526146\n",
      "Iteration: 12480 Training Accuracy: 1.0 Loss: 9.80196928139776e-05\n",
      "Iteration: 12490 Training Accuracy: 0.96875 Loss: 0.0008626026101410389\n",
      "Iteration: 12500 Training Accuracy: 1.0 Loss: 0.00017905212007462978\n",
      "Iteration: 12510 Training Accuracy: 0.984375 Loss: 0.0005579183343797922\n",
      "Iteration: 12520 Training Accuracy: 1.0 Loss: 0.00019157363567501307\n",
      "Iteration: 12530 Training Accuracy: 0.984375 Loss: 0.0011250964598730206\n",
      "Iteration: 12540 Training Accuracy: 0.984375 Loss: 0.00045859284000471234\n",
      "Iteration: 12550 Training Accuracy: 0.984375 Loss: 0.00026749668177217245\n",
      "Iteration: 12560 Training Accuracy: 1.0 Loss: 0.00012615298328455538\n",
      "Iteration: 12570 Training Accuracy: 0.984375 Loss: 0.0007436650921590626\n",
      "Iteration: 12580 Training Accuracy: 0.984375 Loss: 0.0009672364685684443\n",
      "Iteration: 12590 Training Accuracy: 1.0 Loss: 0.0002477811649441719\n",
      "Iteration: 12600 Training Accuracy: 1.0 Loss: 0.00024646316887810826\n",
      "Iteration: 12610 Training Accuracy: 1.0 Loss: 0.0008530158665962517\n",
      "Iteration: 12620 Training Accuracy: 1.0 Loss: 0.00013981526717543602\n",
      "Iteration: 12630 Training Accuracy: 1.0 Loss: 0.00019351889204699546\n",
      "Iteration: 12640 Training Accuracy: 0.984375 Loss: 0.000535769562702626\n",
      "Iteration: 12650 Training Accuracy: 1.0 Loss: 0.00015489441284444183\n",
      "Iteration: 12660 Training Accuracy: 0.984375 Loss: 0.0006868264172226191\n",
      "Iteration: 12670 Training Accuracy: 0.984375 Loss: 0.0005827962886542082\n",
      "Iteration: 12680 Training Accuracy: 1.0 Loss: 0.00018390317563898861\n",
      "Iteration: 12690 Training Accuracy: 0.984375 Loss: 0.0010561177041381598\n",
      "Iteration: 12700 Training Accuracy: 0.984375 Loss: 0.0006382187129929662\n",
      "Iteration: 12710 Training Accuracy: 1.0 Loss: 0.00045289244735613465\n",
      "Iteration: 12720 Training Accuracy: 1.0 Loss: 0.00047074153553694487\n",
      "Iteration: 12730 Training Accuracy: 1.0 Loss: 0.0003858993004541844\n",
      "Iteration: 12740 Training Accuracy: 0.984375 Loss: 0.0011613559909164906\n",
      "Iteration: 12750 Training Accuracy: 1.0 Loss: 0.00011757377069443464\n",
      "Iteration: 12760 Training Accuracy: 1.0 Loss: 0.00010143540566787124\n",
      "Iteration: 12770 Training Accuracy: 1.0 Loss: 0.0004624449065886438\n",
      "Iteration: 12780 Training Accuracy: 0.984375 Loss: 0.0006044207257218659\n",
      "Iteration: 12790 Training Accuracy: 1.0 Loss: 0.00045247189700603485\n",
      "Iteration: 12800 Training Accuracy: 1.0 Loss: 0.00019695940136443824\n",
      "Iteration: 12810 Training Accuracy: 1.0 Loss: 0.00011069886386394501\n",
      "Iteration: 12820 Training Accuracy: 1.0 Loss: 8.094177610473707e-05\n",
      "Iteration: 12830 Training Accuracy: 1.0 Loss: 0.00019581362721510231\n",
      "Iteration: 12840 Training Accuracy: 0.953125 Loss: 0.0009167285170406103\n",
      "Iteration: 12850 Training Accuracy: 1.0 Loss: 0.0007005489314906299\n",
      "Iteration: 12860 Training Accuracy: 1.0 Loss: 0.0003451208758633584\n",
      "Iteration: 12870 Training Accuracy: 1.0 Loss: 0.0002627787762321532\n",
      "Iteration: 12880 Training Accuracy: 1.0 Loss: 0.0005597170675173402\n",
      "Iteration: 12890 Training Accuracy: 1.0 Loss: 0.0002964450977742672\n",
      "Iteration: 12900 Training Accuracy: 1.0 Loss: 8.510153566021472e-05\n",
      "Iteration: 12910 Training Accuracy: 0.984375 Loss: 0.0007103443494997919\n",
      "Iteration: 12920 Training Accuracy: 0.984375 Loss: 0.0007267810287885368\n",
      "Iteration: 12930 Training Accuracy: 0.96875 Loss: 0.0009213165612891316\n",
      "Iteration: 12940 Training Accuracy: 0.984375 Loss: 0.0007674680091440678\n",
      "Iteration: 12950 Training Accuracy: 1.0 Loss: 0.0003294323687441647\n",
      "Iteration: 12960 Training Accuracy: 0.984375 Loss: 0.0009261599625460804\n",
      "Iteration: 12970 Training Accuracy: 1.0 Loss: 0.00017101314733736217\n",
      "Iteration: 12980 Training Accuracy: 1.0 Loss: 0.00035624837619252503\n",
      "Iteration: 12990 Training Accuracy: 1.0 Loss: 0.0002748685365077108\n",
      "Iteration: 13000 Training Accuracy: 0.984375 Loss: 0.0003101103939116001\n",
      "Iteration: 13010 Training Accuracy: 1.0 Loss: 0.0001719421852612868\n",
      "Iteration: 13020 Training Accuracy: 1.0 Loss: 0.0007305602193810046\n",
      "Iteration: 13030 Training Accuracy: 1.0 Loss: 0.0005085772136226296\n",
      "Iteration: 13040 Training Accuracy: 0.984375 Loss: 0.0007649723556824028\n",
      "Iteration: 13050 Training Accuracy: 0.96875 Loss: 0.0011094496585428715\n",
      "Iteration: 13060 Training Accuracy: 0.984375 Loss: 0.0005200563464313745\n",
      "Iteration: 13070 Training Accuracy: 1.0 Loss: 0.00022464178618974984\n",
      "Iteration: 13080 Training Accuracy: 0.96875 Loss: 0.0012615893501788378\n",
      "Iteration: 13090 Training Accuracy: 1.0 Loss: 0.00024196886806748807\n",
      "Iteration: 13100 Training Accuracy: 1.0 Loss: 0.00039070745697245\n",
      "Iteration: 13110 Training Accuracy: 1.0 Loss: 0.0004716406110674143\n",
      "Iteration: 13120 Training Accuracy: 0.984375 Loss: 0.0005967182223685086\n",
      "Iteration: 13130 Training Accuracy: 0.96875 Loss: 0.0006562378257513046\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9288333333333333\n",
      "epoch: 14\n",
      "Iteration: 13140 Training Accuracy: 1.0 Loss: 0.00016786152264103293\n",
      "Iteration: 13150 Training Accuracy: 0.984375 Loss: 0.00044785375939682126\n",
      "Iteration: 13160 Training Accuracy: 0.984375 Loss: 0.0006723589613102376\n",
      "Iteration: 13170 Training Accuracy: 1.0 Loss: 0.0002810363075695932\n",
      "Iteration: 13180 Training Accuracy: 1.0 Loss: 0.0003728645679075271\n",
      "Iteration: 13190 Training Accuracy: 1.0 Loss: 0.0002492065541446209\n",
      "Iteration: 13200 Training Accuracy: 0.953125 Loss: 0.0016863812925294042\n",
      "Iteration: 13210 Training Accuracy: 1.0 Loss: 0.0004522721574176103\n",
      "Iteration: 13220 Training Accuracy: 0.984375 Loss: 0.0010200609685853124\n",
      "Iteration: 13230 Training Accuracy: 1.0 Loss: 0.0002438051742501557\n",
      "Iteration: 13240 Training Accuracy: 1.0 Loss: 0.00021384048159234226\n",
      "Iteration: 13250 Training Accuracy: 0.984375 Loss: 0.0008937121601775289\n",
      "Iteration: 13260 Training Accuracy: 1.0 Loss: 0.0003303582197986543\n",
      "Iteration: 13270 Training Accuracy: 1.0 Loss: 0.0006814075168222189\n",
      "Iteration: 13280 Training Accuracy: 0.984375 Loss: 0.0009900147560983896\n",
      "Iteration: 13290 Training Accuracy: 1.0 Loss: 0.00016358736320398748\n",
      "Iteration: 13300 Training Accuracy: 1.0 Loss: 0.0004873556608799845\n",
      "Iteration: 13310 Training Accuracy: 1.0 Loss: 0.0003173332952428609\n",
      "Iteration: 13320 Training Accuracy: 1.0 Loss: 0.00010573620966169983\n",
      "Iteration: 13330 Training Accuracy: 1.0 Loss: 0.00018566376820672303\n",
      "Iteration: 13340 Training Accuracy: 1.0 Loss: 2.2401691239792854e-05\n",
      "Iteration: 13350 Training Accuracy: 1.0 Loss: 0.000391337409382686\n",
      "Iteration: 13360 Training Accuracy: 0.984375 Loss: 0.0007692023064009845\n",
      "Iteration: 13370 Training Accuracy: 1.0 Loss: 0.00033826689468696713\n",
      "Iteration: 13380 Training Accuracy: 1.0 Loss: 0.00033869000617414713\n",
      "Iteration: 13390 Training Accuracy: 1.0 Loss: 0.0004682663129642606\n",
      "Iteration: 13400 Training Accuracy: 0.96875 Loss: 0.001473447191528976\n",
      "Iteration: 13410 Training Accuracy: 0.984375 Loss: 0.000701334560289979\n",
      "Iteration: 13420 Training Accuracy: 1.0 Loss: 0.00032771838596090674\n",
      "Iteration: 13430 Training Accuracy: 1.0 Loss: 0.0006737988442182541\n",
      "Iteration: 13440 Training Accuracy: 1.0 Loss: 0.0003274144255556166\n",
      "Iteration: 13450 Training Accuracy: 0.96875 Loss: 0.001166997360996902\n",
      "Iteration: 13460 Training Accuracy: 1.0 Loss: 0.00011192250531166792\n",
      "Iteration: 13470 Training Accuracy: 1.0 Loss: 7.052328874124214e-05\n",
      "Iteration: 13480 Training Accuracy: 1.0 Loss: 8.319859625771642e-05\n",
      "Iteration: 13490 Training Accuracy: 1.0 Loss: 7.26288853911683e-05\n",
      "Iteration: 13500 Training Accuracy: 0.96875 Loss: 0.001196910860016942\n",
      "Iteration: 13510 Training Accuracy: 0.984375 Loss: 0.001707888557575643\n",
      "Iteration: 13520 Training Accuracy: 0.984375 Loss: 0.001031725900247693\n",
      "Iteration: 13530 Training Accuracy: 0.984375 Loss: 0.0005484330467879772\n",
      "Iteration: 13540 Training Accuracy: 0.984375 Loss: 0.000507119984831661\n",
      "Iteration: 13550 Training Accuracy: 1.0 Loss: 6.370081973727793e-05\n",
      "Iteration: 13560 Training Accuracy: 0.984375 Loss: 0.0007011497509665787\n",
      "Iteration: 13570 Training Accuracy: 1.0 Loss: 0.00015405763406306505\n",
      "Iteration: 13580 Training Accuracy: 1.0 Loss: 0.0005001905956305563\n",
      "Iteration: 13590 Training Accuracy: 1.0 Loss: 7.262843428179622e-05\n",
      "Iteration: 13600 Training Accuracy: 0.984375 Loss: 0.0004325883637648076\n",
      "Iteration: 13610 Training Accuracy: 1.0 Loss: 0.0004885842790827155\n",
      "Iteration: 13620 Training Accuracy: 1.0 Loss: 0.00041370477993041277\n",
      "Iteration: 13630 Training Accuracy: 1.0 Loss: 0.00014447537250816822\n",
      "Iteration: 13640 Training Accuracy: 0.984375 Loss: 0.00038493028841912746\n",
      "Iteration: 13650 Training Accuracy: 1.0 Loss: 0.0005037101218476892\n",
      "Iteration: 13660 Training Accuracy: 1.0 Loss: 0.0004987952415831387\n",
      "Iteration: 13670 Training Accuracy: 0.984375 Loss: 0.0010861542541533709\n",
      "Iteration: 13680 Training Accuracy: 0.96875 Loss: 0.000857546110637486\n",
      "Iteration: 13690 Training Accuracy: 0.984375 Loss: 0.0009472744422964752\n",
      "Iteration: 13700 Training Accuracy: 1.0 Loss: 0.00033806165447458625\n",
      "Iteration: 13710 Training Accuracy: 0.984375 Loss: 0.0006542426417581737\n",
      "Iteration: 13720 Training Accuracy: 1.0 Loss: 0.0004378615121822804\n",
      "Iteration: 13730 Training Accuracy: 1.0 Loss: 0.0001891251013148576\n",
      "Iteration: 13740 Training Accuracy: 1.0 Loss: 0.00013355666305869818\n",
      "Iteration: 13750 Training Accuracy: 1.0 Loss: 0.00013930987915955484\n",
      "Iteration: 13760 Training Accuracy: 1.0 Loss: 0.0004103725077584386\n",
      "Iteration: 13770 Training Accuracy: 1.0 Loss: 0.00033058004919439554\n",
      "Iteration: 13780 Training Accuracy: 1.0 Loss: 0.00017361600475851446\n",
      "Iteration: 13790 Training Accuracy: 0.984375 Loss: 0.0009214218007400632\n",
      "Iteration: 13800 Training Accuracy: 0.9375 Loss: 0.00201185280457139\n",
      "Iteration: 13810 Training Accuracy: 1.0 Loss: 0.00013305053289514035\n",
      "Iteration: 13820 Training Accuracy: 0.984375 Loss: 0.0006657940102741122\n",
      "Iteration: 13830 Training Accuracy: 1.0 Loss: 0.00011027429718524218\n",
      "Iteration: 13840 Training Accuracy: 0.984375 Loss: 0.0008926594164222479\n",
      "Iteration: 13850 Training Accuracy: 0.96875 Loss: 0.0008476505754515529\n",
      "Iteration: 13860 Training Accuracy: 1.0 Loss: 0.0001263635786017403\n",
      "Iteration: 13870 Training Accuracy: 1.0 Loss: 0.00036099867429584265\n",
      "Iteration: 13880 Training Accuracy: 0.984375 Loss: 0.0006931422976776958\n",
      "Iteration: 13890 Training Accuracy: 1.0 Loss: 0.00046700469101779163\n",
      "Iteration: 13900 Training Accuracy: 0.984375 Loss: 0.00045986910117790103\n",
      "Iteration: 13910 Training Accuracy: 0.96875 Loss: 0.0008146212785504758\n",
      "Iteration: 13920 Training Accuracy: 1.0 Loss: 0.0001588480081409216\n",
      "Iteration: 13930 Training Accuracy: 1.0 Loss: 0.0005530812195502222\n",
      "Iteration: 13940 Training Accuracy: 0.984375 Loss: 0.0004735874244943261\n",
      "Iteration: 13950 Training Accuracy: 0.984375 Loss: 0.0007011743728071451\n",
      "Iteration: 13960 Training Accuracy: 1.0 Loss: 0.00013408507220447063\n",
      "Iteration: 13970 Training Accuracy: 1.0 Loss: 0.00015771693142596632\n",
      "Iteration: 13980 Training Accuracy: 1.0 Loss: 0.00023524882271885872\n",
      "Iteration: 13990 Training Accuracy: 1.0 Loss: 0.0003370744816493243\n",
      "Iteration: 14000 Training Accuracy: 0.984375 Loss: 0.0005483716959133744\n",
      "Iteration: 14010 Training Accuracy: 0.984375 Loss: 0.0004720003926195204\n",
      "Iteration: 14020 Training Accuracy: 1.0 Loss: 0.00048634535050950944\n",
      "Iteration: 14030 Training Accuracy: 1.0 Loss: 0.0001855487935245037\n",
      "Iteration: 14040 Training Accuracy: 0.984375 Loss: 0.0004641448031179607\n",
      "Iteration: 14050 Training Accuracy: 1.0 Loss: 0.00034796440741047263\n",
      "Iteration: 14060 Training Accuracy: 1.0 Loss: 0.000612317759077996\n",
      "Iteration: 14070 Training Accuracy: 1.0 Loss: 0.00018194095173384994\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9288333333333333\n",
      "epoch: 15\n",
      "Iteration: 14080 Training Accuracy: 0.984375 Loss: 0.000772591563872993\n",
      "Iteration: 14090 Training Accuracy: 1.0 Loss: 0.00011915152572328225\n",
      "Iteration: 14100 Training Accuracy: 0.984375 Loss: 0.0006753908237442374\n",
      "Iteration: 14110 Training Accuracy: 0.984375 Loss: 0.0008398873033002019\n",
      "Iteration: 14120 Training Accuracy: 0.984375 Loss: 0.0006991839036345482\n",
      "Iteration: 14130 Training Accuracy: 0.984375 Loss: 0.0006599125335924327\n",
      "Iteration: 14140 Training Accuracy: 1.0 Loss: 0.0003855276736430824\n",
      "Iteration: 14150 Training Accuracy: 0.984375 Loss: 0.00102399499155581\n",
      "Iteration: 14160 Training Accuracy: 0.984375 Loss: 0.0007884195074439049\n",
      "Iteration: 14170 Training Accuracy: 1.0 Loss: 0.00031557740294374526\n",
      "Iteration: 14180 Training Accuracy: 0.984375 Loss: 0.0006746781291440129\n",
      "Iteration: 14190 Training Accuracy: 1.0 Loss: 0.00022019410971552134\n",
      "Iteration: 14200 Training Accuracy: 0.984375 Loss: 0.0008204596233554184\n",
      "Iteration: 14210 Training Accuracy: 0.984375 Loss: 0.0007903017685748637\n",
      "Iteration: 14220 Training Accuracy: 0.984375 Loss: 0.0004475556779652834\n",
      "Iteration: 14230 Training Accuracy: 1.0 Loss: 0.0002569100761320442\n",
      "Iteration: 14240 Training Accuracy: 0.984375 Loss: 0.000667905667796731\n",
      "Iteration: 14250 Training Accuracy: 0.96875 Loss: 0.001331092556938529\n",
      "Iteration: 14260 Training Accuracy: 1.0 Loss: 0.0002389292058069259\n",
      "Iteration: 14270 Training Accuracy: 0.984375 Loss: 0.0006521305185742676\n",
      "Iteration: 14280 Training Accuracy: 1.0 Loss: 0.00047985321725718677\n",
      "Iteration: 14290 Training Accuracy: 1.0 Loss: 0.00032455098698846996\n",
      "Iteration: 14300 Training Accuracy: 0.984375 Loss: 0.0007623478304594755\n",
      "Iteration: 14310 Training Accuracy: 0.984375 Loss: 0.0006978170713409781\n",
      "Iteration: 14320 Training Accuracy: 1.0 Loss: 0.00019674989744089544\n",
      "Iteration: 14330 Training Accuracy: 1.0 Loss: 0.00012809623149223626\n",
      "Iteration: 14340 Training Accuracy: 1.0 Loss: 0.00010330329678254202\n",
      "Iteration: 14350 Training Accuracy: 0.984375 Loss: 0.0006011016666889191\n",
      "Iteration: 14360 Training Accuracy: 0.953125 Loss: 0.0009444355382584035\n",
      "Iteration: 14370 Training Accuracy: 0.984375 Loss: 0.0007392737315967679\n",
      "Iteration: 14380 Training Accuracy: 0.96875 Loss: 0.0017895050114020705\n",
      "Iteration: 14390 Training Accuracy: 0.984375 Loss: 0.0006931116804480553\n",
      "Iteration: 14400 Training Accuracy: 1.0 Loss: 0.0003221878141630441\n",
      "Iteration: 14410 Training Accuracy: 1.0 Loss: 7.598248339490965e-05\n",
      "Iteration: 14420 Training Accuracy: 1.0 Loss: 0.00033025117591023445\n",
      "Iteration: 14430 Training Accuracy: 0.984375 Loss: 0.0005334055749699473\n",
      "Iteration: 14440 Training Accuracy: 1.0 Loss: 0.0004213419451843947\n",
      "Iteration: 14450 Training Accuracy: 0.984375 Loss: 0.001159911509603262\n",
      "Iteration: 14460 Training Accuracy: 0.984375 Loss: 0.0008281053742393851\n",
      "Iteration: 14470 Training Accuracy: 1.0 Loss: 0.0003630053251981735\n",
      "Iteration: 14480 Training Accuracy: 1.0 Loss: 0.0001133677942561917\n",
      "Iteration: 14490 Training Accuracy: 1.0 Loss: 0.000307033013086766\n",
      "Iteration: 14500 Training Accuracy: 1.0 Loss: 0.00027563911862671375\n",
      "Iteration: 14510 Training Accuracy: 0.984375 Loss: 0.00028824355104006827\n",
      "Iteration: 14520 Training Accuracy: 1.0 Loss: 0.0004608738236129284\n",
      "Iteration: 14530 Training Accuracy: 1.0 Loss: 0.00018756467034108937\n",
      "Iteration: 14540 Training Accuracy: 1.0 Loss: 0.00022651220206171274\n",
      "Iteration: 14550 Training Accuracy: 1.0 Loss: 9.246378613170236e-05\n",
      "Iteration: 14560 Training Accuracy: 1.0 Loss: 0.000393735128454864\n",
      "Iteration: 14570 Training Accuracy: 0.984375 Loss: 0.0007487554103136063\n",
      "Iteration: 14580 Training Accuracy: 1.0 Loss: 0.00015713248285464942\n",
      "Iteration: 14590 Training Accuracy: 1.0 Loss: 0.0002033807832049206\n",
      "Iteration: 14600 Training Accuracy: 1.0 Loss: 0.00023856520419940352\n",
      "Iteration: 14610 Training Accuracy: 1.0 Loss: 0.00018668343545868993\n",
      "Iteration: 14620 Training Accuracy: 1.0 Loss: 0.00041055440669879317\n",
      "Iteration: 14630 Training Accuracy: 1.0 Loss: 0.0004172478511463851\n",
      "Iteration: 14640 Training Accuracy: 0.96875 Loss: 0.001213352894410491\n",
      "Iteration: 14650 Training Accuracy: 1.0 Loss: 0.00041291111847385764\n",
      "Iteration: 14660 Training Accuracy: 1.0 Loss: 0.00014650463708676398\n",
      "Iteration: 14670 Training Accuracy: 0.984375 Loss: 0.000567074166610837\n",
      "Iteration: 14680 Training Accuracy: 1.0 Loss: 7.513379387091845e-05\n",
      "Iteration: 14690 Training Accuracy: 0.984375 Loss: 0.0013074154267087579\n",
      "Iteration: 14700 Training Accuracy: 1.0 Loss: 0.00015729117149021477\n",
      "Iteration: 14710 Training Accuracy: 1.0 Loss: 0.00030999293085187674\n",
      "Iteration: 14720 Training Accuracy: 1.0 Loss: 0.0003500708262436092\n",
      "Iteration: 14730 Training Accuracy: 0.984375 Loss: 0.0007536250632256269\n",
      "Iteration: 14740 Training Accuracy: 1.0 Loss: 0.00011722707859007642\n",
      "Iteration: 14750 Training Accuracy: 0.984375 Loss: 0.0006252677994780242\n",
      "Iteration: 14760 Training Accuracy: 1.0 Loss: 0.00013931639841757715\n",
      "Iteration: 14770 Training Accuracy: 1.0 Loss: 0.00018973657279275358\n",
      "Iteration: 14780 Training Accuracy: 1.0 Loss: 0.0001521361991763115\n",
      "Iteration: 14790 Training Accuracy: 1.0 Loss: 0.00034329225309193134\n",
      "Iteration: 14800 Training Accuracy: 0.984375 Loss: 0.0010545493569225073\n",
      "Iteration: 14810 Training Accuracy: 1.0 Loss: 0.0003980528563261032\n",
      "Iteration: 14820 Training Accuracy: 0.96875 Loss: 0.0005992931546643376\n",
      "Iteration: 14830 Training Accuracy: 0.984375 Loss: 0.0007477318868041039\n",
      "Iteration: 14840 Training Accuracy: 1.0 Loss: 0.00018813565839082003\n",
      "Iteration: 14850 Training Accuracy: 0.984375 Loss: 0.00099136121571064\n",
      "Iteration: 14860 Training Accuracy: 1.0 Loss: 0.0002463203272782266\n",
      "Iteration: 14870 Training Accuracy: 0.984375 Loss: 0.00085086451144889\n",
      "Iteration: 14880 Training Accuracy: 0.96875 Loss: 0.001396095729433\n",
      "Iteration: 14890 Training Accuracy: 0.984375 Loss: 0.0005235682474449277\n",
      "Iteration: 14900 Training Accuracy: 0.984375 Loss: 0.0006757600349374115\n",
      "Iteration: 14910 Training Accuracy: 0.984375 Loss: 0.0006512464606203139\n",
      "Iteration: 14920 Training Accuracy: 1.0 Loss: 0.0003261592937633395\n",
      "Iteration: 14930 Training Accuracy: 1.0 Loss: 0.00011338029435137287\n",
      "Iteration: 14940 Training Accuracy: 1.0 Loss: 0.0004528521094471216\n",
      "Iteration: 14950 Training Accuracy: 1.0 Loss: 0.0003641472430899739\n",
      "Iteration: 14960 Training Accuracy: 1.0 Loss: 0.0004983381950296462\n",
      "Iteration: 14970 Training Accuracy: 0.984375 Loss: 0.0004957784549333155\n",
      "Iteration: 14980 Training Accuracy: 0.984375 Loss: 0.0010040313936769962\n",
      "Iteration: 14990 Training Accuracy: 1.0 Loss: 0.00022401276510208845\n",
      "Iteration: 15000 Training Accuracy: 1.0 Loss: 7.500173524022102e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 16\n",
      "Iteration: 15010 Training Accuracy: 1.0 Loss: 0.0006493133259937167\n",
      "Iteration: 15020 Training Accuracy: 0.984375 Loss: 0.0009026555344462395\n",
      "Iteration: 15030 Training Accuracy: 0.96875 Loss: 0.0007484693778678775\n",
      "Iteration: 15040 Training Accuracy: 0.984375 Loss: 0.0014899957459419966\n",
      "Iteration: 15050 Training Accuracy: 1.0 Loss: 0.00043456320418044925\n",
      "Iteration: 15060 Training Accuracy: 0.96875 Loss: 0.001304940553382039\n",
      "Iteration: 15070 Training Accuracy: 1.0 Loss: 5.4295771406032145e-05\n",
      "Iteration: 15080 Training Accuracy: 1.0 Loss: 0.0002355293108848855\n",
      "Iteration: 15090 Training Accuracy: 1.0 Loss: 8.85547706275247e-05\n",
      "Iteration: 15100 Training Accuracy: 0.984375 Loss: 0.0006216439069248736\n",
      "Iteration: 15110 Training Accuracy: 0.96875 Loss: 0.000995134119875729\n",
      "Iteration: 15120 Training Accuracy: 0.984375 Loss: 0.00043884472688660026\n",
      "Iteration: 15130 Training Accuracy: 1.0 Loss: 0.0002337139012524858\n",
      "Iteration: 15140 Training Accuracy: 0.984375 Loss: 0.001229769317433238\n",
      "Iteration: 15150 Training Accuracy: 0.984375 Loss: 0.0004709202912636101\n",
      "Iteration: 15160 Training Accuracy: 1.0 Loss: 0.00025462647317908704\n",
      "Iteration: 15170 Training Accuracy: 1.0 Loss: 0.0005045623402111232\n",
      "Iteration: 15180 Training Accuracy: 1.0 Loss: 0.00019107747357338667\n",
      "Iteration: 15190 Training Accuracy: 0.984375 Loss: 0.0005304776132106781\n",
      "Iteration: 15200 Training Accuracy: 1.0 Loss: 0.000339881400577724\n",
      "Iteration: 15210 Training Accuracy: 0.984375 Loss: 0.0006327502778731287\n",
      "Iteration: 15220 Training Accuracy: 1.0 Loss: 0.00015233918384183198\n",
      "Iteration: 15230 Training Accuracy: 1.0 Loss: 0.0005530485650524497\n",
      "Iteration: 15240 Training Accuracy: 0.96875 Loss: 0.0005903062410652637\n",
      "Iteration: 15250 Training Accuracy: 1.0 Loss: 0.0001615759392734617\n",
      "Iteration: 15260 Training Accuracy: 1.0 Loss: 0.0005108175100758672\n",
      "Iteration: 15270 Training Accuracy: 1.0 Loss: 0.0002095093805110082\n",
      "Iteration: 15280 Training Accuracy: 0.984375 Loss: 0.0010253400541841984\n",
      "Iteration: 15290 Training Accuracy: 1.0 Loss: 0.00048382539534941316\n",
      "Iteration: 15300 Training Accuracy: 1.0 Loss: 0.00012041723675793037\n",
      "Iteration: 15310 Training Accuracy: 1.0 Loss: 0.00026082817930728197\n",
      "Iteration: 15320 Training Accuracy: 0.984375 Loss: 0.00048529705964028835\n",
      "Iteration: 15330 Training Accuracy: 0.984375 Loss: 0.0009798886021599174\n",
      "Iteration: 15340 Training Accuracy: 1.0 Loss: 0.0002578140120021999\n",
      "Iteration: 15350 Training Accuracy: 1.0 Loss: 0.00010412316623842344\n",
      "Iteration: 15360 Training Accuracy: 1.0 Loss: 0.00045045383740216494\n",
      "Iteration: 15370 Training Accuracy: 0.984375 Loss: 0.0005090346094220877\n",
      "Iteration: 15380 Training Accuracy: 1.0 Loss: 0.000233529630349949\n",
      "Iteration: 15390 Training Accuracy: 0.984375 Loss: 0.0007684073643758893\n",
      "Iteration: 15400 Training Accuracy: 0.96875 Loss: 0.0008566399337723851\n",
      "Iteration: 15410 Training Accuracy: 1.0 Loss: 0.0003133536083623767\n",
      "Iteration: 15420 Training Accuracy: 0.96875 Loss: 0.0013902081409469247\n",
      "Iteration: 15430 Training Accuracy: 0.984375 Loss: 0.0007953224703669548\n",
      "Iteration: 15440 Training Accuracy: 1.0 Loss: 0.0002824516559485346\n",
      "Iteration: 15450 Training Accuracy: 1.0 Loss: 0.00011949524923693389\n",
      "Iteration: 15460 Training Accuracy: 1.0 Loss: 0.0004471805295906961\n",
      "Iteration: 15470 Training Accuracy: 1.0 Loss: 0.00022761855507269502\n",
      "Iteration: 15480 Training Accuracy: 1.0 Loss: 0.00021253793966025114\n",
      "Iteration: 15490 Training Accuracy: 1.0 Loss: 0.0003612091240938753\n",
      "Iteration: 15500 Training Accuracy: 1.0 Loss: 0.0006132720736786723\n",
      "Iteration: 15510 Training Accuracy: 0.984375 Loss: 0.0005295910523273051\n",
      "Iteration: 15520 Training Accuracy: 0.984375 Loss: 0.0006022956222295761\n",
      "Iteration: 15530 Training Accuracy: 0.984375 Loss: 0.0008449317538179457\n",
      "Iteration: 15540 Training Accuracy: 0.984375 Loss: 0.0008886215509846807\n",
      "Iteration: 15550 Training Accuracy: 1.0 Loss: 0.000400772609282285\n",
      "Iteration: 15560 Training Accuracy: 1.0 Loss: 0.0005412659957073629\n",
      "Iteration: 15570 Training Accuracy: 1.0 Loss: 8.196188719011843e-05\n",
      "Iteration: 15580 Training Accuracy: 1.0 Loss: 0.0005719647160731256\n",
      "Iteration: 15590 Training Accuracy: 1.0 Loss: 0.00024715004838071764\n",
      "Iteration: 15600 Training Accuracy: 0.984375 Loss: 0.0002895302604883909\n",
      "Iteration: 15610 Training Accuracy: 1.0 Loss: 0.00039540932630188763\n",
      "Iteration: 15620 Training Accuracy: 1.0 Loss: 0.0002650875539984554\n",
      "Iteration: 15630 Training Accuracy: 1.0 Loss: 0.00037055733264423907\n",
      "Iteration: 15640 Training Accuracy: 0.984375 Loss: 0.0005598642164841294\n",
      "Iteration: 15650 Training Accuracy: 0.984375 Loss: 0.00021882174769416451\n",
      "Iteration: 15660 Training Accuracy: 1.0 Loss: 0.0003860090218950063\n",
      "Iteration: 15670 Training Accuracy: 0.984375 Loss: 0.00027099065482616425\n",
      "Iteration: 15680 Training Accuracy: 1.0 Loss: 0.00015655216702725738\n",
      "Iteration: 15690 Training Accuracy: 0.96875 Loss: 0.0013371844543144107\n",
      "Iteration: 15700 Training Accuracy: 1.0 Loss: 0.00032564514549449086\n",
      "Iteration: 15710 Training Accuracy: 1.0 Loss: 0.0004416978918015957\n",
      "Iteration: 15720 Training Accuracy: 0.96875 Loss: 0.0008887803414836526\n",
      "Iteration: 15730 Training Accuracy: 1.0 Loss: 0.0002558840496931225\n",
      "Iteration: 15740 Training Accuracy: 0.984375 Loss: 0.00031112192664295435\n",
      "Iteration: 15750 Training Accuracy: 1.0 Loss: 4.165300924796611e-05\n",
      "Iteration: 15760 Training Accuracy: 1.0 Loss: 0.00027133728144690394\n",
      "Iteration: 15770 Training Accuracy: 1.0 Loss: 0.0001960666268132627\n",
      "Iteration: 15780 Training Accuracy: 1.0 Loss: 0.0004342055763117969\n",
      "Iteration: 15790 Training Accuracy: 1.0 Loss: 0.00023194204550236464\n",
      "Iteration: 15800 Training Accuracy: 0.984375 Loss: 0.0006720488891005516\n",
      "Iteration: 15810 Training Accuracy: 0.984375 Loss: 0.0006463142344728112\n",
      "Iteration: 15820 Training Accuracy: 0.984375 Loss: 0.0002821277594193816\n",
      "Iteration: 15830 Training Accuracy: 0.984375 Loss: 0.0005010039894841611\n",
      "Iteration: 15840 Training Accuracy: 1.0 Loss: 0.0003885031910613179\n",
      "Iteration: 15850 Training Accuracy: 0.984375 Loss: 0.0007547032437287271\n",
      "Iteration: 15860 Training Accuracy: 0.984375 Loss: 0.0008221534080803394\n",
      "Iteration: 15870 Training Accuracy: 0.984375 Loss: 0.0005101742572151124\n",
      "Iteration: 15880 Training Accuracy: 1.0 Loss: 0.00015219772467389703\n",
      "Iteration: 15890 Training Accuracy: 0.984375 Loss: 0.0009447658667340875\n",
      "Iteration: 15900 Training Accuracy: 1.0 Loss: 0.0002410758170299232\n",
      "Iteration: 15910 Training Accuracy: 1.0 Loss: 0.000162430529599078\n",
      "Iteration: 15920 Training Accuracy: 0.96875 Loss: 0.0017591435462236404\n",
      "Iteration: 15930 Training Accuracy: 0.984375 Loss: 0.00035883919917978346\n",
      "Iteration: 15940 Training Accuracy: 1.0 Loss: 0.00022702146088704467\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 17\n",
      "Iteration: 15950 Training Accuracy: 1.0 Loss: 0.000588166294619441\n",
      "Iteration: 15960 Training Accuracy: 1.0 Loss: 0.0003272067697253078\n",
      "Iteration: 15970 Training Accuracy: 1.0 Loss: 3.671826561912894e-05\n",
      "Iteration: 15980 Training Accuracy: 1.0 Loss: 0.00029797726892866194\n",
      "Iteration: 15990 Training Accuracy: 1.0 Loss: 0.0002385254920227453\n",
      "Iteration: 16000 Training Accuracy: 0.96875 Loss: 0.0009549807291477919\n",
      "Iteration: 16010 Training Accuracy: 1.0 Loss: 0.0002592405362520367\n",
      "Iteration: 16020 Training Accuracy: 1.0 Loss: 0.00045419292291626334\n",
      "Iteration: 16030 Training Accuracy: 1.0 Loss: 0.00020967322052456439\n",
      "Iteration: 16040 Training Accuracy: 0.984375 Loss: 0.0006784693105146289\n",
      "Iteration: 16050 Training Accuracy: 1.0 Loss: 0.0003074253909289837\n",
      "Iteration: 16060 Training Accuracy: 1.0 Loss: 0.00038208620389923453\n",
      "Iteration: 16070 Training Accuracy: 1.0 Loss: 0.0006698686629533768\n",
      "Iteration: 16080 Training Accuracy: 0.96875 Loss: 0.0012105642817914486\n",
      "Iteration: 16090 Training Accuracy: 1.0 Loss: 0.0003420314169488847\n",
      "Iteration: 16100 Training Accuracy: 1.0 Loss: 0.00022941520728636533\n",
      "Iteration: 16110 Training Accuracy: 1.0 Loss: 0.0002685647923499346\n",
      "Iteration: 16120 Training Accuracy: 0.984375 Loss: 0.0003816463577095419\n",
      "Iteration: 16130 Training Accuracy: 1.0 Loss: 0.0005483574350364506\n",
      "Iteration: 16140 Training Accuracy: 0.984375 Loss: 0.0008168000495061278\n",
      "Iteration: 16150 Training Accuracy: 0.984375 Loss: 0.0003148903779219836\n",
      "Iteration: 16160 Training Accuracy: 1.0 Loss: 0.00038034687167964876\n",
      "Iteration: 16170 Training Accuracy: 0.984375 Loss: 0.00029972614720463753\n",
      "Iteration: 16180 Training Accuracy: 0.984375 Loss: 0.0006425300962291658\n",
      "Iteration: 16190 Training Accuracy: 0.96875 Loss: 0.0009238518541678786\n",
      "Iteration: 16200 Training Accuracy: 1.0 Loss: 0.0003647914563771337\n",
      "Iteration: 16210 Training Accuracy: 1.0 Loss: 0.0002203041221946478\n",
      "Iteration: 16220 Training Accuracy: 1.0 Loss: 0.00014067137090023607\n",
      "Iteration: 16230 Training Accuracy: 0.984375 Loss: 0.0007576137431897223\n",
      "Iteration: 16240 Training Accuracy: 1.0 Loss: 0.0003146749804727733\n",
      "Iteration: 16250 Training Accuracy: 0.984375 Loss: 0.0004921233048662543\n",
      "Iteration: 16260 Training Accuracy: 0.96875 Loss: 0.001067445264197886\n",
      "Iteration: 16270 Training Accuracy: 1.0 Loss: 0.00044539597001858056\n",
      "Iteration: 16280 Training Accuracy: 0.984375 Loss: 0.0005706943338736892\n",
      "Iteration: 16290 Training Accuracy: 1.0 Loss: 0.0005042641423642635\n",
      "Iteration: 16300 Training Accuracy: 1.0 Loss: 3.728075171238743e-05\n",
      "Iteration: 16310 Training Accuracy: 1.0 Loss: 0.00036016531521454453\n",
      "Iteration: 16320 Training Accuracy: 1.0 Loss: 0.00035339960595592856\n",
      "Iteration: 16330 Training Accuracy: 0.984375 Loss: 0.0005959151894785464\n",
      "Iteration: 16340 Training Accuracy: 1.0 Loss: 0.0005481641856022179\n",
      "Iteration: 16350 Training Accuracy: 1.0 Loss: 0.00010569315782049671\n",
      "Iteration: 16360 Training Accuracy: 1.0 Loss: 0.0004020561173092574\n",
      "Iteration: 16370 Training Accuracy: 1.0 Loss: 0.00026083755074068904\n",
      "Iteration: 16380 Training Accuracy: 0.984375 Loss: 0.0005410460289567709\n",
      "Iteration: 16390 Training Accuracy: 0.984375 Loss: 0.0013540664222091436\n",
      "Iteration: 16400 Training Accuracy: 1.0 Loss: 0.0002768251288216561\n",
      "Iteration: 16410 Training Accuracy: 0.984375 Loss: 0.0006868412019684911\n",
      "Iteration: 16420 Training Accuracy: 0.984375 Loss: 0.0003765521105378866\n",
      "Iteration: 16430 Training Accuracy: 0.953125 Loss: 0.001565795042552054\n",
      "Iteration: 16440 Training Accuracy: 1.0 Loss: 7.694494706811383e-05\n",
      "Iteration: 16450 Training Accuracy: 0.921875 Loss: 0.002013671211898327\n",
      "Iteration: 16460 Training Accuracy: 0.96875 Loss: 0.0007512527517974377\n",
      "Iteration: 16470 Training Accuracy: 1.0 Loss: 0.00030550832161679864\n",
      "Iteration: 16480 Training Accuracy: 1.0 Loss: 0.00046665628906339407\n",
      "Iteration: 16490 Training Accuracy: 1.0 Loss: 0.00022940788767300546\n",
      "Iteration: 16500 Training Accuracy: 0.984375 Loss: 0.0009396737441420555\n",
      "Iteration: 16510 Training Accuracy: 1.0 Loss: 0.000253738253377378\n",
      "Iteration: 16520 Training Accuracy: 1.0 Loss: 0.00017634499818086624\n",
      "Iteration: 16530 Training Accuracy: 1.0 Loss: 2.014568235608749e-05\n",
      "Iteration: 16540 Training Accuracy: 1.0 Loss: 0.0006006143521517515\n",
      "Iteration: 16550 Training Accuracy: 1.0 Loss: 0.00022778453421778977\n",
      "Iteration: 16560 Training Accuracy: 0.984375 Loss: 0.0004558291402645409\n",
      "Iteration: 16570 Training Accuracy: 1.0 Loss: 0.0005475710495375097\n",
      "Iteration: 16580 Training Accuracy: 0.984375 Loss: 0.000785701151471585\n",
      "Iteration: 16590 Training Accuracy: 1.0 Loss: 0.0001798408047761768\n",
      "Iteration: 16600 Training Accuracy: 1.0 Loss: 0.00026860146317631006\n",
      "Iteration: 16610 Training Accuracy: 0.984375 Loss: 0.00045945384772494435\n",
      "Iteration: 16620 Training Accuracy: 1.0 Loss: 7.448017277056351e-05\n",
      "Iteration: 16630 Training Accuracy: 1.0 Loss: 0.00019635578792076558\n",
      "Iteration: 16640 Training Accuracy: 1.0 Loss: 0.0001198098179884255\n",
      "Iteration: 16650 Training Accuracy: 0.984375 Loss: 0.0006191408610902727\n",
      "Iteration: 16660 Training Accuracy: 0.984375 Loss: 0.000651104433927685\n",
      "Iteration: 16670 Training Accuracy: 1.0 Loss: 0.0008538998081348836\n",
      "Iteration: 16680 Training Accuracy: 1.0 Loss: 0.0004747906350530684\n",
      "Iteration: 16690 Training Accuracy: 1.0 Loss: 0.000260851695202291\n",
      "Iteration: 16700 Training Accuracy: 0.984375 Loss: 0.000699151074513793\n",
      "Iteration: 16710 Training Accuracy: 0.984375 Loss: 0.0005320997443050146\n",
      "Iteration: 16720 Training Accuracy: 0.984375 Loss: 0.001029154285788536\n",
      "Iteration: 16730 Training Accuracy: 1.0 Loss: 0.000590802519582212\n",
      "Iteration: 16740 Training Accuracy: 0.984375 Loss: 0.0012016108958050609\n",
      "Iteration: 16750 Training Accuracy: 1.0 Loss: 0.00028954673325642943\n",
      "Iteration: 16760 Training Accuracy: 1.0 Loss: 0.00029019141220487654\n",
      "Iteration: 16770 Training Accuracy: 1.0 Loss: 0.00021793294581584632\n",
      "Iteration: 16780 Training Accuracy: 0.96875 Loss: 0.0008235297864302993\n",
      "Iteration: 16790 Training Accuracy: 0.984375 Loss: 0.0008410022128373384\n",
      "Iteration: 16800 Training Accuracy: 1.0 Loss: 0.0005649455706588924\n",
      "Iteration: 16810 Training Accuracy: 1.0 Loss: 0.0002719630138017237\n",
      "Iteration: 16820 Training Accuracy: 0.96875 Loss: 0.00069381610956043\n",
      "Iteration: 16830 Training Accuracy: 1.0 Loss: 0.0004455825546756387\n",
      "Iteration: 16840 Training Accuracy: 0.984375 Loss: 0.0010311583755537868\n",
      "Iteration: 16850 Training Accuracy: 1.0 Loss: 0.00016209542809519917\n",
      "Iteration: 16860 Training Accuracy: 1.0 Loss: 0.0006457106792367995\n",
      "Iteration: 16870 Training Accuracy: 1.0 Loss: 0.0007838152232579887\n",
      "Iteration: 16880 Training Accuracy: 1.0 Loss: 0.00021966436179354787\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9286666666666666\n",
      "epoch: 18\n",
      "Iteration: 16890 Training Accuracy: 1.0 Loss: 0.0002259051543660462\n",
      "Iteration: 16900 Training Accuracy: 1.0 Loss: 0.0002112117363139987\n",
      "Iteration: 16910 Training Accuracy: 1.0 Loss: 0.00011335060116834939\n",
      "Iteration: 16920 Training Accuracy: 1.0 Loss: 0.0003268879372626543\n",
      "Iteration: 16930 Training Accuracy: 1.0 Loss: 0.00019667809829115868\n",
      "Iteration: 16940 Training Accuracy: 1.0 Loss: 0.0002491710474714637\n",
      "Iteration: 16950 Training Accuracy: 0.984375 Loss: 0.0010788397630676627\n",
      "Iteration: 16960 Training Accuracy: 0.984375 Loss: 0.0008106241002678871\n",
      "Iteration: 16970 Training Accuracy: 0.984375 Loss: 0.0013032357674092054\n",
      "Iteration: 16980 Training Accuracy: 0.984375 Loss: 0.0007029228145256639\n",
      "Iteration: 16990 Training Accuracy: 0.984375 Loss: 0.0011334804585203528\n",
      "Iteration: 17000 Training Accuracy: 1.0 Loss: 0.00019210770551580936\n",
      "Iteration: 17010 Training Accuracy: 0.96875 Loss: 0.0009944597259163857\n",
      "Iteration: 17020 Training Accuracy: 1.0 Loss: 0.00016297036199830472\n",
      "Iteration: 17030 Training Accuracy: 0.984375 Loss: 0.000533861166331917\n",
      "Iteration: 17040 Training Accuracy: 0.984375 Loss: 0.0009576081647537649\n",
      "Iteration: 17050 Training Accuracy: 1.0 Loss: 0.0004045330570079386\n",
      "Iteration: 17060 Training Accuracy: 0.984375 Loss: 0.0005420720553956926\n",
      "Iteration: 17070 Training Accuracy: 1.0 Loss: 0.00014801829820498824\n",
      "Iteration: 17080 Training Accuracy: 1.0 Loss: 0.00035537470830604434\n",
      "Iteration: 17090 Training Accuracy: 0.984375 Loss: 0.0008314721053466201\n",
      "Iteration: 17100 Training Accuracy: 0.984375 Loss: 0.0008445248240604997\n",
      "Iteration: 17110 Training Accuracy: 0.984375 Loss: 0.0009045206243172288\n",
      "Iteration: 17120 Training Accuracy: 1.0 Loss: 0.00020304940699134022\n",
      "Iteration: 17130 Training Accuracy: 0.984375 Loss: 0.0006689715082757175\n",
      "Iteration: 17140 Training Accuracy: 1.0 Loss: 0.00014360278146341443\n",
      "Iteration: 17150 Training Accuracy: 0.984375 Loss: 0.0004886372480541468\n",
      "Iteration: 17160 Training Accuracy: 0.984375 Loss: 0.0009102091426029801\n",
      "Iteration: 17170 Training Accuracy: 1.0 Loss: 9.575358126312494e-05\n",
      "Iteration: 17180 Training Accuracy: 0.96875 Loss: 0.000857479521073401\n",
      "Iteration: 17190 Training Accuracy: 1.0 Loss: 0.00017903803382068872\n",
      "Iteration: 17200 Training Accuracy: 0.984375 Loss: 0.0005396141204982996\n",
      "Iteration: 17210 Training Accuracy: 1.0 Loss: 0.00018865472520701587\n",
      "Iteration: 17220 Training Accuracy: 0.984375 Loss: 0.0011054854840040207\n",
      "Iteration: 17230 Training Accuracy: 0.984375 Loss: 0.00045285592204891145\n",
      "Iteration: 17240 Training Accuracy: 0.984375 Loss: 0.0002592912351246923\n",
      "Iteration: 17250 Training Accuracy: 1.0 Loss: 0.00012531715037766844\n",
      "Iteration: 17260 Training Accuracy: 0.984375 Loss: 0.0007442235364578664\n",
      "Iteration: 17270 Training Accuracy: 0.984375 Loss: 0.0009521417086943984\n",
      "Iteration: 17280 Training Accuracy: 1.0 Loss: 0.0002539407287258655\n",
      "Iteration: 17290 Training Accuracy: 1.0 Loss: 0.0002445161808282137\n",
      "Iteration: 17300 Training Accuracy: 1.0 Loss: 0.0008511239429935813\n",
      "Iteration: 17310 Training Accuracy: 1.0 Loss: 0.0001369270758004859\n",
      "Iteration: 17320 Training Accuracy: 1.0 Loss: 0.00019313261145725846\n",
      "Iteration: 17330 Training Accuracy: 0.984375 Loss: 0.000532170117367059\n",
      "Iteration: 17340 Training Accuracy: 1.0 Loss: 0.0001474493183195591\n",
      "Iteration: 17350 Training Accuracy: 0.984375 Loss: 0.0006641422514803708\n",
      "Iteration: 17360 Training Accuracy: 0.984375 Loss: 0.0005693935672752559\n",
      "Iteration: 17370 Training Accuracy: 1.0 Loss: 0.00018531308160163462\n",
      "Iteration: 17380 Training Accuracy: 0.984375 Loss: 0.0010572477476671338\n",
      "Iteration: 17390 Training Accuracy: 0.984375 Loss: 0.000627877947408706\n",
      "Iteration: 17400 Training Accuracy: 1.0 Loss: 0.0004474982852116227\n",
      "Iteration: 17410 Training Accuracy: 1.0 Loss: 0.0004710627254098654\n",
      "Iteration: 17420 Training Accuracy: 1.0 Loss: 0.00038105412386357784\n",
      "Iteration: 17430 Training Accuracy: 0.984375 Loss: 0.0011334649752825499\n",
      "Iteration: 17440 Training Accuracy: 1.0 Loss: 0.0001125380804296583\n",
      "Iteration: 17450 Training Accuracy: 1.0 Loss: 0.0001000876582111232\n",
      "Iteration: 17460 Training Accuracy: 1.0 Loss: 0.0004554464539978653\n",
      "Iteration: 17470 Training Accuracy: 0.984375 Loss: 0.000597658974584192\n",
      "Iteration: 17480 Training Accuracy: 1.0 Loss: 0.00045050238259136677\n",
      "Iteration: 17490 Training Accuracy: 1.0 Loss: 0.000195498185348697\n",
      "Iteration: 17500 Training Accuracy: 1.0 Loss: 0.00010925627429969609\n",
      "Iteration: 17510 Training Accuracy: 1.0 Loss: 8.36388862808235e-05\n",
      "Iteration: 17520 Training Accuracy: 1.0 Loss: 0.00019490980776026845\n",
      "Iteration: 17530 Training Accuracy: 0.953125 Loss: 0.0009109774837270379\n",
      "Iteration: 17540 Training Accuracy: 1.0 Loss: 0.0006817223038524389\n",
      "Iteration: 17550 Training Accuracy: 1.0 Loss: 0.0003497854631859809\n",
      "Iteration: 17560 Training Accuracy: 1.0 Loss: 0.0002581214066594839\n",
      "Iteration: 17570 Training Accuracy: 1.0 Loss: 0.0005482973647303879\n",
      "Iteration: 17580 Training Accuracy: 1.0 Loss: 0.000292499375063926\n",
      "Iteration: 17590 Training Accuracy: 1.0 Loss: 8.099024125840515e-05\n",
      "Iteration: 17600 Training Accuracy: 0.984375 Loss: 0.0006935656419955194\n",
      "Iteration: 17610 Training Accuracy: 0.984375 Loss: 0.0007169403834268451\n",
      "Iteration: 17620 Training Accuracy: 0.96875 Loss: 0.0009081509779207408\n",
      "Iteration: 17630 Training Accuracy: 0.984375 Loss: 0.0007577179349027574\n",
      "Iteration: 17640 Training Accuracy: 1.0 Loss: 0.00032306055072695017\n",
      "Iteration: 17650 Training Accuracy: 0.984375 Loss: 0.0009175605373457074\n",
      "Iteration: 17660 Training Accuracy: 1.0 Loss: 0.0001710400392767042\n",
      "Iteration: 17670 Training Accuracy: 1.0 Loss: 0.0003532890696078539\n",
      "Iteration: 17680 Training Accuracy: 1.0 Loss: 0.0002740671916399151\n",
      "Iteration: 17690 Training Accuracy: 0.984375 Loss: 0.00030933713424019516\n",
      "Iteration: 17700 Training Accuracy: 1.0 Loss: 0.00017073904746212065\n",
      "Iteration: 17710 Training Accuracy: 1.0 Loss: 0.0007136782514862716\n",
      "Iteration: 17720 Training Accuracy: 1.0 Loss: 0.0005019114469178021\n",
      "Iteration: 17730 Training Accuracy: 0.984375 Loss: 0.0007485970854759216\n",
      "Iteration: 17740 Training Accuracy: 0.984375 Loss: 0.001080801710486412\n",
      "Iteration: 17750 Training Accuracy: 0.984375 Loss: 0.0005127949989400804\n",
      "Iteration: 17760 Training Accuracy: 1.0 Loss: 0.00022135268955025822\n",
      "Iteration: 17770 Training Accuracy: 0.96875 Loss: 0.0012411132920533419\n",
      "Iteration: 17780 Training Accuracy: 1.0 Loss: 0.00023260692250914872\n",
      "Iteration: 17790 Training Accuracy: 1.0 Loss: 0.00038328178925439715\n",
      "Iteration: 17800 Training Accuracy: 1.0 Loss: 0.00046035926789045334\n",
      "Iteration: 17810 Training Accuracy: 0.984375 Loss: 0.0005901769618503749\n",
      "Iteration: 17820 Training Accuracy: 0.96875 Loss: 0.0006513230036944151\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.929\n",
      "epoch: 19\n",
      "Iteration: 17830 Training Accuracy: 1.0 Loss: 0.0001674620434641838\n",
      "Iteration: 17840 Training Accuracy: 0.984375 Loss: 0.0004486573161557317\n",
      "Iteration: 17850 Training Accuracy: 0.984375 Loss: 0.0006689606234431267\n",
      "Iteration: 17860 Training Accuracy: 1.0 Loss: 0.00028048467356711626\n",
      "Iteration: 17870 Training Accuracy: 1.0 Loss: 0.00037275231443345547\n",
      "Iteration: 17880 Training Accuracy: 1.0 Loss: 0.0002498662506695837\n",
      "Iteration: 17890 Training Accuracy: 0.953125 Loss: 0.0016808189684525132\n",
      "Iteration: 17900 Training Accuracy: 1.0 Loss: 0.00044992443872615695\n",
      "Iteration: 17910 Training Accuracy: 0.984375 Loss: 0.001007363200187683\n",
      "Iteration: 17920 Training Accuracy: 1.0 Loss: 0.00024239643244072795\n",
      "Iteration: 17930 Training Accuracy: 1.0 Loss: 0.00021200953051447868\n",
      "Iteration: 17940 Training Accuracy: 0.984375 Loss: 0.0008848640718497336\n",
      "Iteration: 17950 Training Accuracy: 1.0 Loss: 0.0003300647367723286\n",
      "Iteration: 17960 Training Accuracy: 1.0 Loss: 0.0006793772336095572\n",
      "Iteration: 17970 Training Accuracy: 0.984375 Loss: 0.0009860913269221783\n",
      "Iteration: 17980 Training Accuracy: 1.0 Loss: 0.00016266926832031459\n",
      "Iteration: 17990 Training Accuracy: 1.0 Loss: 0.0004883415531367064\n",
      "Iteration: 18000 Training Accuracy: 1.0 Loss: 0.0003099267778452486\n",
      "Iteration: 18010 Training Accuracy: 1.0 Loss: 0.00010539854702074081\n",
      "Iteration: 18020 Training Accuracy: 1.0 Loss: 0.00018515453848522156\n",
      "Iteration: 18030 Training Accuracy: 1.0 Loss: 2.256871448480524e-05\n",
      "Iteration: 18040 Training Accuracy: 1.0 Loss: 0.0003868153435178101\n",
      "Iteration: 18050 Training Accuracy: 0.984375 Loss: 0.0007679211557842791\n",
      "Iteration: 18060 Training Accuracy: 1.0 Loss: 0.0003396444662939757\n",
      "Iteration: 18070 Training Accuracy: 1.0 Loss: 0.00033787506981752813\n",
      "Iteration: 18080 Training Accuracy: 1.0 Loss: 0.00046486358041875064\n",
      "Iteration: 18090 Training Accuracy: 0.96875 Loss: 0.0014745991211384535\n",
      "Iteration: 18100 Training Accuracy: 0.984375 Loss: 0.0006944502238184214\n",
      "Iteration: 18110 Training Accuracy: 1.0 Loss: 0.00032823460060171783\n",
      "Iteration: 18120 Training Accuracy: 1.0 Loss: 0.0006735922652296722\n",
      "Iteration: 18130 Training Accuracy: 1.0 Loss: 0.00032318843295797706\n",
      "Iteration: 18140 Training Accuracy: 0.96875 Loss: 0.0011723076459020376\n",
      "Iteration: 18150 Training Accuracy: 1.0 Loss: 0.00010994472540915012\n",
      "Iteration: 18160 Training Accuracy: 1.0 Loss: 7.000351615715772e-05\n",
      "Iteration: 18170 Training Accuracy: 1.0 Loss: 8.353139128303155e-05\n",
      "Iteration: 18180 Training Accuracy: 1.0 Loss: 7.306711631827056e-05\n",
      "Iteration: 18190 Training Accuracy: 0.96875 Loss: 0.0011923947604373097\n",
      "Iteration: 18200 Training Accuracy: 0.984375 Loss: 0.0017101054545491934\n",
      "Iteration: 18210 Training Accuracy: 0.984375 Loss: 0.0010296248365193605\n",
      "Iteration: 18220 Training Accuracy: 0.984375 Loss: 0.000545077899005264\n",
      "Iteration: 18230 Training Accuracy: 0.984375 Loss: 0.0005069917533546686\n",
      "Iteration: 18240 Training Accuracy: 1.0 Loss: 6.404757732525468e-05\n",
      "Iteration: 18250 Training Accuracy: 0.984375 Loss: 0.0007021366036497056\n",
      "Iteration: 18260 Training Accuracy: 1.0 Loss: 0.0001538949436508119\n",
      "Iteration: 18270 Training Accuracy: 1.0 Loss: 0.0005005723796784878\n",
      "Iteration: 18280 Training Accuracy: 1.0 Loss: 7.29389867046848e-05\n",
      "Iteration: 18290 Training Accuracy: 0.984375 Loss: 0.00043131376150995493\n",
      "Iteration: 18300 Training Accuracy: 1.0 Loss: 0.00048798153875395656\n",
      "Iteration: 18310 Training Accuracy: 1.0 Loss: 0.00041009834967553616\n",
      "Iteration: 18320 Training Accuracy: 1.0 Loss: 0.00014424906112253666\n",
      "Iteration: 18330 Training Accuracy: 0.984375 Loss: 0.0003832282091025263\n",
      "Iteration: 18340 Training Accuracy: 1.0 Loss: 0.0005012201145291328\n",
      "Iteration: 18350 Training Accuracy: 1.0 Loss: 0.0004963119863532484\n",
      "Iteration: 18360 Training Accuracy: 0.984375 Loss: 0.0010810456005856395\n",
      "Iteration: 18370 Training Accuracy: 0.96875 Loss: 0.0008553197840228677\n",
      "Iteration: 18380 Training Accuracy: 0.984375 Loss: 0.000943309860303998\n",
      "Iteration: 18390 Training Accuracy: 1.0 Loss: 0.0003342697164043784\n",
      "Iteration: 18400 Training Accuracy: 0.984375 Loss: 0.00065119459759444\n",
      "Iteration: 18410 Training Accuracy: 1.0 Loss: 0.0004403973580338061\n",
      "Iteration: 18420 Training Accuracy: 1.0 Loss: 0.00018851693312171847\n",
      "Iteration: 18430 Training Accuracy: 1.0 Loss: 0.0001327437930740416\n",
      "Iteration: 18440 Training Accuracy: 1.0 Loss: 0.00013906543608754873\n",
      "Iteration: 18450 Training Accuracy: 1.0 Loss: 0.0004095564363524318\n",
      "Iteration: 18460 Training Accuracy: 1.0 Loss: 0.00032838270999491215\n",
      "Iteration: 18470 Training Accuracy: 1.0 Loss: 0.00017184671014547348\n",
      "Iteration: 18480 Training Accuracy: 0.984375 Loss: 0.0009140191250480711\n",
      "Iteration: 18490 Training Accuracy: 0.9375 Loss: 0.0020019139628857374\n",
      "Iteration: 18500 Training Accuracy: 1.0 Loss: 0.00013304667663760483\n",
      "Iteration: 18510 Training Accuracy: 0.984375 Loss: 0.0006594876758754253\n",
      "Iteration: 18520 Training Accuracy: 1.0 Loss: 0.00010916705650743097\n",
      "Iteration: 18530 Training Accuracy: 0.984375 Loss: 0.0008924491703510284\n",
      "Iteration: 18540 Training Accuracy: 0.96875 Loss: 0.0008455768693238497\n",
      "Iteration: 18550 Training Accuracy: 1.0 Loss: 0.00012659071944653988\n",
      "Iteration: 18560 Training Accuracy: 1.0 Loss: 0.000361971789970994\n",
      "Iteration: 18570 Training Accuracy: 0.984375 Loss: 0.0006917791906744242\n",
      "Iteration: 18580 Training Accuracy: 1.0 Loss: 0.0004649555776268244\n",
      "Iteration: 18590 Training Accuracy: 0.984375 Loss: 0.0004577474610414356\n",
      "Iteration: 18600 Training Accuracy: 0.96875 Loss: 0.0008145011379383504\n",
      "Iteration: 18610 Training Accuracy: 1.0 Loss: 0.00015750486636534333\n",
      "Iteration: 18620 Training Accuracy: 1.0 Loss: 0.0005519588012248278\n",
      "Iteration: 18630 Training Accuracy: 0.984375 Loss: 0.00047512142919003963\n",
      "Iteration: 18640 Training Accuracy: 0.984375 Loss: 0.0006972443661652505\n",
      "Iteration: 18650 Training Accuracy: 1.0 Loss: 0.00013372726971283555\n",
      "Iteration: 18660 Training Accuracy: 1.0 Loss: 0.0001584506535436958\n",
      "Iteration: 18670 Training Accuracy: 1.0 Loss: 0.00023819925263524055\n",
      "Iteration: 18680 Training Accuracy: 1.0 Loss: 0.00033756729681044817\n",
      "Iteration: 18690 Training Accuracy: 0.984375 Loss: 0.0005417913780547678\n",
      "Iteration: 18700 Training Accuracy: 0.984375 Loss: 0.0004708453780040145\n",
      "Iteration: 18710 Training Accuracy: 1.0 Loss: 0.0004850648401770741\n",
      "Iteration: 18720 Training Accuracy: 1.0 Loss: 0.00018489727517589927\n",
      "Iteration: 18730 Training Accuracy: 0.984375 Loss: 0.0004651290364563465\n",
      "Iteration: 18740 Training Accuracy: 1.0 Loss: 0.0003479445294942707\n",
      "Iteration: 18750 Training Accuracy: 1.0 Loss: 0.000610367045737803\n",
      "Iteration: 18760 Training Accuracy: 1.0 Loss: 0.0001826236111810431\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9288333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.99185\n",
      "Validation Accuracy = 0.9288333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RESNET18(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use res net \n",
    "model = torch.load(\"full_resnet18_5.pth\")\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.859375 Loss: 0.004849899560213089\n",
      "Iteration: 20 Training Accuracy: 0.890625 Loss: 0.0038420697674155235\n",
      "Iteration: 30 Training Accuracy: 0.953125 Loss: 0.0021469087805598974\n",
      "Iteration: 40 Training Accuracy: 0.921875 Loss: 0.005274956114590168\n",
      "Iteration: 50 Training Accuracy: 0.84375 Loss: 0.007362027652561665\n",
      "Iteration: 60 Training Accuracy: 0.859375 Loss: 0.004609496798366308\n",
      "Iteration: 70 Training Accuracy: 0.984375 Loss: 0.00166227575391531\n",
      "Iteration: 80 Training Accuracy: 0.890625 Loss: 0.005489868111908436\n",
      "Iteration: 90 Training Accuracy: 0.953125 Loss: 0.002098931698128581\n",
      "Iteration: 100 Training Accuracy: 0.96875 Loss: 0.002774599939584732\n",
      "Iteration: 110 Training Accuracy: 0.921875 Loss: 0.0035131778568029404\n",
      "Iteration: 120 Training Accuracy: 0.953125 Loss: 0.001518884440883994\n",
      "Iteration: 130 Training Accuracy: 0.90625 Loss: 0.0035913933534175158\n",
      "Iteration: 140 Training Accuracy: 0.859375 Loss: 0.005524161271750927\n",
      "Iteration: 150 Training Accuracy: 0.890625 Loss: 0.005662910174578428\n",
      "Iteration: 160 Training Accuracy: 0.9375 Loss: 0.00426420709118247\n",
      "Iteration: 170 Training Accuracy: 0.96875 Loss: 0.0021845363080501556\n",
      "Iteration: 180 Training Accuracy: 0.875 Loss: 0.005059042014181614\n",
      "Iteration: 190 Training Accuracy: 0.875 Loss: 0.005172012839466333\n",
      "Iteration: 200 Training Accuracy: 0.9375 Loss: 0.0032394048757851124\n",
      "Iteration: 210 Training Accuracy: 0.9375 Loss: 0.003332138527184725\n",
      "Iteration: 220 Training Accuracy: 0.96875 Loss: 0.0021700426004827023\n",
      "Iteration: 230 Training Accuracy: 0.96875 Loss: 0.0020378022454679012\n",
      "Iteration: 240 Training Accuracy: 0.859375 Loss: 0.00605644378811121\n",
      "Iteration: 250 Training Accuracy: 0.9375 Loss: 0.004535222426056862\n",
      "Iteration: 260 Training Accuracy: 0.921875 Loss: 0.0026361849159002304\n",
      "Iteration: 270 Training Accuracy: 1.0 Loss: 0.0010603254195302725\n",
      "Iteration: 280 Training Accuracy: 0.890625 Loss: 0.005706980358809233\n",
      "Iteration: 290 Training Accuracy: 0.9375 Loss: 0.0021337552461773157\n",
      "Iteration: 300 Training Accuracy: 0.921875 Loss: 0.006107073277235031\n",
      "Iteration: 310 Training Accuracy: 0.96875 Loss: 0.0019305963069200516\n",
      "Iteration: 320 Training Accuracy: 0.921875 Loss: 0.00367748667486012\n",
      "Iteration: 330 Training Accuracy: 0.90625 Loss: 0.004284772556275129\n",
      "Iteration: 340 Training Accuracy: 0.953125 Loss: 0.0013596181524917483\n",
      "Iteration: 350 Training Accuracy: 0.984375 Loss: 0.0012309432495385408\n",
      "Iteration: 360 Training Accuracy: 0.90625 Loss: 0.0028113271109759808\n",
      "Iteration: 370 Training Accuracy: 0.9375 Loss: 0.0031725638546049595\n",
      "Iteration: 380 Training Accuracy: 0.921875 Loss: 0.00374927232041955\n",
      "Iteration: 390 Training Accuracy: 0.90625 Loss: 0.004361224360764027\n",
      "Iteration: 400 Training Accuracy: 0.953125 Loss: 0.0028591491281986237\n",
      "Iteration: 410 Training Accuracy: 0.953125 Loss: 0.002074254211038351\n",
      "Iteration: 420 Training Accuracy: 0.90625 Loss: 0.0025988949928432703\n",
      "Iteration: 430 Training Accuracy: 0.90625 Loss: 0.0036589426454156637\n",
      "Iteration: 440 Training Accuracy: 0.921875 Loss: 0.002960712183266878\n",
      "Iteration: 450 Training Accuracy: 0.953125 Loss: 0.0027768989093601704\n",
      "Iteration: 460 Training Accuracy: 0.921875 Loss: 0.0032664097379893064\n",
      "Iteration: 470 Training Accuracy: 0.9375 Loss: 0.0034347062464803457\n",
      "Iteration: 480 Training Accuracy: 0.984375 Loss: 0.0009732979815453291\n",
      "Iteration: 490 Training Accuracy: 0.984375 Loss: 0.0008180848672054708\n",
      "Iteration: 500 Training Accuracy: 0.9375 Loss: 0.0035668048076331615\n",
      "Iteration: 510 Training Accuracy: 0.9375 Loss: 0.001876056776382029\n",
      "Iteration: 520 Training Accuracy: 0.984375 Loss: 0.001456505386158824\n",
      "Iteration: 530 Training Accuracy: 0.96875 Loss: 0.002392885275185108\n",
      "Iteration: 540 Training Accuracy: 0.96875 Loss: 0.0012930785305798054\n",
      "Iteration: 550 Training Accuracy: 0.953125 Loss: 0.0022073788568377495\n",
      "Iteration: 560 Training Accuracy: 0.921875 Loss: 0.0033286409452557564\n",
      "Iteration: 570 Training Accuracy: 0.953125 Loss: 0.003992920275777578\n",
      "Iteration: 580 Training Accuracy: 0.90625 Loss: 0.0053273835219442844\n",
      "Iteration: 590 Training Accuracy: 0.9375 Loss: 0.0023648261558264494\n",
      "Iteration: 600 Training Accuracy: 0.953125 Loss: 0.0024591609835624695\n",
      "Iteration: 610 Training Accuracy: 0.96875 Loss: 0.0017090665642172098\n",
      "Iteration: 620 Training Accuracy: 0.921875 Loss: 0.005045892670750618\n",
      "Iteration: 630 Training Accuracy: 0.96875 Loss: 0.001472478499636054\n",
      "Iteration: 640 Training Accuracy: 0.953125 Loss: 0.0034012342803180218\n",
      "Iteration: 650 Training Accuracy: 0.96875 Loss: 0.002349653746932745\n",
      "Iteration: 660 Training Accuracy: 0.921875 Loss: 0.0035842638462781906\n",
      "Iteration: 670 Training Accuracy: 1.0 Loss: 0.0009125582873821259\n",
      "Iteration: 680 Training Accuracy: 0.921875 Loss: 0.00477168895304203\n",
      "Iteration: 690 Training Accuracy: 0.9375 Loss: 0.002409010659903288\n",
      "Iteration: 700 Training Accuracy: 0.9375 Loss: 0.0019636983051896095\n",
      "Iteration: 710 Training Accuracy: 0.984375 Loss: 0.0004536043852567673\n",
      "Iteration: 720 Training Accuracy: 0.90625 Loss: 0.0029206681065261364\n",
      "Iteration: 730 Training Accuracy: 0.921875 Loss: 0.0042997864075005054\n",
      "Iteration: 740 Training Accuracy: 0.953125 Loss: 0.0020371642895042896\n",
      "Iteration: 750 Training Accuracy: 0.921875 Loss: 0.002878375817090273\n",
      "Iteration: 760 Training Accuracy: 0.9375 Loss: 0.002712830202654004\n",
      "Iteration: 770 Training Accuracy: 0.921875 Loss: 0.0027295644395053387\n",
      "Iteration: 780 Training Accuracy: 0.96875 Loss: 0.0019235885702073574\n",
      "Iteration: 790 Training Accuracy: 0.9375 Loss: 0.002723257290199399\n",
      "Iteration: 800 Training Accuracy: 0.875 Loss: 0.004780590534210205\n",
      "Iteration: 810 Training Accuracy: 0.953125 Loss: 0.004373024683445692\n",
      "Iteration: 820 Training Accuracy: 0.953125 Loss: 0.0027164609637111425\n",
      "Iteration: 830 Training Accuracy: 0.875 Loss: 0.004963560029864311\n",
      "Iteration: 840 Training Accuracy: 0.96875 Loss: 0.0016548821004107594\n",
      "Iteration: 850 Training Accuracy: 0.9375 Loss: 0.0030269152484834194\n",
      "Iteration: 860 Training Accuracy: 0.984375 Loss: 0.0008075764635577798\n",
      "Iteration: 870 Training Accuracy: 1.0 Loss: 0.00040200079092755914\n",
      "Iteration: 880 Training Accuracy: 1.0 Loss: 0.000536247156560421\n",
      "Iteration: 890 Training Accuracy: 0.9375 Loss: 0.0019841042812913656\n",
      "Iteration: 900 Training Accuracy: 0.96875 Loss: 0.0011604686733335257\n",
      "Iteration: 910 Training Accuracy: 0.921875 Loss: 0.004196316935122013\n",
      "Iteration: 920 Training Accuracy: 0.984375 Loss: 0.0012031643418595195\n",
      "Iteration: 930 Training Accuracy: 0.9375 Loss: 0.0022483263164758682\n",
      "Training Accuracy = 0.90625\n",
      "Validation Accuracy = 0.9543333333333334\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 0.953125 Loss: 0.0015895895194262266\n",
      "Iteration: 950 Training Accuracy: 0.984375 Loss: 0.0009112227708101273\n",
      "Iteration: 960 Training Accuracy: 0.96875 Loss: 0.0011396785266697407\n",
      "Iteration: 970 Training Accuracy: 0.984375 Loss: 0.0009905799524858594\n",
      "Iteration: 980 Training Accuracy: 0.984375 Loss: 0.0008460280951112509\n",
      "Iteration: 990 Training Accuracy: 0.953125 Loss: 0.0018409723415970802\n",
      "Iteration: 1000 Training Accuracy: 1.0 Loss: 0.00021965234191156924\n",
      "Iteration: 1010 Training Accuracy: 0.984375 Loss: 0.0007870125118643045\n",
      "Iteration: 1020 Training Accuracy: 1.0 Loss: 0.0001474406017223373\n",
      "Iteration: 1030 Training Accuracy: 0.953125 Loss: 0.0020066292490810156\n",
      "Iteration: 1040 Training Accuracy: 0.984375 Loss: 0.0006380141712725163\n",
      "Iteration: 1050 Training Accuracy: 0.984375 Loss: 0.0006556794396601617\n",
      "Iteration: 1060 Training Accuracy: 1.0 Loss: 0.00028148028650321066\n",
      "Iteration: 1070 Training Accuracy: 0.96875 Loss: 0.0016033216379582882\n",
      "Iteration: 1080 Training Accuracy: 0.96875 Loss: 0.00078622653381899\n",
      "Iteration: 1090 Training Accuracy: 0.984375 Loss: 0.0005406508571468294\n",
      "Iteration: 1100 Training Accuracy: 0.984375 Loss: 0.0004966214182786644\n",
      "Iteration: 1110 Training Accuracy: 0.984375 Loss: 0.00044511063606478274\n",
      "Iteration: 1120 Training Accuracy: 1.0 Loss: 0.00018564757192507386\n",
      "Iteration: 1130 Training Accuracy: 0.96875 Loss: 0.0010894678998738527\n",
      "Iteration: 1140 Training Accuracy: 0.984375 Loss: 0.0005391163867898285\n",
      "Iteration: 1150 Training Accuracy: 1.0 Loss: 0.0006631319411098957\n",
      "Iteration: 1160 Training Accuracy: 1.0 Loss: 0.0005590271321125329\n",
      "Iteration: 1170 Training Accuracy: 0.984375 Loss: 0.0007044865633361042\n",
      "Iteration: 1180 Training Accuracy: 0.984375 Loss: 0.00042518528061918914\n",
      "Iteration: 1190 Training Accuracy: 1.0 Loss: 0.000567270559258759\n",
      "Iteration: 1200 Training Accuracy: 0.984375 Loss: 0.00034981538192369044\n",
      "Iteration: 1210 Training Accuracy: 0.953125 Loss: 0.0016882047057151794\n",
      "Iteration: 1220 Training Accuracy: 1.0 Loss: 0.0010222619166597724\n",
      "Iteration: 1230 Training Accuracy: 0.984375 Loss: 0.0005225773202255368\n",
      "Iteration: 1240 Training Accuracy: 0.953125 Loss: 0.0008393817697651684\n",
      "Iteration: 1250 Training Accuracy: 0.921875 Loss: 0.001947174547240138\n",
      "Iteration: 1260 Training Accuracy: 0.984375 Loss: 0.0008170443470589817\n",
      "Iteration: 1270 Training Accuracy: 0.984375 Loss: 0.0009639839408919215\n",
      "Iteration: 1280 Training Accuracy: 1.0 Loss: 0.0003020750591531396\n",
      "Iteration: 1290 Training Accuracy: 0.984375 Loss: 0.0006449472857639194\n",
      "Iteration: 1300 Training Accuracy: 0.984375 Loss: 0.00036982190795242786\n",
      "Iteration: 1310 Training Accuracy: 1.0 Loss: 0.0004769915249198675\n",
      "Iteration: 1320 Training Accuracy: 0.984375 Loss: 0.0010791956447064877\n",
      "Iteration: 1330 Training Accuracy: 0.984375 Loss: 0.0006541423499584198\n",
      "Iteration: 1340 Training Accuracy: 1.0 Loss: 0.00022864562924951315\n",
      "Iteration: 1350 Training Accuracy: 0.96875 Loss: 0.0011816825717687607\n",
      "Iteration: 1360 Training Accuracy: 0.984375 Loss: 0.0008984564919956028\n",
      "Iteration: 1370 Training Accuracy: 1.0 Loss: 0.0002777881163638085\n",
      "Iteration: 1380 Training Accuracy: 1.0 Loss: 0.0001535166084067896\n",
      "Iteration: 1390 Training Accuracy: 0.96875 Loss: 0.0014875975903123617\n",
      "Iteration: 1400 Training Accuracy: 1.0 Loss: 0.00023685995256528258\n",
      "Iteration: 1410 Training Accuracy: 1.0 Loss: 9.095469431485981e-05\n",
      "Iteration: 1420 Training Accuracy: 1.0 Loss: 0.00023420172510668635\n",
      "Iteration: 1430 Training Accuracy: 0.984375 Loss: 0.000715952250175178\n",
      "Iteration: 1440 Training Accuracy: 0.984375 Loss: 0.000936996890231967\n",
      "Iteration: 1450 Training Accuracy: 0.953125 Loss: 0.0020736465230584145\n",
      "Iteration: 1460 Training Accuracy: 0.96875 Loss: 0.0009484186884947121\n",
      "Iteration: 1470 Training Accuracy: 1.0 Loss: 0.00041964638512581587\n",
      "Iteration: 1480 Training Accuracy: 0.984375 Loss: 0.000515056075528264\n",
      "Iteration: 1490 Training Accuracy: 1.0 Loss: 0.0004805333155672997\n",
      "Iteration: 1500 Training Accuracy: 1.0 Loss: 6.904755719006062e-05\n",
      "Iteration: 1510 Training Accuracy: 1.0 Loss: 0.0005818352801725268\n",
      "Iteration: 1520 Training Accuracy: 0.984375 Loss: 0.0007804598426446319\n",
      "Iteration: 1530 Training Accuracy: 0.96875 Loss: 0.000800040434114635\n",
      "Iteration: 1540 Training Accuracy: 1.0 Loss: 0.000299585924949497\n",
      "Iteration: 1550 Training Accuracy: 0.984375 Loss: 0.0004810819518752396\n",
      "Iteration: 1560 Training Accuracy: 0.984375 Loss: 0.0010684659937396646\n",
      "Iteration: 1570 Training Accuracy: 0.96875 Loss: 0.0008321129716932774\n",
      "Iteration: 1580 Training Accuracy: 0.984375 Loss: 0.000300686078844592\n",
      "Iteration: 1590 Training Accuracy: 1.0 Loss: 0.0001624708529561758\n",
      "Iteration: 1600 Training Accuracy: 0.984375 Loss: 0.0008746760431677103\n",
      "Iteration: 1610 Training Accuracy: 0.96875 Loss: 0.0027630149852484465\n",
      "Iteration: 1620 Training Accuracy: 0.984375 Loss: 0.0006658030906692147\n",
      "Iteration: 1630 Training Accuracy: 0.984375 Loss: 0.0004940925864502788\n",
      "Iteration: 1640 Training Accuracy: 0.96875 Loss: 0.0007175348000600934\n",
      "Iteration: 1650 Training Accuracy: 0.984375 Loss: 0.0003875516995321959\n",
      "Iteration: 1660 Training Accuracy: 1.0 Loss: 0.000362339720595628\n",
      "Iteration: 1670 Training Accuracy: 1.0 Loss: 0.0004004377406090498\n",
      "Iteration: 1680 Training Accuracy: 1.0 Loss: 7.921146607259288e-05\n",
      "Iteration: 1690 Training Accuracy: 1.0 Loss: 0.00017502575065009296\n",
      "Iteration: 1700 Training Accuracy: 0.984375 Loss: 0.0005021810065954924\n",
      "Iteration: 1710 Training Accuracy: 1.0 Loss: 0.0005224751075729728\n",
      "Iteration: 1720 Training Accuracy: 0.984375 Loss: 0.0004281688015908003\n",
      "Iteration: 1730 Training Accuracy: 0.984375 Loss: 0.001426052302122116\n",
      "Iteration: 1740 Training Accuracy: 0.984375 Loss: 0.00046974726137705147\n",
      "Iteration: 1750 Training Accuracy: 0.984375 Loss: 0.00028877618024125695\n",
      "Iteration: 1760 Training Accuracy: 0.96875 Loss: 0.0010527732083573937\n",
      "Iteration: 1770 Training Accuracy: 1.0 Loss: 0.0002355483447900042\n",
      "Iteration: 1780 Training Accuracy: 0.984375 Loss: 0.0006919211009517312\n",
      "Iteration: 1790 Training Accuracy: 0.96875 Loss: 0.0012064919574186206\n",
      "Iteration: 1800 Training Accuracy: 0.984375 Loss: 0.00042135591502301395\n",
      "Iteration: 1810 Training Accuracy: 0.984375 Loss: 0.0004801391623914242\n",
      "Iteration: 1820 Training Accuracy: 0.96875 Loss: 0.0010083543602377176\n",
      "Iteration: 1830 Training Accuracy: 1.0 Loss: 0.00017564691370353103\n",
      "Iteration: 1840 Training Accuracy: 1.0 Loss: 0.0002734677691478282\n",
      "Iteration: 1850 Training Accuracy: 0.984375 Loss: 0.0010757530108094215\n",
      "Iteration: 1860 Training Accuracy: 1.0 Loss: 0.00021426219609566033\n",
      "Iteration: 1870 Training Accuracy: 1.0 Loss: 0.00028270864277146757\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9666666666666667\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 1.0 Loss: 0.0003369427868165076\n",
      "Iteration: 1890 Training Accuracy: 1.0 Loss: 0.00017738822498358786\n",
      "Iteration: 1900 Training Accuracy: 1.0 Loss: 4.0875318518374115e-05\n",
      "Iteration: 1910 Training Accuracy: 1.0 Loss: 0.00033820344833657146\n",
      "Iteration: 1920 Training Accuracy: 0.984375 Loss: 0.000555961043573916\n",
      "Iteration: 1930 Training Accuracy: 0.953125 Loss: 0.0014611962251365185\n",
      "Iteration: 1940 Training Accuracy: 0.984375 Loss: 0.00035951012978330255\n",
      "Iteration: 1950 Training Accuracy: 1.0 Loss: 0.00028565479442477226\n",
      "Iteration: 1960 Training Accuracy: 1.0 Loss: 4.700612771557644e-05\n",
      "Iteration: 1970 Training Accuracy: 0.96875 Loss: 0.0008354373858310282\n",
      "Iteration: 1980 Training Accuracy: 0.984375 Loss: 0.0003307634324301034\n",
      "Iteration: 1990 Training Accuracy: 0.984375 Loss: 0.00028988998383283615\n",
      "Iteration: 2000 Training Accuracy: 0.984375 Loss: 0.00037688305019401014\n",
      "Iteration: 2010 Training Accuracy: 0.953125 Loss: 0.0012883528834208846\n",
      "Iteration: 2020 Training Accuracy: 1.0 Loss: 0.0003087306395173073\n",
      "Iteration: 2030 Training Accuracy: 0.984375 Loss: 0.0004357214493211359\n",
      "Iteration: 2040 Training Accuracy: 1.0 Loss: 0.00022862644982524216\n",
      "Iteration: 2050 Training Accuracy: 1.0 Loss: 0.0003695968771353364\n",
      "Iteration: 2060 Training Accuracy: 0.984375 Loss: 0.0012739701196551323\n",
      "Iteration: 2070 Training Accuracy: 0.984375 Loss: 0.0009617803152650595\n",
      "Iteration: 2080 Training Accuracy: 1.0 Loss: 6.114818825153634e-05\n",
      "Iteration: 2090 Training Accuracy: 1.0 Loss: 0.0004977226490154862\n",
      "Iteration: 2100 Training Accuracy: 1.0 Loss: 0.00023586550378240645\n",
      "Iteration: 2110 Training Accuracy: 1.0 Loss: 0.000301039544865489\n",
      "Iteration: 2120 Training Accuracy: 0.984375 Loss: 0.0005022656405344605\n",
      "Iteration: 2130 Training Accuracy: 1.0 Loss: 0.00011104165605502203\n",
      "Iteration: 2140 Training Accuracy: 1.0 Loss: 0.00013512515579350293\n",
      "Iteration: 2150 Training Accuracy: 1.0 Loss: 3.618948903749697e-05\n",
      "Iteration: 2160 Training Accuracy: 1.0 Loss: 0.00046934233978390694\n",
      "Iteration: 2170 Training Accuracy: 0.984375 Loss: 0.001260665012523532\n",
      "Iteration: 2180 Training Accuracy: 0.96875 Loss: 0.0005843891412951052\n",
      "Iteration: 2190 Training Accuracy: 0.96875 Loss: 0.0013042250648140907\n",
      "Iteration: 2200 Training Accuracy: 1.0 Loss: 0.0002534892119001597\n",
      "Iteration: 2210 Training Accuracy: 1.0 Loss: 0.00038894324097782373\n",
      "Iteration: 2220 Training Accuracy: 1.0 Loss: 0.0005673380219377577\n",
      "Iteration: 2230 Training Accuracy: 1.0 Loss: 2.5314659069408663e-05\n",
      "Iteration: 2240 Training Accuracy: 0.984375 Loss: 0.0008005747804418206\n",
      "Iteration: 2250 Training Accuracy: 1.0 Loss: 0.0002545118622947484\n",
      "Iteration: 2260 Training Accuracy: 0.984375 Loss: 0.0005651210085488856\n",
      "Iteration: 2270 Training Accuracy: 1.0 Loss: 0.0005711372941732407\n",
      "Iteration: 2280 Training Accuracy: 1.0 Loss: 5.8539877500152215e-05\n",
      "Iteration: 2290 Training Accuracy: 1.0 Loss: 9.099677845370024e-05\n",
      "Iteration: 2300 Training Accuracy: 1.0 Loss: 4.123033431824297e-05\n",
      "Iteration: 2310 Training Accuracy: 0.984375 Loss: 0.0008628484210930765\n",
      "Iteration: 2320 Training Accuracy: 0.984375 Loss: 0.0008362847147509456\n",
      "Iteration: 2330 Training Accuracy: 0.984375 Loss: 0.0004522146482486278\n",
      "Iteration: 2340 Training Accuracy: 0.984375 Loss: 0.0007817242876626551\n",
      "Iteration: 2350 Training Accuracy: 0.984375 Loss: 0.000706050603184849\n",
      "Iteration: 2360 Training Accuracy: 0.984375 Loss: 0.0007909000269137323\n",
      "Iteration: 2370 Training Accuracy: 1.0 Loss: 0.00016669936303514987\n",
      "Iteration: 2380 Training Accuracy: 0.984375 Loss: 0.0011777655454352498\n",
      "Iteration: 2390 Training Accuracy: 0.984375 Loss: 0.0005539397825486958\n",
      "Iteration: 2400 Training Accuracy: 1.0 Loss: 0.00022192337200976908\n",
      "Iteration: 2410 Training Accuracy: 0.984375 Loss: 0.0004295826074667275\n",
      "Iteration: 2420 Training Accuracy: 1.0 Loss: 5.137533298693597e-05\n",
      "Iteration: 2430 Training Accuracy: 1.0 Loss: 0.0001293027598876506\n",
      "Iteration: 2440 Training Accuracy: 1.0 Loss: 0.00011736396118067205\n",
      "Iteration: 2450 Training Accuracy: 1.0 Loss: 0.0002538991975598037\n",
      "Iteration: 2460 Training Accuracy: 1.0 Loss: 2.8456177460611798e-05\n",
      "Iteration: 2470 Training Accuracy: 1.0 Loss: 0.00028613273752853274\n",
      "Iteration: 2480 Training Accuracy: 1.0 Loss: 0.0001540138036943972\n",
      "Iteration: 2490 Training Accuracy: 1.0 Loss: 0.00028428464429453015\n",
      "Iteration: 2500 Training Accuracy: 1.0 Loss: 0.00025484972866252065\n",
      "Iteration: 2510 Training Accuracy: 1.0 Loss: 0.00025024230126291513\n",
      "Iteration: 2520 Training Accuracy: 1.0 Loss: 2.3522305127698928e-05\n",
      "Iteration: 2530 Training Accuracy: 1.0 Loss: 0.00014071605983190238\n",
      "Iteration: 2540 Training Accuracy: 0.984375 Loss: 0.001007409067824483\n",
      "Iteration: 2550 Training Accuracy: 1.0 Loss: 9.200109343510121e-05\n",
      "Iteration: 2560 Training Accuracy: 0.984375 Loss: 0.00024910568026825786\n",
      "Iteration: 2570 Training Accuracy: 1.0 Loss: 0.0001299117284361273\n",
      "Iteration: 2580 Training Accuracy: 0.984375 Loss: 0.0005978469853289425\n",
      "Iteration: 2590 Training Accuracy: 0.984375 Loss: 0.0009204614907503128\n",
      "Iteration: 2600 Training Accuracy: 0.96875 Loss: 0.0008725599618628621\n",
      "Iteration: 2610 Training Accuracy: 1.0 Loss: 0.0004041353240609169\n",
      "Iteration: 2620 Training Accuracy: 0.96875 Loss: 0.001233444083482027\n",
      "Iteration: 2630 Training Accuracy: 1.0 Loss: 0.0005907301092520356\n",
      "Iteration: 2640 Training Accuracy: 0.984375 Loss: 0.00099687441252172\n",
      "Iteration: 2650 Training Accuracy: 0.96875 Loss: 0.0006769374012947083\n",
      "Iteration: 2660 Training Accuracy: 0.984375 Loss: 0.0008183679892681539\n",
      "Iteration: 2670 Training Accuracy: 0.984375 Loss: 0.0005015207570977509\n",
      "Iteration: 2680 Training Accuracy: 1.0 Loss: 3.7050725950393826e-05\n",
      "Iteration: 2690 Training Accuracy: 1.0 Loss: 0.0001463160733692348\n",
      "Iteration: 2700 Training Accuracy: 0.984375 Loss: 0.00032192308572120965\n",
      "Iteration: 2710 Training Accuracy: 0.984375 Loss: 0.0008374698227271438\n",
      "Iteration: 2720 Training Accuracy: 0.984375 Loss: 0.0011280812323093414\n",
      "Iteration: 2730 Training Accuracy: 1.0 Loss: 0.0001355531276203692\n",
      "Iteration: 2740 Training Accuracy: 1.0 Loss: 0.0006041149026714265\n",
      "Iteration: 2750 Training Accuracy: 1.0 Loss: 0.0003641134244389832\n",
      "Iteration: 2760 Training Accuracy: 0.984375 Loss: 0.00107527372892946\n",
      "Iteration: 2770 Training Accuracy: 1.0 Loss: 0.0007212907075881958\n",
      "Iteration: 2780 Training Accuracy: 1.0 Loss: 0.0001295196416322142\n",
      "Iteration: 2790 Training Accuracy: 1.0 Loss: 0.00016413898265454918\n",
      "Iteration: 2800 Training Accuracy: 1.0 Loss: 0.00043258460937067866\n",
      "Iteration: 2810 Training Accuracy: 0.96875 Loss: 0.0006274707848206162\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9673333333333334\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 1.0 Loss: 8.000509842531756e-05\n",
      "Iteration: 2830 Training Accuracy: 1.0 Loss: 0.0003136005543638021\n",
      "Iteration: 2840 Training Accuracy: 1.0 Loss: 0.00015441581490449607\n",
      "Iteration: 2850 Training Accuracy: 1.0 Loss: 0.00028978995396755636\n",
      "Iteration: 2860 Training Accuracy: 1.0 Loss: 7.388025551335886e-05\n",
      "Iteration: 2870 Training Accuracy: 0.984375 Loss: 0.00025813939282670617\n",
      "Iteration: 2880 Training Accuracy: 0.984375 Loss: 0.0008685963111929595\n",
      "Iteration: 2890 Training Accuracy: 1.0 Loss: 0.00033212624839507043\n",
      "Iteration: 2900 Training Accuracy: 1.0 Loss: 0.0004115152405574918\n",
      "Iteration: 2910 Training Accuracy: 0.984375 Loss: 0.00036964542232453823\n",
      "Iteration: 2920 Training Accuracy: 0.984375 Loss: 0.0009914111578837037\n",
      "Iteration: 2930 Training Accuracy: 1.0 Loss: 4.092571180080995e-05\n",
      "Iteration: 2940 Training Accuracy: 0.984375 Loss: 0.00039383917464874685\n",
      "Iteration: 2950 Training Accuracy: 1.0 Loss: 0.0001295608381042257\n",
      "Iteration: 2960 Training Accuracy: 1.0 Loss: 8.235294080805033e-05\n",
      "Iteration: 2970 Training Accuracy: 0.984375 Loss: 0.0006648918497376144\n",
      "Iteration: 2980 Training Accuracy: 0.984375 Loss: 0.0003501025203149766\n",
      "Iteration: 2990 Training Accuracy: 1.0 Loss: 8.663914923090488e-05\n",
      "Iteration: 3000 Training Accuracy: 1.0 Loss: 3.940224996767938e-05\n",
      "Iteration: 3010 Training Accuracy: 1.0 Loss: 0.00015637111209798604\n",
      "Iteration: 3020 Training Accuracy: 1.0 Loss: 0.00038578861858695745\n",
      "Iteration: 3030 Training Accuracy: 0.984375 Loss: 0.0009148239041678607\n",
      "Iteration: 3040 Training Accuracy: 0.984375 Loss: 0.00046324345748871565\n",
      "Iteration: 3050 Training Accuracy: 1.0 Loss: 9.742974361870438e-05\n",
      "Iteration: 3060 Training Accuracy: 0.984375 Loss: 0.0006247714627534151\n",
      "Iteration: 3070 Training Accuracy: 1.0 Loss: 0.00016047104145400226\n",
      "Iteration: 3080 Training Accuracy: 1.0 Loss: 0.00025322704459540546\n",
      "Iteration: 3090 Training Accuracy: 1.0 Loss: 6.711454625474289e-05\n",
      "Iteration: 3100 Training Accuracy: 1.0 Loss: 6.590387056348845e-05\n",
      "Iteration: 3110 Training Accuracy: 1.0 Loss: 0.00015904199972283095\n",
      "Iteration: 3120 Training Accuracy: 0.984375 Loss: 0.00021746761922258884\n",
      "Iteration: 3130 Training Accuracy: 1.0 Loss: 9.323818812845275e-05\n",
      "Iteration: 3140 Training Accuracy: 1.0 Loss: 3.922840187442489e-05\n",
      "Iteration: 3150 Training Accuracy: 0.984375 Loss: 0.0009016240946948528\n",
      "Iteration: 3160 Training Accuracy: 0.984375 Loss: 0.0004435808805283159\n",
      "Iteration: 3170 Training Accuracy: 1.0 Loss: 0.0001174304707092233\n",
      "Iteration: 3180 Training Accuracy: 1.0 Loss: 3.0182860427885316e-05\n",
      "Iteration: 3190 Training Accuracy: 1.0 Loss: 0.00023478576622437686\n",
      "Iteration: 3200 Training Accuracy: 0.984375 Loss: 0.0006059820298105478\n",
      "Iteration: 3210 Training Accuracy: 0.984375 Loss: 0.0003759458486456424\n",
      "Iteration: 3220 Training Accuracy: 1.0 Loss: 0.0001481663202866912\n",
      "Iteration: 3230 Training Accuracy: 1.0 Loss: 0.0001797940203687176\n",
      "Iteration: 3240 Training Accuracy: 1.0 Loss: 6.187257531564683e-05\n",
      "Iteration: 3250 Training Accuracy: 1.0 Loss: 0.00011991804785793647\n",
      "Iteration: 3260 Training Accuracy: 1.0 Loss: 0.00014605495380237699\n",
      "Iteration: 3270 Training Accuracy: 1.0 Loss: 4.6853561798343435e-05\n",
      "Iteration: 3280 Training Accuracy: 1.0 Loss: 9.565755317453295e-05\n",
      "Iteration: 3290 Training Accuracy: 1.0 Loss: 0.00020861736265942454\n",
      "Iteration: 3300 Training Accuracy: 0.984375 Loss: 0.000274311110842973\n",
      "Iteration: 3310 Training Accuracy: 0.984375 Loss: 0.0006559423636645079\n",
      "Iteration: 3320 Training Accuracy: 1.0 Loss: 6.96469796821475e-05\n",
      "Iteration: 3330 Training Accuracy: 1.0 Loss: 8.791722939349711e-05\n",
      "Iteration: 3340 Training Accuracy: 1.0 Loss: 0.000131128792418167\n",
      "Iteration: 3350 Training Accuracy: 0.984375 Loss: 0.0002841678506229073\n",
      "Iteration: 3360 Training Accuracy: 0.984375 Loss: 0.00046385909081436694\n",
      "Iteration: 3370 Training Accuracy: 1.0 Loss: 0.00016164399858098477\n",
      "Iteration: 3380 Training Accuracy: 1.0 Loss: 6.336953811114654e-05\n",
      "Iteration: 3390 Training Accuracy: 1.0 Loss: 5.774842429673299e-05\n",
      "Iteration: 3400 Training Accuracy: 1.0 Loss: 4.5455166400643066e-05\n",
      "Iteration: 3410 Training Accuracy: 0.984375 Loss: 0.0004646992601919919\n",
      "Iteration: 3420 Training Accuracy: 1.0 Loss: 0.00016794570547062904\n",
      "Iteration: 3430 Training Accuracy: 1.0 Loss: 0.00011445795826148242\n",
      "Iteration: 3440 Training Accuracy: 1.0 Loss: 9.604293154552579e-06\n",
      "Iteration: 3450 Training Accuracy: 1.0 Loss: 5.568999768001959e-05\n",
      "Iteration: 3460 Training Accuracy: 0.984375 Loss: 0.0004784635966643691\n",
      "Iteration: 3470 Training Accuracy: 1.0 Loss: 0.0005559147102758288\n",
      "Iteration: 3480 Training Accuracy: 1.0 Loss: 0.00027573975967243314\n",
      "Iteration: 3490 Training Accuracy: 1.0 Loss: 0.0002866146678570658\n",
      "Iteration: 3500 Training Accuracy: 1.0 Loss: 0.00022013916168361902\n",
      "Iteration: 3510 Training Accuracy: 1.0 Loss: 0.00025343531160615385\n",
      "Iteration: 3520 Training Accuracy: 0.984375 Loss: 0.0002662961487658322\n",
      "Iteration: 3530 Training Accuracy: 1.0 Loss: 0.00013533019227907062\n",
      "Iteration: 3540 Training Accuracy: 1.0 Loss: 0.00011677972361212596\n",
      "Iteration: 3550 Training Accuracy: 1.0 Loss: 0.0005869498709216714\n",
      "Iteration: 3560 Training Accuracy: 0.984375 Loss: 0.0006400554557330906\n",
      "Iteration: 3570 Training Accuracy: 1.0 Loss: 0.00020167719048913568\n",
      "Iteration: 3580 Training Accuracy: 0.984375 Loss: 0.0003956249274779111\n",
      "Iteration: 3590 Training Accuracy: 1.0 Loss: 5.0585498684085906e-05\n",
      "Iteration: 3600 Training Accuracy: 1.0 Loss: 0.0001813767885323614\n",
      "Iteration: 3610 Training Accuracy: 0.984375 Loss: 0.0003954026033170521\n",
      "Iteration: 3620 Training Accuracy: 1.0 Loss: 0.00010288589692208916\n",
      "Iteration: 3630 Training Accuracy: 1.0 Loss: 6.867638148833066e-05\n",
      "Iteration: 3640 Training Accuracy: 0.984375 Loss: 0.00039192920667119324\n",
      "Iteration: 3650 Training Accuracy: 1.0 Loss: 0.0002746079699136317\n",
      "Iteration: 3660 Training Accuracy: 1.0 Loss: 0.00023396775941364467\n",
      "Iteration: 3670 Training Accuracy: 1.0 Loss: 0.00023940835671965033\n",
      "Iteration: 3680 Training Accuracy: 0.984375 Loss: 0.00033400760730728507\n",
      "Iteration: 3690 Training Accuracy: 1.0 Loss: 0.0001231810892932117\n",
      "Iteration: 3700 Training Accuracy: 1.0 Loss: 0.00024766166461631656\n",
      "Iteration: 3710 Training Accuracy: 1.0 Loss: 5.152808080310933e-05\n",
      "Iteration: 3720 Training Accuracy: 0.984375 Loss: 0.000303151406114921\n",
      "Iteration: 3730 Training Accuracy: 1.0 Loss: 0.00024119505542330444\n",
      "Iteration: 3740 Training Accuracy: 1.0 Loss: 2.0745163055835292e-05\n",
      "Iteration: 3750 Training Accuracy: 1.0 Loss: 6.274816405493766e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9696666666666667\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 1.0 Loss: 5.486717782332562e-05\n",
      "Iteration: 3770 Training Accuracy: 1.0 Loss: 2.8360467695165426e-05\n",
      "Iteration: 3780 Training Accuracy: 0.984375 Loss: 0.00042537134140729904\n",
      "Iteration: 3790 Training Accuracy: 1.0 Loss: 0.0001461879292037338\n",
      "Iteration: 3800 Training Accuracy: 1.0 Loss: 0.0001628812460694462\n",
      "Iteration: 3810 Training Accuracy: 1.0 Loss: 9.403791045770049e-05\n",
      "Iteration: 3820 Training Accuracy: 1.0 Loss: 0.00026937873917631805\n",
      "Iteration: 3830 Training Accuracy: 1.0 Loss: 0.00028170866426080465\n",
      "Iteration: 3840 Training Accuracy: 0.984375 Loss: 0.00038255052641034126\n",
      "Iteration: 3850 Training Accuracy: 1.0 Loss: 8.794500172371045e-05\n",
      "Iteration: 3860 Training Accuracy: 1.0 Loss: 0.00016050098929554224\n",
      "Iteration: 3870 Training Accuracy: 1.0 Loss: 0.0001258604315808043\n",
      "Iteration: 3880 Training Accuracy: 1.0 Loss: 0.00021090218797326088\n",
      "Iteration: 3890 Training Accuracy: 1.0 Loss: 0.0003870819928124547\n",
      "Iteration: 3900 Training Accuracy: 1.0 Loss: 0.00011842528328998014\n",
      "Iteration: 3910 Training Accuracy: 1.0 Loss: 2.8651187676587142e-05\n",
      "Iteration: 3920 Training Accuracy: 1.0 Loss: 0.00011413561878725886\n",
      "Iteration: 3930 Training Accuracy: 1.0 Loss: 3.544516584952362e-05\n",
      "Iteration: 3940 Training Accuracy: 1.0 Loss: 9.077276627067477e-05\n",
      "Iteration: 3950 Training Accuracy: 1.0 Loss: 7.014590664766729e-05\n",
      "Iteration: 3960 Training Accuracy: 1.0 Loss: 4.394279676489532e-05\n",
      "Iteration: 3970 Training Accuracy: 1.0 Loss: 0.0002325708046555519\n",
      "Iteration: 3980 Training Accuracy: 1.0 Loss: 9.111004328588024e-05\n",
      "Iteration: 3990 Training Accuracy: 1.0 Loss: 5.467026858241297e-05\n",
      "Iteration: 4000 Training Accuracy: 1.0 Loss: 0.00024827130255289376\n",
      "Iteration: 4010 Training Accuracy: 1.0 Loss: 0.00017569016199558973\n",
      "Iteration: 4020 Training Accuracy: 0.96875 Loss: 0.001386031392030418\n",
      "Iteration: 4030 Training Accuracy: 1.0 Loss: 0.00019634320051409304\n",
      "Iteration: 4040 Training Accuracy: 1.0 Loss: 8.965301094576716e-05\n",
      "Iteration: 4050 Training Accuracy: 1.0 Loss: 9.290575690101832e-05\n",
      "Iteration: 4060 Training Accuracy: 1.0 Loss: 8.738860924495384e-05\n",
      "Iteration: 4070 Training Accuracy: 1.0 Loss: 0.00030785400304012\n",
      "Iteration: 4080 Training Accuracy: 1.0 Loss: 1.4938751519366633e-05\n",
      "Iteration: 4090 Training Accuracy: 1.0 Loss: 2.7547132049221545e-05\n",
      "Iteration: 4100 Training Accuracy: 1.0 Loss: 1.6691834389348514e-05\n",
      "Iteration: 4110 Training Accuracy: 1.0 Loss: 1.79834842128912e-05\n",
      "Iteration: 4120 Training Accuracy: 1.0 Loss: 0.0005075467051938176\n",
      "Iteration: 4130 Training Accuracy: 0.984375 Loss: 0.0007553258910775185\n",
      "Iteration: 4140 Training Accuracy: 1.0 Loss: 4.7376212023664266e-05\n",
      "Iteration: 4150 Training Accuracy: 1.0 Loss: 3.066059434786439e-05\n",
      "Iteration: 4160 Training Accuracy: 1.0 Loss: 0.00020545623556245118\n",
      "Iteration: 4170 Training Accuracy: 1.0 Loss: 1.685852839727886e-05\n",
      "Iteration: 4180 Training Accuracy: 1.0 Loss: 0.0002736759779509157\n",
      "Iteration: 4190 Training Accuracy: 1.0 Loss: 0.00017565462621860206\n",
      "Iteration: 4200 Training Accuracy: 1.0 Loss: 0.0002866873110178858\n",
      "Iteration: 4210 Training Accuracy: 1.0 Loss: 2.3030392185319215e-05\n",
      "Iteration: 4220 Training Accuracy: 1.0 Loss: 0.00010306847980245948\n",
      "Iteration: 4230 Training Accuracy: 0.984375 Loss: 0.00035176618257537484\n",
      "Iteration: 4240 Training Accuracy: 1.0 Loss: 4.6630037104478106e-05\n",
      "Iteration: 4250 Training Accuracy: 1.0 Loss: 5.57196035515517e-05\n",
      "Iteration: 4260 Training Accuracy: 1.0 Loss: 3.771165938815102e-05\n",
      "Iteration: 4270 Training Accuracy: 1.0 Loss: 0.0001288317725993693\n",
      "Iteration: 4280 Training Accuracy: 0.984375 Loss: 0.00043020694283768535\n",
      "Iteration: 4290 Training Accuracy: 1.0 Loss: 0.0003733334015123546\n",
      "Iteration: 4300 Training Accuracy: 1.0 Loss: 0.0003224056272301823\n",
      "Iteration: 4310 Training Accuracy: 1.0 Loss: 0.00016216287622228265\n",
      "Iteration: 4320 Training Accuracy: 1.0 Loss: 0.00012117197184124961\n",
      "Iteration: 4330 Training Accuracy: 1.0 Loss: 0.00015960326709318906\n",
      "Iteration: 4340 Training Accuracy: 1.0 Loss: 0.00030804716516286135\n",
      "Iteration: 4350 Training Accuracy: 1.0 Loss: 7.752014062134549e-05\n",
      "Iteration: 4360 Training Accuracy: 1.0 Loss: 4.267031908966601e-05\n",
      "Iteration: 4370 Training Accuracy: 1.0 Loss: 3.9773400203557685e-05\n",
      "Iteration: 4380 Training Accuracy: 1.0 Loss: 6.165924423839897e-05\n",
      "Iteration: 4390 Training Accuracy: 1.0 Loss: 0.00026599279954098165\n",
      "Iteration: 4400 Training Accuracy: 1.0 Loss: 6.7599462454381865e-06\n",
      "Iteration: 4410 Training Accuracy: 1.0 Loss: 6.533027772093192e-05\n",
      "Iteration: 4420 Training Accuracy: 1.0 Loss: 0.0003044366021640599\n",
      "Iteration: 4430 Training Accuracy: 0.984375 Loss: 0.0002035392535617575\n",
      "Iteration: 4440 Training Accuracy: 1.0 Loss: 0.0001559905067551881\n",
      "Iteration: 4450 Training Accuracy: 1.0 Loss: 7.725678187853191e-06\n",
      "Iteration: 4460 Training Accuracy: 1.0 Loss: 0.00020468288857955486\n",
      "Iteration: 4470 Training Accuracy: 0.984375 Loss: 0.0005090921767987311\n",
      "Iteration: 4480 Training Accuracy: 1.0 Loss: 3.5707686038222164e-05\n",
      "Iteration: 4490 Training Accuracy: 1.0 Loss: 0.0002049332979368046\n",
      "Iteration: 4500 Training Accuracy: 0.984375 Loss: 0.0003871328372042626\n",
      "Iteration: 4510 Training Accuracy: 0.984375 Loss: 0.00021775727509520948\n",
      "Iteration: 4520 Training Accuracy: 0.984375 Loss: 0.0003072984400205314\n",
      "Iteration: 4530 Training Accuracy: 1.0 Loss: 0.00023236832930706441\n",
      "Iteration: 4540 Training Accuracy: 1.0 Loss: 3.555183866410516e-05\n",
      "Iteration: 4550 Training Accuracy: 0.984375 Loss: 0.0003187398542650044\n",
      "Iteration: 4560 Training Accuracy: 1.0 Loss: 5.3705658501712605e-05\n",
      "Iteration: 4570 Training Accuracy: 0.984375 Loss: 0.00048017973313108087\n",
      "Iteration: 4580 Training Accuracy: 1.0 Loss: 2.5989451387431473e-05\n",
      "Iteration: 4590 Training Accuracy: 1.0 Loss: 1.1435080523369834e-05\n",
      "Iteration: 4600 Training Accuracy: 1.0 Loss: 0.00012114764103898779\n",
      "Iteration: 4610 Training Accuracy: 1.0 Loss: 0.00013174755440559238\n",
      "Iteration: 4620 Training Accuracy: 1.0 Loss: 0.00018260626529809088\n",
      "Iteration: 4630 Training Accuracy: 0.984375 Loss: 0.0006384990410879254\n",
      "Iteration: 4640 Training Accuracy: 1.0 Loss: 0.0004143144760746509\n",
      "Iteration: 4650 Training Accuracy: 1.0 Loss: 7.433877271978417e-06\n",
      "Iteration: 4660 Training Accuracy: 1.0 Loss: 9.514028351986781e-05\n",
      "Iteration: 4670 Training Accuracy: 1.0 Loss: 6.603189831366763e-05\n",
      "Iteration: 4680 Training Accuracy: 1.0 Loss: 0.00013067973486613482\n",
      "Iteration: 4690 Training Accuracy: 1.0 Loss: 3.246785490773618e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9716666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.9983833333333333\n",
      "Validation Accuracy = 0.9716666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RESNET18(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use res net \n",
    "model = torch.load(\"full_resnet18_4.pth\")\n",
    "\n",
    "\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'full_resnet18_100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features1 = 256\n",
    "out_features2 = 128\n",
    "out_features3 = 64\n",
    "\n",
    "p = 0.6\n",
    "\n",
    "class RESNET18_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RESNET18_2, self).__init__()\n",
    "        # remove fully connected layer at the end\n",
    "        self.resnet = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "        # freeze parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.fc1 = nn.Linear(resnet18.fc.in_features, out_features1)\n",
    "        self.fc2 = nn.Linear(out_features1, out_features2)\n",
    "        self.fc3 = nn.Linear(out_features2, out_features3)\n",
    "        self.fc4 = nn.Linear(out_features3, 30)\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_features1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(out_features2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(out_features3)\n",
    "\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        \n",
    "        # Forward pass through your fully connected layers\n",
    "        x = F.relu(self.batchnorm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.batchnorm3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.015625 Loss: 0.053651198744773865\n",
      "Iteration: 20 Training Accuracy: 0.109375 Loss: 0.05259404703974724\n",
      "Iteration: 30 Training Accuracy: 0.09375 Loss: 0.051963672041893005\n",
      "Iteration: 40 Training Accuracy: 0.046875 Loss: 0.052610356360673904\n",
      "Iteration: 50 Training Accuracy: 0.046875 Loss: 0.05193588510155678\n",
      "Iteration: 60 Training Accuracy: 0.078125 Loss: 0.05143144726753235\n",
      "Iteration: 70 Training Accuracy: 0.078125 Loss: 0.0509256012737751\n",
      "Iteration: 80 Training Accuracy: 0.125 Loss: 0.049269311130046844\n",
      "Iteration: 90 Training Accuracy: 0.125 Loss: 0.04803340882062912\n",
      "Iteration: 100 Training Accuracy: 0.125 Loss: 0.0479058176279068\n",
      "Iteration: 110 Training Accuracy: 0.21875 Loss: 0.04518899694085121\n",
      "Iteration: 120 Training Accuracy: 0.28125 Loss: 0.042560920119285583\n",
      "Iteration: 130 Training Accuracy: 0.171875 Loss: 0.045052893459796906\n",
      "Iteration: 140 Training Accuracy: 0.328125 Loss: 0.0406201034784317\n",
      "Iteration: 150 Training Accuracy: 0.234375 Loss: 0.04261288791894913\n",
      "Iteration: 160 Training Accuracy: 0.265625 Loss: 0.040208347141742706\n",
      "Iteration: 170 Training Accuracy: 0.265625 Loss: 0.0390530601143837\n",
      "Iteration: 180 Training Accuracy: 0.3125 Loss: 0.03788512945175171\n",
      "Iteration: 190 Training Accuracy: 0.34375 Loss: 0.03856990486383438\n",
      "Iteration: 200 Training Accuracy: 0.4375 Loss: 0.036565035581588745\n",
      "Iteration: 210 Training Accuracy: 0.3125 Loss: 0.03904443979263306\n",
      "Iteration: 220 Training Accuracy: 0.359375 Loss: 0.03460698202252388\n",
      "Iteration: 230 Training Accuracy: 0.359375 Loss: 0.03453011438250542\n",
      "Iteration: 240 Training Accuracy: 0.28125 Loss: 0.03585340827703476\n",
      "Iteration: 250 Training Accuracy: 0.34375 Loss: 0.03266014903783798\n",
      "Iteration: 260 Training Accuracy: 0.4375 Loss: 0.030066831037402153\n",
      "Iteration: 270 Training Accuracy: 0.375 Loss: 0.033310770988464355\n",
      "Iteration: 280 Training Accuracy: 0.4375 Loss: 0.031731389462947845\n",
      "Iteration: 290 Training Accuracy: 0.46875 Loss: 0.028365731239318848\n",
      "Iteration: 300 Training Accuracy: 0.484375 Loss: 0.028949249535799026\n",
      "Iteration: 310 Training Accuracy: 0.3125 Loss: 0.03486772999167442\n",
      "Iteration: 320 Training Accuracy: 0.390625 Loss: 0.031031064689159393\n",
      "Iteration: 330 Training Accuracy: 0.40625 Loss: 0.027324318885803223\n",
      "Iteration: 340 Training Accuracy: 0.359375 Loss: 0.029795071110129356\n",
      "Iteration: 350 Training Accuracy: 0.421875 Loss: 0.02917836233973503\n",
      "Iteration: 360 Training Accuracy: 0.390625 Loss: 0.02999749593436718\n",
      "Iteration: 370 Training Accuracy: 0.484375 Loss: 0.025745421648025513\n",
      "Iteration: 380 Training Accuracy: 0.4375 Loss: 0.030613373965024948\n",
      "Iteration: 390 Training Accuracy: 0.40625 Loss: 0.02896278351545334\n",
      "Iteration: 400 Training Accuracy: 0.4375 Loss: 0.026745138689875603\n",
      "Iteration: 410 Training Accuracy: 0.4375 Loss: 0.02918846160173416\n",
      "Iteration: 420 Training Accuracy: 0.515625 Loss: 0.024541329592466354\n",
      "Iteration: 430 Training Accuracy: 0.375 Loss: 0.029587220400571823\n",
      "Iteration: 440 Training Accuracy: 0.515625 Loss: 0.021849840879440308\n",
      "Iteration: 450 Training Accuracy: 0.390625 Loss: 0.026158684864640236\n",
      "Iteration: 460 Training Accuracy: 0.46875 Loss: 0.02487490139901638\n",
      "Iteration: 470 Training Accuracy: 0.546875 Loss: 0.0242738276720047\n",
      "Iteration: 480 Training Accuracy: 0.5 Loss: 0.025172030553221703\n",
      "Iteration: 490 Training Accuracy: 0.5 Loss: 0.022283516824245453\n",
      "Iteration: 500 Training Accuracy: 0.390625 Loss: 0.027632854878902435\n",
      "Iteration: 510 Training Accuracy: 0.5 Loss: 0.024482475593686104\n",
      "Iteration: 520 Training Accuracy: 0.46875 Loss: 0.02471906691789627\n",
      "Iteration: 530 Training Accuracy: 0.515625 Loss: 0.020939446985721588\n",
      "Iteration: 540 Training Accuracy: 0.5 Loss: 0.02318372018635273\n",
      "Iteration: 550 Training Accuracy: 0.5 Loss: 0.02419263869524002\n",
      "Iteration: 560 Training Accuracy: 0.625 Loss: 0.02484498918056488\n",
      "Iteration: 570 Training Accuracy: 0.46875 Loss: 0.02312992513179779\n",
      "Iteration: 580 Training Accuracy: 0.484375 Loss: 0.02605864219367504\n",
      "Iteration: 590 Training Accuracy: 0.546875 Loss: 0.02083565481007099\n",
      "Iteration: 600 Training Accuracy: 0.53125 Loss: 0.023634005337953568\n",
      "Iteration: 610 Training Accuracy: 0.625 Loss: 0.01782066375017166\n",
      "Iteration: 620 Training Accuracy: 0.484375 Loss: 0.02719438448548317\n",
      "Iteration: 630 Training Accuracy: 0.46875 Loss: 0.02272876724600792\n",
      "Iteration: 640 Training Accuracy: 0.5625 Loss: 0.022161783650517464\n",
      "Iteration: 650 Training Accuracy: 0.578125 Loss: 0.018357109278440475\n",
      "Iteration: 660 Training Accuracy: 0.390625 Loss: 0.023239487782120705\n",
      "Iteration: 670 Training Accuracy: 0.46875 Loss: 0.023410694673657417\n",
      "Iteration: 680 Training Accuracy: 0.453125 Loss: 0.027215540409088135\n",
      "Iteration: 690 Training Accuracy: 0.546875 Loss: 0.020973272621631622\n",
      "Iteration: 700 Training Accuracy: 0.59375 Loss: 0.02115534618496895\n",
      "Iteration: 710 Training Accuracy: 0.6875 Loss: 0.016603048890829086\n",
      "Iteration: 720 Training Accuracy: 0.390625 Loss: 0.028350234031677246\n",
      "Iteration: 730 Training Accuracy: 0.578125 Loss: 0.02223014459013939\n",
      "Iteration: 740 Training Accuracy: 0.546875 Loss: 0.023143786936998367\n",
      "Iteration: 750 Training Accuracy: 0.53125 Loss: 0.021997343748807907\n",
      "Iteration: 760 Training Accuracy: 0.578125 Loss: 0.022260243073105812\n",
      "Iteration: 770 Training Accuracy: 0.578125 Loss: 0.019870061427354813\n",
      "Iteration: 780 Training Accuracy: 0.546875 Loss: 0.022649195045232773\n",
      "Iteration: 790 Training Accuracy: 0.59375 Loss: 0.022961467504501343\n",
      "Iteration: 800 Training Accuracy: 0.4375 Loss: 0.023419499397277832\n",
      "Iteration: 810 Training Accuracy: 0.59375 Loss: 0.02162603661417961\n",
      "Iteration: 820 Training Accuracy: 0.546875 Loss: 0.020559167489409447\n",
      "Iteration: 830 Training Accuracy: 0.5 Loss: 0.024414505809545517\n",
      "Iteration: 840 Training Accuracy: 0.609375 Loss: 0.018286537379026413\n",
      "Iteration: 850 Training Accuracy: 0.515625 Loss: 0.022462591528892517\n",
      "Iteration: 860 Training Accuracy: 0.546875 Loss: 0.02108215168118477\n",
      "Iteration: 870 Training Accuracy: 0.515625 Loss: 0.021667754277586937\n",
      "Iteration: 880 Training Accuracy: 0.546875 Loss: 0.02109719067811966\n",
      "Iteration: 890 Training Accuracy: 0.59375 Loss: 0.018325304612517357\n",
      "Iteration: 900 Training Accuracy: 0.59375 Loss: 0.01722653955221176\n",
      "Iteration: 910 Training Accuracy: 0.5625 Loss: 0.02089863084256649\n",
      "Iteration: 920 Training Accuracy: 0.65625 Loss: 0.019244303926825523\n",
      "Iteration: 930 Training Accuracy: 0.609375 Loss: 0.02121947705745697\n",
      "Training Accuracy = 0.53125\n",
      "Validation Accuracy = 0.7298333333333333\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 0.71875 Loss: 0.018035288900136948\n",
      "Iteration: 950 Training Accuracy: 0.71875 Loss: 0.016521811485290527\n",
      "Iteration: 960 Training Accuracy: 0.6875 Loss: 0.012904156930744648\n",
      "Iteration: 970 Training Accuracy: 0.640625 Loss: 0.013388637453317642\n",
      "Iteration: 980 Training Accuracy: 0.734375 Loss: 0.013211077079176903\n",
      "Iteration: 990 Training Accuracy: 0.8125 Loss: 0.009882116690278053\n",
      "Iteration: 1000 Training Accuracy: 0.765625 Loss: 0.01126844622194767\n",
      "Iteration: 1010 Training Accuracy: 0.796875 Loss: 0.010753530077636242\n",
      "Iteration: 1020 Training Accuracy: 0.8125 Loss: 0.008668044582009315\n",
      "Iteration: 1030 Training Accuracy: 0.765625 Loss: 0.009683424606919289\n",
      "Iteration: 1040 Training Accuracy: 0.671875 Loss: 0.012638386338949203\n",
      "Iteration: 1050 Training Accuracy: 0.703125 Loss: 0.014365816488862038\n",
      "Iteration: 1060 Training Accuracy: 0.734375 Loss: 0.01089724525809288\n",
      "Iteration: 1070 Training Accuracy: 0.703125 Loss: 0.01271210890263319\n",
      "Iteration: 1080 Training Accuracy: 0.84375 Loss: 0.008942721411585808\n",
      "Iteration: 1090 Training Accuracy: 0.8125 Loss: 0.011695382185280323\n",
      "Iteration: 1100 Training Accuracy: 0.765625 Loss: 0.010706968605518341\n",
      "Iteration: 1110 Training Accuracy: 0.90625 Loss: 0.007534967735409737\n",
      "Iteration: 1120 Training Accuracy: 0.875 Loss: 0.0084114084020257\n",
      "Iteration: 1130 Training Accuracy: 0.921875 Loss: 0.007257363293319941\n",
      "Iteration: 1140 Training Accuracy: 0.890625 Loss: 0.00727406982332468\n",
      "Iteration: 1150 Training Accuracy: 0.734375 Loss: 0.012027261778712273\n",
      "Iteration: 1160 Training Accuracy: 0.8125 Loss: 0.01238523330539465\n",
      "Iteration: 1170 Training Accuracy: 0.734375 Loss: 0.011955353431403637\n",
      "Iteration: 1180 Training Accuracy: 0.90625 Loss: 0.0074489181861281395\n",
      "Iteration: 1190 Training Accuracy: 0.734375 Loss: 0.010331335477530956\n",
      "Iteration: 1200 Training Accuracy: 0.796875 Loss: 0.010680684819817543\n",
      "Iteration: 1210 Training Accuracy: 0.765625 Loss: 0.01054757833480835\n",
      "Iteration: 1220 Training Accuracy: 0.734375 Loss: 0.012997008860111237\n",
      "Iteration: 1230 Training Accuracy: 0.734375 Loss: 0.012398292310535908\n",
      "Iteration: 1240 Training Accuracy: 0.78125 Loss: 0.008878592401742935\n",
      "Iteration: 1250 Training Accuracy: 0.765625 Loss: 0.01094761397689581\n",
      "Iteration: 1260 Training Accuracy: 0.78125 Loss: 0.011746807023882866\n",
      "Iteration: 1270 Training Accuracy: 0.90625 Loss: 0.0061975824646651745\n",
      "Iteration: 1280 Training Accuracy: 0.8125 Loss: 0.006702562794089317\n",
      "Iteration: 1290 Training Accuracy: 0.765625 Loss: 0.010017438791692257\n",
      "Iteration: 1300 Training Accuracy: 0.734375 Loss: 0.008840879425406456\n",
      "Iteration: 1310 Training Accuracy: 0.828125 Loss: 0.00888284482061863\n",
      "Iteration: 1320 Training Accuracy: 0.75 Loss: 0.012321729212999344\n",
      "Iteration: 1330 Training Accuracy: 0.8125 Loss: 0.008948461152613163\n",
      "Iteration: 1340 Training Accuracy: 0.78125 Loss: 0.01037224754691124\n",
      "Iteration: 1350 Training Accuracy: 0.75 Loss: 0.01110963337123394\n",
      "Iteration: 1360 Training Accuracy: 0.734375 Loss: 0.011078955605626106\n",
      "Iteration: 1370 Training Accuracy: 0.71875 Loss: 0.01139635406434536\n",
      "Iteration: 1380 Training Accuracy: 0.84375 Loss: 0.005947981029748917\n",
      "Iteration: 1390 Training Accuracy: 0.71875 Loss: 0.013775631785392761\n",
      "Iteration: 1400 Training Accuracy: 0.796875 Loss: 0.0096886046230793\n",
      "Iteration: 1410 Training Accuracy: 0.828125 Loss: 0.007925559766590595\n",
      "Iteration: 1420 Training Accuracy: 0.8125 Loss: 0.0088003259152174\n",
      "Iteration: 1430 Training Accuracy: 0.875 Loss: 0.007426198571920395\n",
      "Iteration: 1440 Training Accuracy: 0.8125 Loss: 0.008260970935225487\n",
      "Iteration: 1450 Training Accuracy: 0.828125 Loss: 0.009283751249313354\n",
      "Iteration: 1460 Training Accuracy: 0.78125 Loss: 0.008317291736602783\n",
      "Iteration: 1470 Training Accuracy: 0.8125 Loss: 0.00943855568766594\n",
      "Iteration: 1480 Training Accuracy: 0.828125 Loss: 0.00855371356010437\n",
      "Iteration: 1490 Training Accuracy: 0.78125 Loss: 0.01004246436059475\n",
      "Iteration: 1500 Training Accuracy: 0.90625 Loss: 0.0045035723596811295\n",
      "Iteration: 1510 Training Accuracy: 0.8125 Loss: 0.00831950455904007\n",
      "Iteration: 1520 Training Accuracy: 0.875 Loss: 0.007258332800120115\n",
      "Iteration: 1530 Training Accuracy: 0.84375 Loss: 0.006757928524166346\n",
      "Iteration: 1540 Training Accuracy: 0.84375 Loss: 0.010143330320715904\n",
      "Iteration: 1550 Training Accuracy: 0.75 Loss: 0.009533199481666088\n",
      "Iteration: 1560 Training Accuracy: 0.78125 Loss: 0.009053068235516548\n",
      "Iteration: 1570 Training Accuracy: 0.84375 Loss: 0.007843414321541786\n",
      "Iteration: 1580 Training Accuracy: 0.828125 Loss: 0.006483702454715967\n",
      "Iteration: 1590 Training Accuracy: 0.859375 Loss: 0.008340545929968357\n",
      "Iteration: 1600 Training Accuracy: 0.828125 Loss: 0.006661230698227882\n",
      "Iteration: 1610 Training Accuracy: 0.875 Loss: 0.00941153522580862\n",
      "Iteration: 1620 Training Accuracy: 0.875 Loss: 0.007316454313695431\n",
      "Iteration: 1630 Training Accuracy: 0.828125 Loss: 0.008546194061636925\n",
      "Iteration: 1640 Training Accuracy: 0.765625 Loss: 0.012053150683641434\n",
      "Iteration: 1650 Training Accuracy: 0.875 Loss: 0.006918992847204208\n",
      "Iteration: 1660 Training Accuracy: 0.859375 Loss: 0.007044938858598471\n",
      "Iteration: 1670 Training Accuracy: 0.859375 Loss: 0.006645976100116968\n",
      "Iteration: 1680 Training Accuracy: 0.921875 Loss: 0.0048472145572304726\n",
      "Iteration: 1690 Training Accuracy: 0.84375 Loss: 0.006985334679484367\n",
      "Iteration: 1700 Training Accuracy: 0.828125 Loss: 0.008248932659626007\n",
      "Iteration: 1710 Training Accuracy: 0.796875 Loss: 0.008917378261685371\n",
      "Iteration: 1720 Training Accuracy: 0.8125 Loss: 0.009198176674544811\n",
      "Iteration: 1730 Training Accuracy: 0.8125 Loss: 0.009867715649306774\n",
      "Iteration: 1740 Training Accuracy: 0.796875 Loss: 0.008808361366391182\n",
      "Iteration: 1750 Training Accuracy: 0.78125 Loss: 0.008020827546715736\n",
      "Iteration: 1760 Training Accuracy: 0.828125 Loss: 0.007929641753435135\n",
      "Iteration: 1770 Training Accuracy: 0.875 Loss: 0.007136547937989235\n",
      "Iteration: 1780 Training Accuracy: 0.78125 Loss: 0.00989480409771204\n",
      "Iteration: 1790 Training Accuracy: 0.765625 Loss: 0.010925663635134697\n",
      "Iteration: 1800 Training Accuracy: 0.8125 Loss: 0.007775654084980488\n",
      "Iteration: 1810 Training Accuracy: 0.921875 Loss: 0.004535791464149952\n",
      "Iteration: 1820 Training Accuracy: 0.828125 Loss: 0.008747824467718601\n",
      "Iteration: 1830 Training Accuracy: 0.828125 Loss: 0.006485660560429096\n",
      "Iteration: 1840 Training Accuracy: 0.875 Loss: 0.00509156147018075\n",
      "Iteration: 1850 Training Accuracy: 0.828125 Loss: 0.009113327600061893\n",
      "Iteration: 1860 Training Accuracy: 0.84375 Loss: 0.008075474761426449\n",
      "Iteration: 1870 Training Accuracy: 0.90625 Loss: 0.005572555586695671\n",
      "Training Accuracy = 0.84375\n",
      "Validation Accuracy = 0.8235\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 0.734375 Loss: 0.012717833742499352\n",
      "Iteration: 1890 Training Accuracy: 0.84375 Loss: 0.008141194470226765\n",
      "Iteration: 1900 Training Accuracy: 0.875 Loss: 0.005135181825608015\n",
      "Iteration: 1910 Training Accuracy: 0.859375 Loss: 0.006627579685300589\n",
      "Iteration: 1920 Training Accuracy: 0.8125 Loss: 0.008893553167581558\n",
      "Iteration: 1930 Training Accuracy: 0.8125 Loss: 0.0073605673387646675\n",
      "Iteration: 1940 Training Accuracy: 0.859375 Loss: 0.0063030472956597805\n",
      "Iteration: 1950 Training Accuracy: 0.8125 Loss: 0.00874166190624237\n",
      "Iteration: 1960 Training Accuracy: 0.9375 Loss: 0.0037796031683683395\n",
      "Iteration: 1970 Training Accuracy: 0.859375 Loss: 0.0052204495295882225\n",
      "Iteration: 1980 Training Accuracy: 0.8125 Loss: 0.00796850211918354\n",
      "Iteration: 1990 Training Accuracy: 0.90625 Loss: 0.006355626508593559\n",
      "Iteration: 2000 Training Accuracy: 0.875 Loss: 0.006738009862601757\n",
      "Iteration: 2010 Training Accuracy: 0.890625 Loss: 0.007949583232402802\n",
      "Iteration: 2020 Training Accuracy: 0.875 Loss: 0.005521806888282299\n",
      "Iteration: 2030 Training Accuracy: 0.90625 Loss: 0.006265274714678526\n",
      "Iteration: 2040 Training Accuracy: 0.859375 Loss: 0.004287418909370899\n",
      "Iteration: 2050 Training Accuracy: 0.859375 Loss: 0.0064688511192798615\n",
      "Iteration: 2060 Training Accuracy: 0.8125 Loss: 0.007934808731079102\n",
      "Iteration: 2070 Training Accuracy: 0.875 Loss: 0.0065796044655144215\n",
      "Iteration: 2080 Training Accuracy: 0.96875 Loss: 0.0020409715361893177\n",
      "Iteration: 2090 Training Accuracy: 0.875 Loss: 0.006447144318372011\n",
      "Iteration: 2100 Training Accuracy: 0.890625 Loss: 0.005091025028377771\n",
      "Iteration: 2110 Training Accuracy: 0.765625 Loss: 0.008602644316852093\n",
      "Iteration: 2120 Training Accuracy: 0.828125 Loss: 0.00875061471015215\n",
      "Iteration: 2130 Training Accuracy: 0.9375 Loss: 0.005248480010777712\n",
      "Iteration: 2140 Training Accuracy: 0.921875 Loss: 0.003974891267716885\n",
      "Iteration: 2150 Training Accuracy: 0.859375 Loss: 0.0072470903396606445\n",
      "Iteration: 2160 Training Accuracy: 0.796875 Loss: 0.008453807793557644\n",
      "Iteration: 2170 Training Accuracy: 0.8125 Loss: 0.008943617343902588\n",
      "Iteration: 2180 Training Accuracy: 0.859375 Loss: 0.00550220999866724\n",
      "Iteration: 2190 Training Accuracy: 0.90625 Loss: 0.007009946741163731\n",
      "Iteration: 2200 Training Accuracy: 0.828125 Loss: 0.010294556617736816\n",
      "Iteration: 2210 Training Accuracy: 0.859375 Loss: 0.005920904688537121\n",
      "Iteration: 2220 Training Accuracy: 0.859375 Loss: 0.0063835992477834225\n",
      "Iteration: 2230 Training Accuracy: 0.890625 Loss: 0.005654815584421158\n",
      "Iteration: 2240 Training Accuracy: 0.8125 Loss: 0.01110840868204832\n",
      "Iteration: 2250 Training Accuracy: 0.875 Loss: 0.006673279218375683\n",
      "Iteration: 2260 Training Accuracy: 0.78125 Loss: 0.011494796723127365\n",
      "Iteration: 2270 Training Accuracy: 0.875 Loss: 0.004793671891093254\n",
      "Iteration: 2280 Training Accuracy: 0.921875 Loss: 0.004921958316117525\n",
      "Iteration: 2290 Training Accuracy: 0.90625 Loss: 0.004126045852899551\n",
      "Iteration: 2300 Training Accuracy: 0.9375 Loss: 0.004614795092493296\n",
      "Iteration: 2310 Training Accuracy: 0.9375 Loss: 0.004196059424430132\n",
      "Iteration: 2320 Training Accuracy: 0.890625 Loss: 0.0077997236512601376\n",
      "Iteration: 2330 Training Accuracy: 0.890625 Loss: 0.00612744502723217\n",
      "Iteration: 2340 Training Accuracy: 0.890625 Loss: 0.005707656964659691\n",
      "Iteration: 2350 Training Accuracy: 0.875 Loss: 0.006393996067345142\n",
      "Iteration: 2360 Training Accuracy: 0.828125 Loss: 0.00928831659257412\n",
      "Iteration: 2370 Training Accuracy: 0.921875 Loss: 0.005713110323995352\n",
      "Iteration: 2380 Training Accuracy: 0.875 Loss: 0.006472667213529348\n",
      "Iteration: 2390 Training Accuracy: 0.828125 Loss: 0.008628986775875092\n",
      "Iteration: 2400 Training Accuracy: 0.875 Loss: 0.007410519756376743\n",
      "Iteration: 2410 Training Accuracy: 0.828125 Loss: 0.007310803048312664\n",
      "Iteration: 2420 Training Accuracy: 0.921875 Loss: 0.005049362778663635\n",
      "Iteration: 2430 Training Accuracy: 0.78125 Loss: 0.00836394913494587\n",
      "Iteration: 2440 Training Accuracy: 0.921875 Loss: 0.004583679139614105\n",
      "Iteration: 2450 Training Accuracy: 0.859375 Loss: 0.005293903406709433\n",
      "Iteration: 2460 Training Accuracy: 0.953125 Loss: 0.0020400099456310272\n",
      "Iteration: 2470 Training Accuracy: 0.859375 Loss: 0.0063667395152151585\n",
      "Iteration: 2480 Training Accuracy: 0.84375 Loss: 0.006851742975413799\n",
      "Iteration: 2490 Training Accuracy: 0.875 Loss: 0.0053208572790026665\n",
      "Iteration: 2500 Training Accuracy: 0.796875 Loss: 0.008484307676553726\n",
      "Iteration: 2510 Training Accuracy: 0.90625 Loss: 0.005160626024007797\n",
      "Iteration: 2520 Training Accuracy: 0.875 Loss: 0.005527350585907698\n",
      "Iteration: 2530 Training Accuracy: 0.90625 Loss: 0.005660020746290684\n",
      "Iteration: 2540 Training Accuracy: 0.90625 Loss: 0.004383050836622715\n",
      "Iteration: 2550 Training Accuracy: 0.8125 Loss: 0.008792897686362267\n",
      "Iteration: 2560 Training Accuracy: 0.921875 Loss: 0.004357186146080494\n",
      "Iteration: 2570 Training Accuracy: 0.875 Loss: 0.005889963358640671\n",
      "Iteration: 2580 Training Accuracy: 0.828125 Loss: 0.007725344505161047\n",
      "Iteration: 2590 Training Accuracy: 0.90625 Loss: 0.00611039437353611\n",
      "Iteration: 2600 Training Accuracy: 0.859375 Loss: 0.008086834102869034\n",
      "Iteration: 2610 Training Accuracy: 0.875 Loss: 0.0050466228276491165\n",
      "Iteration: 2620 Training Accuracy: 0.875 Loss: 0.006705073639750481\n",
      "Iteration: 2630 Training Accuracy: 0.875 Loss: 0.005699235945940018\n",
      "Iteration: 2640 Training Accuracy: 0.875 Loss: 0.00827135518193245\n",
      "Iteration: 2650 Training Accuracy: 0.890625 Loss: 0.00683834170922637\n",
      "Iteration: 2660 Training Accuracy: 0.796875 Loss: 0.008779154159128666\n",
      "Iteration: 2670 Training Accuracy: 0.953125 Loss: 0.003946704789996147\n",
      "Iteration: 2680 Training Accuracy: 0.84375 Loss: 0.006197095382958651\n",
      "Iteration: 2690 Training Accuracy: 0.859375 Loss: 0.008696925826370716\n",
      "Iteration: 2700 Training Accuracy: 0.859375 Loss: 0.0065975431352853775\n",
      "Iteration: 2710 Training Accuracy: 0.859375 Loss: 0.005492428783327341\n",
      "Iteration: 2720 Training Accuracy: 0.90625 Loss: 0.005335005931556225\n",
      "Iteration: 2730 Training Accuracy: 0.84375 Loss: 0.007167843170464039\n",
      "Iteration: 2740 Training Accuracy: 0.875 Loss: 0.005461295135319233\n",
      "Iteration: 2750 Training Accuracy: 0.84375 Loss: 0.0075790658593177795\n",
      "Iteration: 2760 Training Accuracy: 0.859375 Loss: 0.007109522353857756\n",
      "Iteration: 2770 Training Accuracy: 0.78125 Loss: 0.009320366196334362\n",
      "Iteration: 2780 Training Accuracy: 0.921875 Loss: 0.0031673165503889322\n",
      "Iteration: 2790 Training Accuracy: 0.859375 Loss: 0.0065877302549779415\n",
      "Iteration: 2800 Training Accuracy: 0.84375 Loss: 0.007255060598254204\n",
      "Iteration: 2810 Training Accuracy: 0.953125 Loss: 0.003009395208209753\n",
      "Training Accuracy = 0.875\n",
      "Validation Accuracy = 0.8516666666666667\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 0.875 Loss: 0.004275237210094929\n",
      "Iteration: 2830 Training Accuracy: 0.921875 Loss: 0.004268689546734095\n",
      "Iteration: 2840 Training Accuracy: 0.921875 Loss: 0.002536914311349392\n",
      "Iteration: 2850 Training Accuracy: 0.84375 Loss: 0.006877914536744356\n",
      "Iteration: 2860 Training Accuracy: 0.90625 Loss: 0.0037438501603901386\n",
      "Iteration: 2870 Training Accuracy: 0.9375 Loss: 0.0038685319013893604\n",
      "Iteration: 2880 Training Accuracy: 0.84375 Loss: 0.009296857751905918\n",
      "Iteration: 2890 Training Accuracy: 0.875 Loss: 0.005652384366840124\n",
      "Iteration: 2900 Training Accuracy: 0.921875 Loss: 0.00431409664452076\n",
      "Iteration: 2910 Training Accuracy: 0.90625 Loss: 0.0054763369262218475\n",
      "Iteration: 2920 Training Accuracy: 0.84375 Loss: 0.005864311475306749\n",
      "Iteration: 2930 Training Accuracy: 0.90625 Loss: 0.00586685910820961\n",
      "Iteration: 2940 Training Accuracy: 0.921875 Loss: 0.004687137436121702\n",
      "Iteration: 2950 Training Accuracy: 0.921875 Loss: 0.0034532565623521805\n",
      "Iteration: 2960 Training Accuracy: 0.84375 Loss: 0.005842111073434353\n",
      "Iteration: 2970 Training Accuracy: 0.890625 Loss: 0.005136548541486263\n",
      "Iteration: 2980 Training Accuracy: 0.890625 Loss: 0.0042234910652041435\n",
      "Iteration: 2990 Training Accuracy: 0.9375 Loss: 0.004227540921419859\n",
      "Iteration: 3000 Training Accuracy: 0.953125 Loss: 0.0031083044596016407\n",
      "Iteration: 3010 Training Accuracy: 0.953125 Loss: 0.002576140919700265\n",
      "Iteration: 3020 Training Accuracy: 0.890625 Loss: 0.00612895842641592\n",
      "Iteration: 3030 Training Accuracy: 0.9375 Loss: 0.006535063963383436\n",
      "Iteration: 3040 Training Accuracy: 0.90625 Loss: 0.005629413761198521\n",
      "Iteration: 3050 Training Accuracy: 0.875 Loss: 0.0060740187764167786\n",
      "Iteration: 3060 Training Accuracy: 0.859375 Loss: 0.005229164846241474\n",
      "Iteration: 3070 Training Accuracy: 0.921875 Loss: 0.004392941482365131\n",
      "Iteration: 3080 Training Accuracy: 0.90625 Loss: 0.0035869739949703217\n",
      "Iteration: 3090 Training Accuracy: 0.859375 Loss: 0.006694054696708918\n",
      "Iteration: 3100 Training Accuracy: 0.953125 Loss: 0.003518611192703247\n",
      "Iteration: 3110 Training Accuracy: 0.859375 Loss: 0.006538877729326487\n",
      "Iteration: 3120 Training Accuracy: 0.96875 Loss: 0.0027426877059042454\n",
      "Iteration: 3130 Training Accuracy: 0.90625 Loss: 0.005775094497948885\n",
      "Iteration: 3140 Training Accuracy: 0.9375 Loss: 0.002775521483272314\n",
      "Iteration: 3150 Training Accuracy: 0.84375 Loss: 0.007123650051653385\n",
      "Iteration: 3160 Training Accuracy: 0.953125 Loss: 0.0031500300392508507\n",
      "Iteration: 3170 Training Accuracy: 0.921875 Loss: 0.00395972840487957\n",
      "Iteration: 3180 Training Accuracy: 0.953125 Loss: 0.003301170188933611\n",
      "Iteration: 3190 Training Accuracy: 0.890625 Loss: 0.004642459098249674\n",
      "Iteration: 3200 Training Accuracy: 0.859375 Loss: 0.007730206474661827\n",
      "Iteration: 3210 Training Accuracy: 0.953125 Loss: 0.0036408237647265196\n",
      "Iteration: 3220 Training Accuracy: 0.96875 Loss: 0.002061425242573023\n",
      "Iteration: 3230 Training Accuracy: 0.828125 Loss: 0.01014823280274868\n",
      "Iteration: 3240 Training Accuracy: 0.890625 Loss: 0.004638194106519222\n",
      "Iteration: 3250 Training Accuracy: 0.921875 Loss: 0.00468235881999135\n",
      "Iteration: 3260 Training Accuracy: 0.921875 Loss: 0.00498597789555788\n",
      "Iteration: 3270 Training Accuracy: 0.875 Loss: 0.0053896186873316765\n",
      "Iteration: 3280 Training Accuracy: 0.84375 Loss: 0.007035032380372286\n",
      "Iteration: 3290 Training Accuracy: 0.921875 Loss: 0.005411596968770027\n",
      "Iteration: 3300 Training Accuracy: 0.890625 Loss: 0.004101437516510487\n",
      "Iteration: 3310 Training Accuracy: 0.828125 Loss: 0.005763866938650608\n",
      "Iteration: 3320 Training Accuracy: 0.890625 Loss: 0.005634959787130356\n",
      "Iteration: 3330 Training Accuracy: 0.890625 Loss: 0.005646444857120514\n",
      "Iteration: 3340 Training Accuracy: 0.90625 Loss: 0.005646837409585714\n",
      "Iteration: 3350 Training Accuracy: 0.9375 Loss: 0.005239227320998907\n",
      "Iteration: 3360 Training Accuracy: 0.90625 Loss: 0.004795938264578581\n",
      "Iteration: 3370 Training Accuracy: 0.953125 Loss: 0.0017064246349036694\n",
      "Iteration: 3380 Training Accuracy: 0.953125 Loss: 0.0026640775613486767\n",
      "Iteration: 3390 Training Accuracy: 0.875 Loss: 0.004536551423370838\n",
      "Iteration: 3400 Training Accuracy: 0.890625 Loss: 0.005107021890580654\n",
      "Iteration: 3410 Training Accuracy: 0.921875 Loss: 0.004377049393951893\n",
      "Iteration: 3420 Training Accuracy: 0.921875 Loss: 0.003917424939572811\n",
      "Iteration: 3430 Training Accuracy: 0.90625 Loss: 0.003582532284781337\n",
      "Iteration: 3440 Training Accuracy: 0.890625 Loss: 0.0034603034146130085\n",
      "Iteration: 3450 Training Accuracy: 0.921875 Loss: 0.0030385563150048256\n",
      "Iteration: 3460 Training Accuracy: 0.8125 Loss: 0.008936363272368908\n",
      "Iteration: 3470 Training Accuracy: 0.859375 Loss: 0.00620413850992918\n",
      "Iteration: 3480 Training Accuracy: 0.953125 Loss: 0.002608277602121234\n",
      "Iteration: 3490 Training Accuracy: 0.921875 Loss: 0.003947876393795013\n",
      "Iteration: 3500 Training Accuracy: 0.859375 Loss: 0.006630542688071728\n",
      "Iteration: 3510 Training Accuracy: 0.90625 Loss: 0.0044183917343616486\n",
      "Iteration: 3520 Training Accuracy: 0.9375 Loss: 0.0026506767608225346\n",
      "Iteration: 3530 Training Accuracy: 0.890625 Loss: 0.005374017171561718\n",
      "Iteration: 3540 Training Accuracy: 0.90625 Loss: 0.003178797895088792\n",
      "Iteration: 3550 Training Accuracy: 0.8125 Loss: 0.008576923049986362\n",
      "Iteration: 3560 Training Accuracy: 0.875 Loss: 0.0048902444541454315\n",
      "Iteration: 3570 Training Accuracy: 0.890625 Loss: 0.004309677518904209\n",
      "Iteration: 3580 Training Accuracy: 0.921875 Loss: 0.0038669328205287457\n",
      "Iteration: 3590 Training Accuracy: 0.890625 Loss: 0.0042463643476367\n",
      "Iteration: 3600 Training Accuracy: 0.90625 Loss: 0.0038446071557700634\n",
      "Iteration: 3610 Training Accuracy: 0.875 Loss: 0.004145875573158264\n",
      "Iteration: 3620 Training Accuracy: 0.984375 Loss: 0.001939008361659944\n",
      "Iteration: 3630 Training Accuracy: 0.90625 Loss: 0.005625466350466013\n",
      "Iteration: 3640 Training Accuracy: 0.921875 Loss: 0.004753825720399618\n",
      "Iteration: 3650 Training Accuracy: 0.84375 Loss: 0.00461675226688385\n",
      "Iteration: 3660 Training Accuracy: 0.921875 Loss: 0.004569721408188343\n",
      "Iteration: 3670 Training Accuracy: 0.828125 Loss: 0.008939656428992748\n",
      "Iteration: 3680 Training Accuracy: 0.90625 Loss: 0.0042215571738779545\n",
      "Iteration: 3690 Training Accuracy: 0.890625 Loss: 0.003960100933909416\n",
      "Iteration: 3700 Training Accuracy: 0.828125 Loss: 0.006407636217772961\n",
      "Iteration: 3710 Training Accuracy: 0.890625 Loss: 0.003783784806728363\n",
      "Iteration: 3720 Training Accuracy: 0.875 Loss: 0.004565421026200056\n",
      "Iteration: 3730 Training Accuracy: 0.90625 Loss: 0.004210942890495062\n",
      "Iteration: 3740 Training Accuracy: 0.90625 Loss: 0.00346481055021286\n",
      "Iteration: 3750 Training Accuracy: 0.90625 Loss: 0.0032713450491428375\n",
      "Training Accuracy = 0.96875\n",
      "Validation Accuracy = 0.8688333333333333\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 0.9375 Loss: 0.0037097155582159758\n",
      "Iteration: 3770 Training Accuracy: 0.953125 Loss: 0.00346343289129436\n",
      "Iteration: 3780 Training Accuracy: 0.90625 Loss: 0.004454059526324272\n",
      "Iteration: 3790 Training Accuracy: 0.890625 Loss: 0.003896404290571809\n",
      "Iteration: 3800 Training Accuracy: 0.859375 Loss: 0.004974283277988434\n",
      "Iteration: 3810 Training Accuracy: 0.953125 Loss: 0.0030465731397271156\n",
      "Iteration: 3820 Training Accuracy: 0.921875 Loss: 0.005096332170069218\n",
      "Iteration: 3830 Training Accuracy: 0.921875 Loss: 0.0047083692625164986\n",
      "Iteration: 3840 Training Accuracy: 0.875 Loss: 0.006189448293298483\n",
      "Iteration: 3850 Training Accuracy: 0.921875 Loss: 0.002871470060199499\n",
      "Iteration: 3860 Training Accuracy: 0.921875 Loss: 0.0039051049388945103\n",
      "Iteration: 3870 Training Accuracy: 0.859375 Loss: 0.006560725159943104\n",
      "Iteration: 3880 Training Accuracy: 0.9375 Loss: 0.0027958485297858715\n",
      "Iteration: 3890 Training Accuracy: 0.859375 Loss: 0.006864837370812893\n",
      "Iteration: 3900 Training Accuracy: 0.890625 Loss: 0.005708608776330948\n",
      "Iteration: 3910 Training Accuracy: 0.9375 Loss: 0.0031240698881447315\n",
      "Iteration: 3920 Training Accuracy: 0.96875 Loss: 0.002626645378768444\n",
      "Iteration: 3930 Training Accuracy: 0.921875 Loss: 0.003979393281042576\n",
      "Iteration: 3940 Training Accuracy: 0.9375 Loss: 0.003581495024263859\n",
      "Iteration: 3950 Training Accuracy: 0.9375 Loss: 0.003710741875693202\n",
      "Iteration: 3960 Training Accuracy: 0.953125 Loss: 0.0017348143737763166\n",
      "Iteration: 3970 Training Accuracy: 0.953125 Loss: 0.0030067777261137962\n",
      "Iteration: 3980 Training Accuracy: 0.890625 Loss: 0.0055809929035604\n",
      "Iteration: 3990 Training Accuracy: 0.890625 Loss: 0.004938474856317043\n",
      "Iteration: 4000 Training Accuracy: 0.875 Loss: 0.0039346907287836075\n",
      "Iteration: 4010 Training Accuracy: 0.90625 Loss: 0.004144563339650631\n",
      "Iteration: 4020 Training Accuracy: 0.90625 Loss: 0.0048217447474598885\n",
      "Iteration: 4030 Training Accuracy: 0.9375 Loss: 0.004471547435969114\n",
      "Iteration: 4040 Training Accuracy: 0.875 Loss: 0.004423857666552067\n",
      "Iteration: 4050 Training Accuracy: 0.90625 Loss: 0.004758098628371954\n",
      "Iteration: 4060 Training Accuracy: 0.890625 Loss: 0.004006571602076292\n",
      "Iteration: 4070 Training Accuracy: 0.84375 Loss: 0.007107099052518606\n",
      "Iteration: 4080 Training Accuracy: 0.9375 Loss: 0.003751937299966812\n",
      "Iteration: 4090 Training Accuracy: 0.96875 Loss: 0.001231210888363421\n",
      "Iteration: 4100 Training Accuracy: 0.953125 Loss: 0.003431218210607767\n",
      "Iteration: 4110 Training Accuracy: 0.9375 Loss: 0.003088491503149271\n",
      "Iteration: 4120 Training Accuracy: 0.9375 Loss: 0.0042360443621873856\n",
      "Iteration: 4130 Training Accuracy: 0.890625 Loss: 0.004292638041079044\n",
      "Iteration: 4140 Training Accuracy: 0.890625 Loss: 0.00585121288895607\n",
      "Iteration: 4150 Training Accuracy: 0.875 Loss: 0.0052892351523041725\n",
      "Iteration: 4160 Training Accuracy: 0.90625 Loss: 0.0033912272192537785\n",
      "Iteration: 4170 Training Accuracy: 0.953125 Loss: 0.0018986749928444624\n",
      "Iteration: 4180 Training Accuracy: 0.890625 Loss: 0.0057580741122365\n",
      "Iteration: 4190 Training Accuracy: 0.90625 Loss: 0.002710639499127865\n",
      "Iteration: 4200 Training Accuracy: 0.859375 Loss: 0.00504255760461092\n",
      "Iteration: 4210 Training Accuracy: 0.984375 Loss: 0.0019009171519428492\n",
      "Iteration: 4220 Training Accuracy: 0.9375 Loss: 0.00367582100443542\n",
      "Iteration: 4230 Training Accuracy: 0.921875 Loss: 0.0039060558192431927\n",
      "Iteration: 4240 Training Accuracy: 0.90625 Loss: 0.004389306530356407\n",
      "Iteration: 4250 Training Accuracy: 0.9375 Loss: 0.0025468082167208195\n",
      "Iteration: 4260 Training Accuracy: 0.9375 Loss: 0.0031862077303230762\n",
      "Iteration: 4270 Training Accuracy: 0.890625 Loss: 0.005514638498425484\n",
      "Iteration: 4280 Training Accuracy: 0.953125 Loss: 0.002811725251376629\n",
      "Iteration: 4290 Training Accuracy: 0.90625 Loss: 0.0042546032927930355\n",
      "Iteration: 4300 Training Accuracy: 0.9375 Loss: 0.0037555708549916744\n",
      "Iteration: 4310 Training Accuracy: 0.90625 Loss: 0.004978388547897339\n",
      "Iteration: 4320 Training Accuracy: 0.9375 Loss: 0.0027206107042729855\n",
      "Iteration: 4330 Training Accuracy: 0.9375 Loss: 0.002994057023897767\n",
      "Iteration: 4340 Training Accuracy: 0.9375 Loss: 0.003924489952623844\n",
      "Iteration: 4350 Training Accuracy: 0.96875 Loss: 0.002208003308624029\n",
      "Iteration: 4360 Training Accuracy: 0.953125 Loss: 0.0031129447743296623\n",
      "Iteration: 4370 Training Accuracy: 0.953125 Loss: 0.0026489722076803446\n",
      "Iteration: 4380 Training Accuracy: 0.90625 Loss: 0.003800738137215376\n",
      "Iteration: 4390 Training Accuracy: 0.890625 Loss: 0.0051201642490923405\n",
      "Iteration: 4400 Training Accuracy: 0.953125 Loss: 0.0024393752682954073\n",
      "Iteration: 4410 Training Accuracy: 0.890625 Loss: 0.0044554672203958035\n",
      "Iteration: 4420 Training Accuracy: 0.796875 Loss: 0.00870687235146761\n",
      "Iteration: 4430 Training Accuracy: 0.984375 Loss: 0.0016566049307584763\n",
      "Iteration: 4440 Training Accuracy: 0.921875 Loss: 0.003039219882339239\n",
      "Iteration: 4450 Training Accuracy: 0.953125 Loss: 0.002744147554039955\n",
      "Iteration: 4460 Training Accuracy: 0.953125 Loss: 0.0026548251044005156\n",
      "Iteration: 4470 Training Accuracy: 0.859375 Loss: 0.004093336872756481\n",
      "Iteration: 4480 Training Accuracy: 0.9375 Loss: 0.002963020233437419\n",
      "Iteration: 4490 Training Accuracy: 0.921875 Loss: 0.0035219560377299786\n",
      "Iteration: 4500 Training Accuracy: 0.890625 Loss: 0.0036824187263846397\n",
      "Iteration: 4510 Training Accuracy: 0.890625 Loss: 0.004233572166413069\n",
      "Iteration: 4520 Training Accuracy: 0.9375 Loss: 0.0033140499144792557\n",
      "Iteration: 4530 Training Accuracy: 0.90625 Loss: 0.003565303049981594\n",
      "Iteration: 4540 Training Accuracy: 0.90625 Loss: 0.004044333007186651\n",
      "Iteration: 4550 Training Accuracy: 0.921875 Loss: 0.002978050149977207\n",
      "Iteration: 4560 Training Accuracy: 0.859375 Loss: 0.007386069744825363\n",
      "Iteration: 4570 Training Accuracy: 0.84375 Loss: 0.0062715234234929085\n",
      "Iteration: 4580 Training Accuracy: 0.96875 Loss: 0.001304036471992731\n",
      "Iteration: 4590 Training Accuracy: 0.953125 Loss: 0.002246250631287694\n",
      "Iteration: 4600 Training Accuracy: 0.953125 Loss: 0.0022492767311632633\n",
      "Iteration: 4610 Training Accuracy: 0.875 Loss: 0.0065391487441957\n",
      "Iteration: 4620 Training Accuracy: 0.859375 Loss: 0.003628723556175828\n",
      "Iteration: 4630 Training Accuracy: 0.921875 Loss: 0.003411530517041683\n",
      "Iteration: 4640 Training Accuracy: 0.90625 Loss: 0.005255325697362423\n",
      "Iteration: 4650 Training Accuracy: 0.96875 Loss: 0.0015280279330909252\n",
      "Iteration: 4660 Training Accuracy: 0.90625 Loss: 0.004105168394744396\n",
      "Iteration: 4670 Training Accuracy: 0.875 Loss: 0.004708048887550831\n",
      "Iteration: 4680 Training Accuracy: 0.90625 Loss: 0.004212839063256979\n",
      "Iteration: 4690 Training Accuracy: 0.9375 Loss: 0.0029858569614589214\n",
      "Training Accuracy = 0.9375\n",
      "Validation Accuracy = 0.8696666666666667\n",
      "epoch: 5\n",
      "Iteration: 4700 Training Accuracy: 0.9375 Loss: 0.0030380249954760075\n",
      "Iteration: 4710 Training Accuracy: 0.9375 Loss: 0.003088529221713543\n",
      "Iteration: 4720 Training Accuracy: 0.921875 Loss: 0.0038488793652504683\n",
      "Iteration: 4730 Training Accuracy: 0.921875 Loss: 0.004218770191073418\n",
      "Iteration: 4740 Training Accuracy: 0.890625 Loss: 0.006268349476158619\n",
      "Iteration: 4750 Training Accuracy: 0.921875 Loss: 0.002772230189293623\n",
      "Iteration: 4760 Training Accuracy: 0.921875 Loss: 0.004377852659672499\n",
      "Iteration: 4770 Training Accuracy: 0.890625 Loss: 0.004854490980505943\n",
      "Iteration: 4780 Training Accuracy: 0.921875 Loss: 0.0035062693059444427\n",
      "Iteration: 4790 Training Accuracy: 0.9375 Loss: 0.003300462616607547\n",
      "Iteration: 4800 Training Accuracy: 0.96875 Loss: 0.0026235065888613462\n",
      "Iteration: 4810 Training Accuracy: 0.96875 Loss: 0.0013512643054127693\n",
      "Iteration: 4820 Training Accuracy: 0.875 Loss: 0.00656625721603632\n",
      "Iteration: 4830 Training Accuracy: 0.84375 Loss: 0.007216393947601318\n",
      "Iteration: 4840 Training Accuracy: 0.9375 Loss: 0.0038077402859926224\n",
      "Iteration: 4850 Training Accuracy: 0.9375 Loss: 0.0023661889135837555\n",
      "Iteration: 4860 Training Accuracy: 0.875 Loss: 0.0053063491359353065\n",
      "Iteration: 4870 Training Accuracy: 0.953125 Loss: 0.0036841053515672684\n",
      "Iteration: 4880 Training Accuracy: 0.9375 Loss: 0.002565902192145586\n",
      "Iteration: 4890 Training Accuracy: 0.9375 Loss: 0.0030553461983799934\n",
      "Iteration: 4900 Training Accuracy: 0.953125 Loss: 0.0020056222565472126\n",
      "Iteration: 4910 Training Accuracy: 0.9375 Loss: 0.0030502581503242254\n",
      "Iteration: 4920 Training Accuracy: 0.890625 Loss: 0.0049952236004173756\n",
      "Iteration: 4930 Training Accuracy: 0.890625 Loss: 0.004768692422658205\n",
      "Iteration: 4940 Training Accuracy: 0.921875 Loss: 0.0028615579940378666\n",
      "Iteration: 4950 Training Accuracy: 0.9375 Loss: 0.002567487768828869\n",
      "Iteration: 4960 Training Accuracy: 0.953125 Loss: 0.002422467339783907\n",
      "Iteration: 4970 Training Accuracy: 0.890625 Loss: 0.0043046181090176105\n",
      "Iteration: 4980 Training Accuracy: 0.9375 Loss: 0.0030135042034089565\n",
      "Iteration: 4990 Training Accuracy: 0.875 Loss: 0.0037320186384022236\n",
      "Iteration: 5000 Training Accuracy: 0.84375 Loss: 0.008209263905882835\n",
      "Iteration: 5010 Training Accuracy: 0.9375 Loss: 0.0030491235665977\n",
      "Iteration: 5020 Training Accuracy: 0.953125 Loss: 0.002488140482455492\n",
      "Iteration: 5030 Training Accuracy: 0.9375 Loss: 0.0022995404433459044\n",
      "Iteration: 5040 Training Accuracy: 0.96875 Loss: 0.0018875255482271314\n",
      "Iteration: 5050 Training Accuracy: 0.921875 Loss: 0.0032360530458390713\n",
      "Iteration: 5060 Training Accuracy: 0.859375 Loss: 0.004741712473332882\n",
      "Iteration: 5070 Training Accuracy: 0.890625 Loss: 0.005846560001373291\n",
      "Iteration: 5080 Training Accuracy: 0.859375 Loss: 0.00542324548587203\n",
      "Iteration: 5090 Training Accuracy: 0.90625 Loss: 0.003610718296840787\n",
      "Iteration: 5100 Training Accuracy: 0.96875 Loss: 0.0030388347804546356\n",
      "Iteration: 5110 Training Accuracy: 0.9375 Loss: 0.0024290187284350395\n",
      "Iteration: 5120 Training Accuracy: 0.90625 Loss: 0.0032859083730727434\n",
      "Iteration: 5130 Training Accuracy: 0.921875 Loss: 0.0030401546973735094\n",
      "Iteration: 5140 Training Accuracy: 0.921875 Loss: 0.0037863317411392927\n",
      "Iteration: 5150 Training Accuracy: 0.96875 Loss: 0.0029365597292780876\n",
      "Iteration: 5160 Training Accuracy: 0.90625 Loss: 0.003710984718054533\n",
      "Iteration: 5170 Training Accuracy: 0.90625 Loss: 0.0040925294160842896\n",
      "Iteration: 5180 Training Accuracy: 0.9375 Loss: 0.004198373761028051\n",
      "Iteration: 5190 Training Accuracy: 0.890625 Loss: 0.0039952644146978855\n",
      "Iteration: 5200 Training Accuracy: 0.953125 Loss: 0.0020400816574692726\n",
      "Iteration: 5210 Training Accuracy: 0.96875 Loss: 0.0008448213920928538\n",
      "Iteration: 5220 Training Accuracy: 0.984375 Loss: 0.0009140826296061277\n",
      "Iteration: 5230 Training Accuracy: 0.96875 Loss: 0.0012935625854879618\n",
      "Iteration: 5240 Training Accuracy: 0.921875 Loss: 0.003937628585845232\n",
      "Iteration: 5250 Training Accuracy: 0.875 Loss: 0.003607393940910697\n",
      "Iteration: 5260 Training Accuracy: 0.875 Loss: 0.00540231354534626\n",
      "Iteration: 5270 Training Accuracy: 0.921875 Loss: 0.004083410836756229\n",
      "Iteration: 5280 Training Accuracy: 0.953125 Loss: 0.002547911833971739\n",
      "Iteration: 5290 Training Accuracy: 0.90625 Loss: 0.004986526444554329\n",
      "Iteration: 5300 Training Accuracy: 0.984375 Loss: 0.0008406653651036322\n",
      "Iteration: 5310 Training Accuracy: 0.921875 Loss: 0.0047853742726147175\n",
      "Iteration: 5320 Training Accuracy: 0.96875 Loss: 0.0027229581028223038\n",
      "Iteration: 5330 Training Accuracy: 0.875 Loss: 0.006310535594820976\n",
      "Iteration: 5340 Training Accuracy: 0.9375 Loss: 0.0027972243260592222\n",
      "Iteration: 5350 Training Accuracy: 0.890625 Loss: 0.004348508547991514\n",
      "Iteration: 5360 Training Accuracy: 0.953125 Loss: 0.002355807228013873\n",
      "Iteration: 5370 Training Accuracy: 0.90625 Loss: 0.003848356194794178\n",
      "Iteration: 5380 Training Accuracy: 0.921875 Loss: 0.002810861449688673\n",
      "Iteration: 5390 Training Accuracy: 0.90625 Loss: 0.0030380841344594955\n",
      "Iteration: 5400 Training Accuracy: 0.984375 Loss: 0.0012701849918812513\n",
      "Iteration: 5410 Training Accuracy: 0.9375 Loss: 0.0029504182748496532\n",
      "Iteration: 5420 Training Accuracy: 0.875 Loss: 0.0049246130511164665\n",
      "Iteration: 5430 Training Accuracy: 0.96875 Loss: 0.003064820310100913\n",
      "Iteration: 5440 Training Accuracy: 0.859375 Loss: 0.005893838591873646\n",
      "Iteration: 5450 Training Accuracy: 0.96875 Loss: 0.00249112443998456\n",
      "Iteration: 5460 Training Accuracy: 0.890625 Loss: 0.003945091739296913\n",
      "Iteration: 5470 Training Accuracy: 0.9375 Loss: 0.003651278791949153\n",
      "Iteration: 5480 Training Accuracy: 0.921875 Loss: 0.003600291209295392\n",
      "Iteration: 5490 Training Accuracy: 0.8125 Loss: 0.009819983504712582\n",
      "Iteration: 5500 Training Accuracy: 0.875 Loss: 0.005730193108320236\n",
      "Iteration: 5510 Training Accuracy: 0.90625 Loss: 0.0033743474632501602\n",
      "Iteration: 5520 Training Accuracy: 0.90625 Loss: 0.005820167250931263\n",
      "Iteration: 5530 Training Accuracy: 0.96875 Loss: 0.002895771525800228\n",
      "Iteration: 5540 Training Accuracy: 0.9375 Loss: 0.0037780888378620148\n",
      "Iteration: 5550 Training Accuracy: 1.0 Loss: 0.000410853186622262\n",
      "Iteration: 5560 Training Accuracy: 1.0 Loss: 0.0007612694753333926\n",
      "Iteration: 5570 Training Accuracy: 0.953125 Loss: 0.001447569695301354\n",
      "Iteration: 5580 Training Accuracy: 0.921875 Loss: 0.002521403366699815\n",
      "Iteration: 5590 Training Accuracy: 0.9375 Loss: 0.0024940073490142822\n",
      "Iteration: 5600 Training Accuracy: 0.953125 Loss: 0.0029774473514407873\n",
      "Iteration: 5610 Training Accuracy: 0.96875 Loss: 0.0020619661081582308\n",
      "Iteration: 5620 Training Accuracy: 0.9375 Loss: 0.0027879877015948296\n",
      "Training Accuracy = 0.96875\n",
      "Validation Accuracy = 0.8745\n",
      "epoch: 6\n",
      "Iteration: 5630 Training Accuracy: 0.890625 Loss: 0.005325313191860914\n",
      "Iteration: 5640 Training Accuracy: 0.9375 Loss: 0.0028932145796716213\n",
      "Iteration: 5650 Training Accuracy: 0.890625 Loss: 0.004208813421428204\n",
      "Iteration: 5660 Training Accuracy: 0.9375 Loss: 0.0017453267937526107\n",
      "Iteration: 5670 Training Accuracy: 0.9375 Loss: 0.0026300959289073944\n",
      "Iteration: 5680 Training Accuracy: 0.96875 Loss: 0.0020536831580102444\n",
      "Iteration: 5690 Training Accuracy: 0.953125 Loss: 0.0015370636247098446\n",
      "Iteration: 5700 Training Accuracy: 0.953125 Loss: 0.0021543200127780437\n",
      "Iteration: 5710 Training Accuracy: 0.984375 Loss: 0.0015962619800120592\n",
      "Iteration: 5720 Training Accuracy: 0.953125 Loss: 0.0018263633828610182\n",
      "Iteration: 5730 Training Accuracy: 0.890625 Loss: 0.004151186905801296\n",
      "Iteration: 5740 Training Accuracy: 0.96875 Loss: 0.0020048932638019323\n",
      "Iteration: 5750 Training Accuracy: 0.96875 Loss: 0.0014584078453481197\n",
      "Iteration: 5760 Training Accuracy: 0.921875 Loss: 0.004579444415867329\n",
      "Iteration: 5770 Training Accuracy: 0.953125 Loss: 0.0026248442009091377\n",
      "Iteration: 5780 Training Accuracy: 0.90625 Loss: 0.004419500939548016\n",
      "Iteration: 5790 Training Accuracy: 0.890625 Loss: 0.0034125768579542637\n",
      "Iteration: 5800 Training Accuracy: 0.96875 Loss: 0.0015698379138484597\n",
      "Iteration: 5810 Training Accuracy: 0.953125 Loss: 0.002352789044380188\n",
      "Iteration: 5820 Training Accuracy: 0.984375 Loss: 0.0011811001459136605\n",
      "Iteration: 5830 Training Accuracy: 0.953125 Loss: 0.001477250480093062\n",
      "Iteration: 5840 Training Accuracy: 1.0 Loss: 0.0004561375826597214\n",
      "Iteration: 5850 Training Accuracy: 0.921875 Loss: 0.004188230261206627\n",
      "Iteration: 5860 Training Accuracy: 0.9375 Loss: 0.003674020292237401\n",
      "Iteration: 5870 Training Accuracy: 0.921875 Loss: 0.0029309375677257776\n",
      "Iteration: 5880 Training Accuracy: 0.953125 Loss: 0.0028010113164782524\n",
      "Iteration: 5890 Training Accuracy: 0.921875 Loss: 0.0031842077150940895\n",
      "Iteration: 5900 Training Accuracy: 0.921875 Loss: 0.005217987112700939\n",
      "Iteration: 5910 Training Accuracy: 0.90625 Loss: 0.00482364185154438\n",
      "Iteration: 5920 Training Accuracy: 0.96875 Loss: 0.0019232176709920168\n",
      "Iteration: 5930 Training Accuracy: 0.921875 Loss: 0.0035137413069605827\n",
      "Iteration: 5940 Training Accuracy: 0.90625 Loss: 0.004575285129249096\n",
      "Iteration: 5950 Training Accuracy: 0.96875 Loss: 0.0022366144694387913\n",
      "Iteration: 5960 Training Accuracy: 0.984375 Loss: 0.0007342186290770769\n",
      "Iteration: 5970 Training Accuracy: 0.984375 Loss: 0.002171674044802785\n",
      "Iteration: 5980 Training Accuracy: 0.953125 Loss: 0.0030237052123993635\n",
      "Iteration: 5990 Training Accuracy: 0.953125 Loss: 0.002476640045642853\n",
      "Iteration: 6000 Training Accuracy: 0.921875 Loss: 0.0033974500838667154\n",
      "Iteration: 6010 Training Accuracy: 0.90625 Loss: 0.0030894773080945015\n",
      "Iteration: 6020 Training Accuracy: 0.90625 Loss: 0.00444252323359251\n",
      "Iteration: 6030 Training Accuracy: 0.96875 Loss: 0.0015658076154068112\n",
      "Iteration: 6040 Training Accuracy: 0.875 Loss: 0.005117645021528006\n",
      "Iteration: 6050 Training Accuracy: 0.90625 Loss: 0.005277735181152821\n",
      "Iteration: 6060 Training Accuracy: 0.890625 Loss: 0.004336914978921413\n",
      "Iteration: 6070 Training Accuracy: 0.9375 Loss: 0.003360962960869074\n",
      "Iteration: 6080 Training Accuracy: 0.875 Loss: 0.005773548968136311\n",
      "Iteration: 6090 Training Accuracy: 0.9375 Loss: 0.0019153500907123089\n",
      "Iteration: 6100 Training Accuracy: 1.0 Loss: 0.0006907840725034475\n",
      "Iteration: 6110 Training Accuracy: 0.96875 Loss: 0.0012597038876265287\n",
      "Iteration: 6120 Training Accuracy: 0.921875 Loss: 0.0031634231563657522\n",
      "Iteration: 6130 Training Accuracy: 0.96875 Loss: 0.002224498661234975\n",
      "Iteration: 6140 Training Accuracy: 0.96875 Loss: 0.00363358692266047\n",
      "Iteration: 6150 Training Accuracy: 0.890625 Loss: 0.0034872221294790506\n",
      "Iteration: 6160 Training Accuracy: 0.921875 Loss: 0.0027981270104646683\n",
      "Iteration: 6170 Training Accuracy: 0.921875 Loss: 0.0020792768336832523\n",
      "Iteration: 6180 Training Accuracy: 0.9375 Loss: 0.0038253627717494965\n",
      "Iteration: 6190 Training Accuracy: 0.984375 Loss: 0.0009577080490998924\n",
      "Iteration: 6200 Training Accuracy: 0.890625 Loss: 0.0041480823419988155\n",
      "Iteration: 6210 Training Accuracy: 0.96875 Loss: 0.0017007988644763827\n",
      "Iteration: 6220 Training Accuracy: 0.984375 Loss: 0.001318570226430893\n",
      "Iteration: 6230 Training Accuracy: 0.9375 Loss: 0.003456603270024061\n",
      "Iteration: 6240 Training Accuracy: 0.90625 Loss: 0.003177262842655182\n",
      "Iteration: 6250 Training Accuracy: 0.953125 Loss: 0.0018166619120165706\n",
      "Iteration: 6260 Training Accuracy: 0.90625 Loss: 0.003202997613698244\n",
      "Iteration: 6270 Training Accuracy: 0.96875 Loss: 0.0013696668902412057\n",
      "Iteration: 6280 Training Accuracy: 0.953125 Loss: 0.0031098276376724243\n",
      "Iteration: 6290 Training Accuracy: 0.984375 Loss: 0.0016113072633743286\n",
      "Iteration: 6300 Training Accuracy: 0.9375 Loss: 0.004200083669275045\n",
      "Iteration: 6310 Training Accuracy: 0.96875 Loss: 0.002691696397960186\n",
      "Iteration: 6320 Training Accuracy: 0.9375 Loss: 0.0028612783644348383\n",
      "Iteration: 6330 Training Accuracy: 0.96875 Loss: 0.0032510629389435053\n",
      "Iteration: 6340 Training Accuracy: 0.96875 Loss: 0.0018270956352353096\n",
      "Iteration: 6350 Training Accuracy: 0.953125 Loss: 0.002566339448094368\n",
      "Iteration: 6360 Training Accuracy: 0.953125 Loss: 0.002157552633434534\n",
      "Iteration: 6370 Training Accuracy: 0.953125 Loss: 0.0012241046642884612\n",
      "Iteration: 6380 Training Accuracy: 0.890625 Loss: 0.005051419138908386\n",
      "Iteration: 6390 Training Accuracy: 0.96875 Loss: 0.0018759118393063545\n",
      "Iteration: 6400 Training Accuracy: 0.9375 Loss: 0.003204982727766037\n",
      "Iteration: 6410 Training Accuracy: 0.96875 Loss: 0.0016469394322484732\n",
      "Iteration: 6420 Training Accuracy: 0.90625 Loss: 0.00431864196434617\n",
      "Iteration: 6430 Training Accuracy: 0.9375 Loss: 0.003091435879468918\n",
      "Iteration: 6440 Training Accuracy: 0.953125 Loss: 0.0022120149806141853\n",
      "Iteration: 6450 Training Accuracy: 0.96875 Loss: 0.0016214345814660192\n",
      "Iteration: 6460 Training Accuracy: 0.921875 Loss: 0.0034325546585023403\n",
      "Iteration: 6470 Training Accuracy: 0.875 Loss: 0.00527279544621706\n",
      "Iteration: 6480 Training Accuracy: 0.9375 Loss: 0.002930216956883669\n",
      "Iteration: 6490 Training Accuracy: 0.984375 Loss: 0.001021282165311277\n",
      "Iteration: 6500 Training Accuracy: 0.953125 Loss: 0.0015809559263288975\n",
      "Iteration: 6510 Training Accuracy: 0.953125 Loss: 0.0034247238654643297\n",
      "Iteration: 6520 Training Accuracy: 0.921875 Loss: 0.003871121909469366\n",
      "Iteration: 6530 Training Accuracy: 0.96875 Loss: 0.0012415515957400203\n",
      "Iteration: 6540 Training Accuracy: 0.90625 Loss: 0.00616965489462018\n",
      "Iteration: 6550 Training Accuracy: 0.96875 Loss: 0.0017467831494286656\n",
      "Iteration: 6560 Training Accuracy: 0.9375 Loss: 0.004538710229098797\n",
      "Training Accuracy = 0.90625\n",
      "Validation Accuracy = 0.8731666666666666\n",
      "epoch: 7\n",
      "Iteration: 6570 Training Accuracy: 0.921875 Loss: 0.0041918037459254265\n",
      "Iteration: 6580 Training Accuracy: 0.9375 Loss: 0.003561291377991438\n",
      "Iteration: 6590 Training Accuracy: 0.984375 Loss: 0.0007360801682807505\n",
      "Iteration: 6600 Training Accuracy: 0.9375 Loss: 0.0031051612459123135\n",
      "Iteration: 6610 Training Accuracy: 0.984375 Loss: 0.000788733595982194\n",
      "Iteration: 6620 Training Accuracy: 0.890625 Loss: 0.0032841910142451525\n",
      "Iteration: 6630 Training Accuracy: 0.96875 Loss: 0.001526392763480544\n",
      "Iteration: 6640 Training Accuracy: 0.953125 Loss: 0.0024669463746249676\n",
      "Iteration: 6650 Training Accuracy: 0.953125 Loss: 0.0010275881504639983\n",
      "Iteration: 6660 Training Accuracy: 0.953125 Loss: 0.0016817401628941298\n",
      "Iteration: 6670 Training Accuracy: 0.984375 Loss: 0.001387834781780839\n",
      "Iteration: 6680 Training Accuracy: 0.96875 Loss: 0.0019397662254050374\n",
      "Iteration: 6690 Training Accuracy: 0.984375 Loss: 0.0012825732119381428\n",
      "Iteration: 6700 Training Accuracy: 0.953125 Loss: 0.002052313880994916\n",
      "Iteration: 6710 Training Accuracy: 0.984375 Loss: 0.0009666777332313359\n",
      "Iteration: 6720 Training Accuracy: 1.0 Loss: 0.0005804861430078745\n",
      "Iteration: 6730 Training Accuracy: 0.96875 Loss: 0.0013047023676335812\n",
      "Iteration: 6740 Training Accuracy: 0.9375 Loss: 0.0021875230595469475\n",
      "Iteration: 6750 Training Accuracy: 0.953125 Loss: 0.0026551196351647377\n",
      "Iteration: 6760 Training Accuracy: 0.9375 Loss: 0.0014581960858777165\n",
      "Iteration: 6770 Training Accuracy: 0.984375 Loss: 0.000409287225920707\n",
      "Iteration: 6780 Training Accuracy: 0.984375 Loss: 0.0011465896386653185\n",
      "Iteration: 6790 Training Accuracy: 0.921875 Loss: 0.0022425870411098003\n",
      "Iteration: 6800 Training Accuracy: 0.96875 Loss: 0.0016271668719127774\n",
      "Iteration: 6810 Training Accuracy: 0.984375 Loss: 0.0018425688613206148\n",
      "Iteration: 6820 Training Accuracy: 0.96875 Loss: 0.0010382276959717274\n",
      "Iteration: 6830 Training Accuracy: 1.0 Loss: 0.0005916948430240154\n",
      "Iteration: 6840 Training Accuracy: 0.984375 Loss: 0.0005688502569682896\n",
      "Iteration: 6850 Training Accuracy: 0.953125 Loss: 0.0020179213024675846\n",
      "Iteration: 6860 Training Accuracy: 0.953125 Loss: 0.0023303546477109194\n",
      "Iteration: 6870 Training Accuracy: 1.0 Loss: 0.00042380846571177244\n",
      "Iteration: 6880 Training Accuracy: 0.984375 Loss: 0.0012660108041018248\n",
      "Iteration: 6890 Training Accuracy: 0.96875 Loss: 0.0014844025718048215\n",
      "Iteration: 6900 Training Accuracy: 0.96875 Loss: 0.001638207584619522\n",
      "Iteration: 6910 Training Accuracy: 0.96875 Loss: 0.002444519894197583\n",
      "Iteration: 6920 Training Accuracy: 1.0 Loss: 0.0003031244850717485\n",
      "Iteration: 6930 Training Accuracy: 0.96875 Loss: 0.0012914349790662527\n",
      "Iteration: 6940 Training Accuracy: 0.96875 Loss: 0.0014480542158707976\n",
      "Iteration: 6950 Training Accuracy: 0.96875 Loss: 0.0019238227978348732\n",
      "Iteration: 6960 Training Accuracy: 0.953125 Loss: 0.0016545599792152643\n",
      "Iteration: 6970 Training Accuracy: 1.0 Loss: 0.0006580136250704527\n",
      "Iteration: 6980 Training Accuracy: 0.953125 Loss: 0.0015537794679403305\n",
      "Iteration: 6990 Training Accuracy: 1.0 Loss: 0.0005171777447685599\n",
      "Iteration: 7000 Training Accuracy: 1.0 Loss: 0.00043372847721911967\n",
      "Iteration: 7010 Training Accuracy: 0.953125 Loss: 0.004098429344594479\n",
      "Iteration: 7020 Training Accuracy: 0.96875 Loss: 0.0016309954226016998\n",
      "Iteration: 7030 Training Accuracy: 1.0 Loss: 0.0007061937940306962\n",
      "Iteration: 7040 Training Accuracy: 0.953125 Loss: 0.001607010723091662\n",
      "Iteration: 7050 Training Accuracy: 0.921875 Loss: 0.0019106038380414248\n",
      "Iteration: 7060 Training Accuracy: 0.984375 Loss: 0.0006758111994713545\n",
      "Iteration: 7070 Training Accuracy: 0.921875 Loss: 0.003302645869553089\n",
      "Iteration: 7080 Training Accuracy: 0.953125 Loss: 0.001589065883308649\n",
      "Iteration: 7090 Training Accuracy: 0.96875 Loss: 0.0013798362342640758\n",
      "Iteration: 7100 Training Accuracy: 0.953125 Loss: 0.0015788469463586807\n",
      "Iteration: 7110 Training Accuracy: 0.984375 Loss: 0.0007689812919124961\n",
      "Iteration: 7120 Training Accuracy: 0.984375 Loss: 0.001040543895214796\n",
      "Iteration: 7130 Training Accuracy: 0.953125 Loss: 0.0016018461901694536\n",
      "Iteration: 7140 Training Accuracy: 0.96875 Loss: 0.001314470311626792\n",
      "Iteration: 7150 Training Accuracy: 1.0 Loss: 2.69950287474785e-05\n",
      "Iteration: 7160 Training Accuracy: 1.0 Loss: 0.000732066691853106\n",
      "Iteration: 7170 Training Accuracy: 1.0 Loss: 0.0007789110532030463\n",
      "Iteration: 7180 Training Accuracy: 0.984375 Loss: 0.0013728886842727661\n",
      "Iteration: 7190 Training Accuracy: 0.96875 Loss: 0.001984819769859314\n",
      "Iteration: 7200 Training Accuracy: 0.96875 Loss: 0.001975381514057517\n",
      "Iteration: 7210 Training Accuracy: 0.984375 Loss: 0.0005480051040649414\n",
      "Iteration: 7220 Training Accuracy: 1.0 Loss: 0.0005956622771918774\n",
      "Iteration: 7230 Training Accuracy: 0.96875 Loss: 0.0012577241286635399\n",
      "Iteration: 7240 Training Accuracy: 0.984375 Loss: 0.0006527872174046934\n",
      "Iteration: 7250 Training Accuracy: 0.984375 Loss: 0.0005490152398124337\n",
      "Iteration: 7260 Training Accuracy: 1.0 Loss: 0.0003954555722884834\n",
      "Iteration: 7270 Training Accuracy: 0.953125 Loss: 0.001913885585963726\n",
      "Iteration: 7280 Training Accuracy: 0.96875 Loss: 0.0017185751348733902\n",
      "Iteration: 7290 Training Accuracy: 0.9375 Loss: 0.002416802803054452\n",
      "Iteration: 7300 Training Accuracy: 0.953125 Loss: 0.0019500867929309607\n",
      "Iteration: 7310 Training Accuracy: 0.984375 Loss: 0.0013178273802623153\n",
      "Iteration: 7320 Training Accuracy: 1.0 Loss: 0.0007591298199258745\n",
      "Iteration: 7330 Training Accuracy: 0.984375 Loss: 0.0020823574159294367\n",
      "Iteration: 7340 Training Accuracy: 0.96875 Loss: 0.0021772815380245447\n",
      "Iteration: 7350 Training Accuracy: 0.96875 Loss: 0.0017538632964715362\n",
      "Iteration: 7360 Training Accuracy: 0.984375 Loss: 0.0011431715684011579\n",
      "Iteration: 7370 Training Accuracy: 0.96875 Loss: 0.0014049827586859465\n",
      "Iteration: 7380 Training Accuracy: 0.984375 Loss: 0.0013139833463355899\n",
      "Iteration: 7390 Training Accuracy: 0.984375 Loss: 0.0009402999421581626\n",
      "Iteration: 7400 Training Accuracy: 0.96875 Loss: 0.0013337874552235007\n",
      "Iteration: 7410 Training Accuracy: 0.9375 Loss: 0.0014757520984858274\n",
      "Iteration: 7420 Training Accuracy: 0.96875 Loss: 0.001099532120861113\n",
      "Iteration: 7430 Training Accuracy: 0.953125 Loss: 0.0018612481653690338\n",
      "Iteration: 7440 Training Accuracy: 0.96875 Loss: 0.0011605966137722135\n",
      "Iteration: 7450 Training Accuracy: 0.953125 Loss: 0.0021210855338722467\n",
      "Iteration: 7460 Training Accuracy: 0.9375 Loss: 0.002198035828769207\n",
      "Iteration: 7470 Training Accuracy: 1.0 Loss: 0.00040689425077289343\n",
      "Iteration: 7480 Training Accuracy: 0.984375 Loss: 0.0008496600203216076\n",
      "Iteration: 7490 Training Accuracy: 1.0 Loss: 0.00103821384254843\n",
      "Iteration: 7500 Training Accuracy: 1.0 Loss: 0.00025280300178565085\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9191666666666667\n",
      "epoch: 8\n",
      "Iteration: 7510 Training Accuracy: 1.0 Loss: 0.0006510330713354051\n",
      "Iteration: 7520 Training Accuracy: 0.984375 Loss: 0.0008398562204092741\n",
      "Iteration: 7530 Training Accuracy: 0.96875 Loss: 0.0008529420010745525\n",
      "Iteration: 7540 Training Accuracy: 0.96875 Loss: 0.001703058835119009\n",
      "Iteration: 7550 Training Accuracy: 0.96875 Loss: 0.0011056603398174047\n",
      "Iteration: 7560 Training Accuracy: 0.984375 Loss: 0.0010002815397456288\n",
      "Iteration: 7570 Training Accuracy: 0.921875 Loss: 0.003055328968912363\n",
      "Iteration: 7580 Training Accuracy: 0.9375 Loss: 0.0032713757827878\n",
      "Iteration: 7590 Training Accuracy: 0.953125 Loss: 0.0021475397516041994\n",
      "Iteration: 7600 Training Accuracy: 0.96875 Loss: 0.0015912220114842057\n",
      "Iteration: 7610 Training Accuracy: 0.953125 Loss: 0.00214274856261909\n",
      "Iteration: 7620 Training Accuracy: 0.984375 Loss: 0.0005044188583269715\n",
      "Iteration: 7630 Training Accuracy: 0.953125 Loss: 0.0021162335760891438\n",
      "Iteration: 7640 Training Accuracy: 1.0 Loss: 0.00027217320166528225\n",
      "Iteration: 7650 Training Accuracy: 1.0 Loss: 0.0008934937650337815\n",
      "Iteration: 7660 Training Accuracy: 0.953125 Loss: 0.001908918609842658\n",
      "Iteration: 7670 Training Accuracy: 1.0 Loss: 0.00046777844545431435\n",
      "Iteration: 7680 Training Accuracy: 0.96875 Loss: 0.001334192929789424\n",
      "Iteration: 7690 Training Accuracy: 0.953125 Loss: 0.001668829470872879\n",
      "Iteration: 7700 Training Accuracy: 0.96875 Loss: 0.0008006244897842407\n",
      "Iteration: 7710 Training Accuracy: 0.96875 Loss: 0.0013115426991134882\n",
      "Iteration: 7720 Training Accuracy: 0.9375 Loss: 0.0027566507924348116\n",
      "Iteration: 7730 Training Accuracy: 0.96875 Loss: 0.0013174728956073523\n",
      "Iteration: 7740 Training Accuracy: 0.984375 Loss: 0.0005982053698971868\n",
      "Iteration: 7750 Training Accuracy: 0.984375 Loss: 0.0011801044456660748\n",
      "Iteration: 7760 Training Accuracy: 1.0 Loss: 0.0004408261738717556\n",
      "Iteration: 7770 Training Accuracy: 0.96875 Loss: 0.0013082539662718773\n",
      "Iteration: 7780 Training Accuracy: 0.984375 Loss: 0.0012160006444901228\n",
      "Iteration: 7790 Training Accuracy: 1.0 Loss: 0.0002736716705840081\n",
      "Iteration: 7800 Training Accuracy: 1.0 Loss: 0.0008428199216723442\n",
      "Iteration: 7810 Training Accuracy: 1.0 Loss: 0.0007202877895906568\n",
      "Iteration: 7820 Training Accuracy: 0.984375 Loss: 0.0014171148650348186\n",
      "Iteration: 7830 Training Accuracy: 1.0 Loss: 0.0002394577895756811\n",
      "Iteration: 7840 Training Accuracy: 0.890625 Loss: 0.004449915140867233\n",
      "Iteration: 7850 Training Accuracy: 0.984375 Loss: 0.0007319250144064426\n",
      "Iteration: 7860 Training Accuracy: 0.984375 Loss: 0.0005056437221355736\n",
      "Iteration: 7870 Training Accuracy: 0.984375 Loss: 0.0008148764027282596\n",
      "Iteration: 7880 Training Accuracy: 0.96875 Loss: 0.0013587269932031631\n",
      "Iteration: 7890 Training Accuracy: 0.953125 Loss: 0.0024915561079978943\n",
      "Iteration: 7900 Training Accuracy: 0.984375 Loss: 0.0006720867240801454\n",
      "Iteration: 7910 Training Accuracy: 1.0 Loss: 0.0003693189937621355\n",
      "Iteration: 7920 Training Accuracy: 0.984375 Loss: 0.0010769431246444583\n",
      "Iteration: 7930 Training Accuracy: 0.984375 Loss: 0.0008901053224690259\n",
      "Iteration: 7940 Training Accuracy: 0.984375 Loss: 0.0008348014089278877\n",
      "Iteration: 7950 Training Accuracy: 0.953125 Loss: 0.0010507599217817187\n",
      "Iteration: 7960 Training Accuracy: 1.0 Loss: 0.0005438812077045441\n",
      "Iteration: 7970 Training Accuracy: 0.984375 Loss: 0.0011184379691258073\n",
      "Iteration: 7980 Training Accuracy: 0.96875 Loss: 0.0012519286246970296\n",
      "Iteration: 7990 Training Accuracy: 0.953125 Loss: 0.0013900508638471365\n",
      "Iteration: 8000 Training Accuracy: 0.984375 Loss: 0.0008666989160701632\n",
      "Iteration: 8010 Training Accuracy: 0.96875 Loss: 0.002548670396208763\n",
      "Iteration: 8020 Training Accuracy: 1.0 Loss: 0.0005069745238870382\n",
      "Iteration: 8030 Training Accuracy: 0.96875 Loss: 0.0008987279143184423\n",
      "Iteration: 8040 Training Accuracy: 0.96875 Loss: 0.0017914542695507407\n",
      "Iteration: 8050 Training Accuracy: 0.953125 Loss: 0.0015765088610351086\n",
      "Iteration: 8060 Training Accuracy: 1.0 Loss: 0.0001904078817460686\n",
      "Iteration: 8070 Training Accuracy: 1.0 Loss: 0.00022084936790633947\n",
      "Iteration: 8080 Training Accuracy: 0.96875 Loss: 0.0010606879368424416\n",
      "Iteration: 8090 Training Accuracy: 1.0 Loss: 0.0007442112546414137\n",
      "Iteration: 8100 Training Accuracy: 0.984375 Loss: 0.0008805895922705531\n",
      "Iteration: 8110 Training Accuracy: 1.0 Loss: 0.0005752312135882676\n",
      "Iteration: 8120 Training Accuracy: 0.984375 Loss: 0.00040665926644578576\n",
      "Iteration: 8130 Training Accuracy: 1.0 Loss: 0.0002231230610050261\n",
      "Iteration: 8140 Training Accuracy: 1.0 Loss: 0.0005038849776610732\n",
      "Iteration: 8150 Training Accuracy: 0.953125 Loss: 0.002072927076369524\n",
      "Iteration: 8160 Training Accuracy: 1.0 Loss: 0.0006866090698167682\n",
      "Iteration: 8170 Training Accuracy: 0.96875 Loss: 0.0009656768525019288\n",
      "Iteration: 8180 Training Accuracy: 0.984375 Loss: 0.0006680403021164238\n",
      "Iteration: 8190 Training Accuracy: 0.953125 Loss: 0.002162903780117631\n",
      "Iteration: 8200 Training Accuracy: 1.0 Loss: 0.0011093291686847806\n",
      "Iteration: 8210 Training Accuracy: 1.0 Loss: 0.00025787277263589203\n",
      "Iteration: 8220 Training Accuracy: 0.96875 Loss: 0.0017755727749317884\n",
      "Iteration: 8230 Training Accuracy: 0.984375 Loss: 0.0011759252520278096\n",
      "Iteration: 8240 Training Accuracy: 0.9375 Loss: 0.002186702098697424\n",
      "Iteration: 8250 Training Accuracy: 0.96875 Loss: 0.0015341457910835743\n",
      "Iteration: 8260 Training Accuracy: 0.96875 Loss: 0.0011811582371592522\n",
      "Iteration: 8270 Training Accuracy: 0.984375 Loss: 0.0007247150642797351\n",
      "Iteration: 8280 Training Accuracy: 0.984375 Loss: 0.00098993512801826\n",
      "Iteration: 8290 Training Accuracy: 1.0 Loss: 0.0004088045097887516\n",
      "Iteration: 8300 Training Accuracy: 0.984375 Loss: 0.0005487136077135801\n",
      "Iteration: 8310 Training Accuracy: 1.0 Loss: 0.0004074567405041307\n",
      "Iteration: 8320 Training Accuracy: 0.9375 Loss: 0.00240731961093843\n",
      "Iteration: 8330 Training Accuracy: 0.984375 Loss: 0.0010049499105662107\n",
      "Iteration: 8340 Training Accuracy: 0.984375 Loss: 0.0005306127713993192\n",
      "Iteration: 8350 Training Accuracy: 0.984375 Loss: 0.001009492320008576\n",
      "Iteration: 8360 Training Accuracy: 0.953125 Loss: 0.002361724153161049\n",
      "Iteration: 8370 Training Accuracy: 0.96875 Loss: 0.0011318859178572893\n",
      "Iteration: 8380 Training Accuracy: 1.0 Loss: 0.0005684112547896802\n",
      "Iteration: 8390 Training Accuracy: 0.96875 Loss: 0.0018345387652516365\n",
      "Iteration: 8400 Training Accuracy: 1.0 Loss: 0.0004413578426465392\n",
      "Iteration: 8410 Training Accuracy: 0.984375 Loss: 0.0008549443446099758\n",
      "Iteration: 8420 Training Accuracy: 0.96875 Loss: 0.001323628704994917\n",
      "Iteration: 8430 Training Accuracy: 0.984375 Loss: 0.0008431297028437257\n",
      "Iteration: 8440 Training Accuracy: 0.96875 Loss: 0.0015788793098181486\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9218333333333333\n",
      "epoch: 9\n",
      "Iteration: 8450 Training Accuracy: 0.984375 Loss: 0.0006453184178099036\n",
      "Iteration: 8460 Training Accuracy: 0.984375 Loss: 0.0005947811296209693\n",
      "Iteration: 8470 Training Accuracy: 0.984375 Loss: 0.001954722451046109\n",
      "Iteration: 8480 Training Accuracy: 0.984375 Loss: 0.001157000777311623\n",
      "Iteration: 8490 Training Accuracy: 0.984375 Loss: 0.0011828318238258362\n",
      "Iteration: 8500 Training Accuracy: 1.0 Loss: 0.00048631196841597557\n",
      "Iteration: 8510 Training Accuracy: 0.953125 Loss: 0.0025291950441896915\n",
      "Iteration: 8520 Training Accuracy: 0.921875 Loss: 0.0024207234382629395\n",
      "Iteration: 8530 Training Accuracy: 0.921875 Loss: 0.0021778051741421223\n",
      "Iteration: 8540 Training Accuracy: 0.984375 Loss: 0.0005950828781351447\n",
      "Iteration: 8550 Training Accuracy: 0.984375 Loss: 0.0009594284347258508\n",
      "Iteration: 8560 Training Accuracy: 0.96875 Loss: 0.001749028917402029\n",
      "Iteration: 8570 Training Accuracy: 1.0 Loss: 0.0002499401452951133\n",
      "Iteration: 8580 Training Accuracy: 0.953125 Loss: 0.002441932912915945\n",
      "Iteration: 8590 Training Accuracy: 0.96875 Loss: 0.0013853672426193953\n",
      "Iteration: 8600 Training Accuracy: 0.96875 Loss: 0.0007450861157849431\n",
      "Iteration: 8610 Training Accuracy: 0.96875 Loss: 0.001151363947428763\n",
      "Iteration: 8620 Training Accuracy: 0.984375 Loss: 0.0006563915521837771\n",
      "Iteration: 8630 Training Accuracy: 1.0 Loss: 0.00045677475281991065\n",
      "Iteration: 8640 Training Accuracy: 1.0 Loss: 0.0004756592388730496\n",
      "Iteration: 8650 Training Accuracy: 1.0 Loss: 0.00026361836353316903\n",
      "Iteration: 8660 Training Accuracy: 0.96875 Loss: 0.0011795067694038153\n",
      "Iteration: 8670 Training Accuracy: 0.9375 Loss: 0.0020319963805377483\n",
      "Iteration: 8680 Training Accuracy: 0.96875 Loss: 0.0011632932582870126\n",
      "Iteration: 8690 Training Accuracy: 0.953125 Loss: 0.0014864148106426\n",
      "Iteration: 8700 Training Accuracy: 0.96875 Loss: 0.0011420187074691057\n",
      "Iteration: 8710 Training Accuracy: 0.984375 Loss: 0.0018429388292133808\n",
      "Iteration: 8720 Training Accuracy: 0.96875 Loss: 0.0014212422538548708\n",
      "Iteration: 8730 Training Accuracy: 0.953125 Loss: 0.0014122677966952324\n",
      "Iteration: 8740 Training Accuracy: 0.984375 Loss: 0.0005971444770693779\n",
      "Iteration: 8750 Training Accuracy: 0.984375 Loss: 0.0006289874436333776\n",
      "Iteration: 8760 Training Accuracy: 0.921875 Loss: 0.0036945780739188194\n",
      "Iteration: 8770 Training Accuracy: 1.0 Loss: 0.0007727132178843021\n",
      "Iteration: 8780 Training Accuracy: 1.0 Loss: 0.00017917543300427496\n",
      "Iteration: 8790 Training Accuracy: 0.984375 Loss: 0.0005556427640840411\n",
      "Iteration: 8800 Training Accuracy: 0.984375 Loss: 0.0005141617148183286\n",
      "Iteration: 8810 Training Accuracy: 0.953125 Loss: 0.00139295042026788\n",
      "Iteration: 8820 Training Accuracy: 0.984375 Loss: 0.0013685740996152163\n",
      "Iteration: 8830 Training Accuracy: 0.984375 Loss: 0.001187350950203836\n",
      "Iteration: 8840 Training Accuracy: 0.984375 Loss: 0.0013971233274787664\n",
      "Iteration: 8850 Training Accuracy: 0.96875 Loss: 0.001431856770068407\n",
      "Iteration: 8860 Training Accuracy: 1.0 Loss: 0.0002218766458099708\n",
      "Iteration: 8870 Training Accuracy: 0.953125 Loss: 0.002151118591427803\n",
      "Iteration: 8880 Training Accuracy: 1.0 Loss: 0.0007983451941981912\n",
      "Iteration: 8890 Training Accuracy: 0.953125 Loss: 0.0017810221761465073\n",
      "Iteration: 8900 Training Accuracy: 1.0 Loss: 0.0002476526133250445\n",
      "Iteration: 8910 Training Accuracy: 0.984375 Loss: 0.000745054567232728\n",
      "Iteration: 8920 Training Accuracy: 0.96875 Loss: 0.0010630807373672724\n",
      "Iteration: 8930 Training Accuracy: 0.984375 Loss: 0.0017572158249095082\n",
      "Iteration: 8940 Training Accuracy: 1.0 Loss: 0.0004934014286845922\n",
      "Iteration: 8950 Training Accuracy: 0.984375 Loss: 0.0008813382592052221\n",
      "Iteration: 8960 Training Accuracy: 0.984375 Loss: 0.0005687192897312343\n",
      "Iteration: 8970 Training Accuracy: 0.96875 Loss: 0.0009446534095332026\n",
      "Iteration: 8980 Training Accuracy: 0.96875 Loss: 0.0014693706762045622\n",
      "Iteration: 8990 Training Accuracy: 0.984375 Loss: 0.0018326662248000503\n",
      "Iteration: 9000 Training Accuracy: 0.96875 Loss: 0.0024186463560909033\n",
      "Iteration: 9010 Training Accuracy: 0.984375 Loss: 0.00044866499956697226\n",
      "Iteration: 9020 Training Accuracy: 0.96875 Loss: 0.0014204188482835889\n",
      "Iteration: 9030 Training Accuracy: 0.984375 Loss: 0.0010414475109428167\n",
      "Iteration: 9040 Training Accuracy: 1.0 Loss: 0.0006261835806071758\n",
      "Iteration: 9050 Training Accuracy: 1.0 Loss: 0.00046315454528667033\n",
      "Iteration: 9060 Training Accuracy: 1.0 Loss: 0.0007064241217449307\n",
      "Iteration: 9070 Training Accuracy: 0.984375 Loss: 0.0009396370151080191\n",
      "Iteration: 9080 Training Accuracy: 0.984375 Loss: 0.0014443169347941875\n",
      "Iteration: 9090 Training Accuracy: 1.0 Loss: 0.0002920054830610752\n",
      "Iteration: 9100 Training Accuracy: 0.953125 Loss: 0.0021225735545158386\n",
      "Iteration: 9110 Training Accuracy: 0.9375 Loss: 0.0027967719361186028\n",
      "Iteration: 9120 Training Accuracy: 1.0 Loss: 0.00019694764341693372\n",
      "Iteration: 9130 Training Accuracy: 1.0 Loss: 0.0004164234269410372\n",
      "Iteration: 9140 Training Accuracy: 1.0 Loss: 0.00040111024281941354\n",
      "Iteration: 9150 Training Accuracy: 0.984375 Loss: 0.0006799344555474818\n",
      "Iteration: 9160 Training Accuracy: 0.96875 Loss: 0.0010026982054114342\n",
      "Iteration: 9170 Training Accuracy: 0.96875 Loss: 0.0009195525199174881\n",
      "Iteration: 9180 Training Accuracy: 0.96875 Loss: 0.0017658518627285957\n",
      "Iteration: 9190 Training Accuracy: 0.9375 Loss: 0.001941627822816372\n",
      "Iteration: 9200 Training Accuracy: 0.984375 Loss: 0.0012813915964215994\n",
      "Iteration: 9210 Training Accuracy: 0.96875 Loss: 0.0013478852342814207\n",
      "Iteration: 9220 Training Accuracy: 0.96875 Loss: 0.0015776443760842085\n",
      "Iteration: 9230 Training Accuracy: 0.984375 Loss: 0.0011060575488954782\n",
      "Iteration: 9240 Training Accuracy: 0.984375 Loss: 0.0008567878976464272\n",
      "Iteration: 9250 Training Accuracy: 0.953125 Loss: 0.0011959407711401582\n",
      "Iteration: 9260 Training Accuracy: 0.96875 Loss: 0.001512068323791027\n",
      "Iteration: 9270 Training Accuracy: 1.0 Loss: 0.00013273817603476346\n",
      "Iteration: 9280 Training Accuracy: 1.0 Loss: 0.0002960815909318626\n",
      "Iteration: 9290 Training Accuracy: 0.96875 Loss: 0.001540169701911509\n",
      "Iteration: 9300 Training Accuracy: 0.984375 Loss: 0.001361706294119358\n",
      "Iteration: 9310 Training Accuracy: 0.984375 Loss: 0.0009698041249066591\n",
      "Iteration: 9320 Training Accuracy: 0.984375 Loss: 0.0008173545356839895\n",
      "Iteration: 9330 Training Accuracy: 0.96875 Loss: 0.001873146160505712\n",
      "Iteration: 9340 Training Accuracy: 0.984375 Loss: 0.0003727442817762494\n",
      "Iteration: 9350 Training Accuracy: 1.0 Loss: 0.00036759459180757403\n",
      "Iteration: 9360 Training Accuracy: 0.984375 Loss: 0.0008212674292735755\n",
      "Iteration: 9370 Training Accuracy: 0.984375 Loss: 0.0009582121856510639\n",
      "Iteration: 9380 Training Accuracy: 1.0 Loss: 0.00040869874646887183\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9216666666666666\n",
      "epoch: 10\n",
      "Iteration: 9390 Training Accuracy: 0.9375 Loss: 0.0018576664151623845\n",
      "Iteration: 9400 Training Accuracy: 1.0 Loss: 0.00038418214535340667\n",
      "Iteration: 9410 Training Accuracy: 0.96875 Loss: 0.0015775300562381744\n",
      "Iteration: 9420 Training Accuracy: 0.953125 Loss: 0.0011628064094111323\n",
      "Iteration: 9430 Training Accuracy: 0.984375 Loss: 0.001640535774640739\n",
      "Iteration: 9440 Training Accuracy: 0.953125 Loss: 0.0013865886721760035\n",
      "Iteration: 9450 Training Accuracy: 0.984375 Loss: 0.0008191232918761671\n",
      "Iteration: 9460 Training Accuracy: 0.96875 Loss: 0.001390160177834332\n",
      "Iteration: 9470 Training Accuracy: 1.0 Loss: 0.000528056756593287\n",
      "Iteration: 9480 Training Accuracy: 0.984375 Loss: 0.0009309082524850965\n",
      "Iteration: 9490 Training Accuracy: 1.0 Loss: 0.0006737688090652227\n",
      "Iteration: 9500 Training Accuracy: 0.96875 Loss: 0.0006969613023102283\n",
      "Iteration: 9510 Training Accuracy: 0.921875 Loss: 0.0021810235921293497\n",
      "Iteration: 9520 Training Accuracy: 0.96875 Loss: 0.0019707626197487116\n",
      "Iteration: 9530 Training Accuracy: 0.953125 Loss: 0.0012127968948334455\n",
      "Iteration: 9540 Training Accuracy: 1.0 Loss: 0.0002688055392354727\n",
      "Iteration: 9550 Training Accuracy: 0.953125 Loss: 0.00209613936021924\n",
      "Iteration: 9560 Training Accuracy: 0.953125 Loss: 0.0017036544159054756\n",
      "Iteration: 9570 Training Accuracy: 1.0 Loss: 0.0006605249363929033\n",
      "Iteration: 9580 Training Accuracy: 0.96875 Loss: 0.0021938923746347427\n",
      "Iteration: 9590 Training Accuracy: 0.984375 Loss: 0.0005133957602083683\n",
      "Iteration: 9600 Training Accuracy: 0.96875 Loss: 0.0009809217881411314\n",
      "Iteration: 9610 Training Accuracy: 0.9375 Loss: 0.0017596916295588017\n",
      "Iteration: 9620 Training Accuracy: 0.984375 Loss: 0.0015322731342166662\n",
      "Iteration: 9630 Training Accuracy: 1.0 Loss: 0.0006064590997993946\n",
      "Iteration: 9640 Training Accuracy: 1.0 Loss: 0.0004296672123018652\n",
      "Iteration: 9650 Training Accuracy: 1.0 Loss: 0.0004256105748936534\n",
      "Iteration: 9660 Training Accuracy: 0.953125 Loss: 0.0013910725247114897\n",
      "Iteration: 9670 Training Accuracy: 0.984375 Loss: 0.0007671516505070031\n",
      "Iteration: 9680 Training Accuracy: 0.96875 Loss: 0.0016885418444871902\n",
      "Iteration: 9690 Training Accuracy: 0.96875 Loss: 0.00391315296292305\n",
      "Iteration: 9700 Training Accuracy: 0.984375 Loss: 0.0010710740461945534\n",
      "Iteration: 9710 Training Accuracy: 0.984375 Loss: 0.0008480568649247289\n",
      "Iteration: 9720 Training Accuracy: 1.0 Loss: 0.00021440739510580897\n",
      "Iteration: 9730 Training Accuracy: 0.984375 Loss: 0.0006256835767999291\n",
      "Iteration: 9740 Training Accuracy: 0.984375 Loss: 0.0006629787385463715\n",
      "Iteration: 9750 Training Accuracy: 1.0 Loss: 0.0009273948962800205\n",
      "Iteration: 9760 Training Accuracy: 1.0 Loss: 0.0009250328876078129\n",
      "Iteration: 9770 Training Accuracy: 0.96875 Loss: 0.0015650312416255474\n",
      "Iteration: 9780 Training Accuracy: 0.96875 Loss: 0.0015511694364249706\n",
      "Iteration: 9790 Training Accuracy: 0.984375 Loss: 0.0006659913342446089\n",
      "Iteration: 9800 Training Accuracy: 0.96875 Loss: 0.0011166614713147283\n",
      "Iteration: 9810 Training Accuracy: 1.0 Loss: 0.0007945668185129762\n",
      "Iteration: 9820 Training Accuracy: 0.984375 Loss: 0.000613910611718893\n",
      "Iteration: 9830 Training Accuracy: 0.96875 Loss: 0.0015612610150128603\n",
      "Iteration: 9840 Training Accuracy: 0.96875 Loss: 0.0011539580300450325\n",
      "Iteration: 9850 Training Accuracy: 1.0 Loss: 0.00041144766146317124\n",
      "Iteration: 9860 Training Accuracy: 0.96875 Loss: 0.0010776754934340715\n",
      "Iteration: 9870 Training Accuracy: 1.0 Loss: 0.000401505792979151\n",
      "Iteration: 9880 Training Accuracy: 0.953125 Loss: 0.001418267609551549\n",
      "Iteration: 9890 Training Accuracy: 1.0 Loss: 0.00027823360869660974\n",
      "Iteration: 9900 Training Accuracy: 0.984375 Loss: 0.00037181988591328263\n",
      "Iteration: 9910 Training Accuracy: 1.0 Loss: 0.0002853026962839067\n",
      "Iteration: 9920 Training Accuracy: 1.0 Loss: 0.00027722230879589915\n",
      "Iteration: 9930 Training Accuracy: 0.984375 Loss: 0.0007723291637375951\n",
      "Iteration: 9940 Training Accuracy: 0.984375 Loss: 0.0004585351562127471\n",
      "Iteration: 9950 Training Accuracy: 0.96875 Loss: 0.0018224217928946018\n",
      "Iteration: 9960 Training Accuracy: 1.0 Loss: 0.0008114750380627811\n",
      "Iteration: 9970 Training Accuracy: 0.984375 Loss: 0.0005551014910452068\n",
      "Iteration: 9980 Training Accuracy: 0.984375 Loss: 0.0007774581899866462\n",
      "Iteration: 9990 Training Accuracy: 1.0 Loss: 0.0002277408493682742\n",
      "Iteration: 10000 Training Accuracy: 0.984375 Loss: 0.0013573720352724195\n",
      "Iteration: 10010 Training Accuracy: 1.0 Loss: 0.0003037144779227674\n",
      "Iteration: 10020 Training Accuracy: 1.0 Loss: 0.0003454050456639379\n",
      "Iteration: 10030 Training Accuracy: 1.0 Loss: 0.00035556426155380905\n",
      "Iteration: 10040 Training Accuracy: 0.953125 Loss: 0.001695241779088974\n",
      "Iteration: 10050 Training Accuracy: 1.0 Loss: 0.0005259852041490376\n",
      "Iteration: 10060 Training Accuracy: 0.96875 Loss: 0.0017970872577279806\n",
      "Iteration: 10070 Training Accuracy: 1.0 Loss: 0.00021776465291623026\n",
      "Iteration: 10080 Training Accuracy: 1.0 Loss: 0.0005463833222165704\n",
      "Iteration: 10090 Training Accuracy: 0.984375 Loss: 0.00028429855592548847\n",
      "Iteration: 10100 Training Accuracy: 0.984375 Loss: 0.001217313576489687\n",
      "Iteration: 10110 Training Accuracy: 0.96875 Loss: 0.0017512779450044036\n",
      "Iteration: 10120 Training Accuracy: 1.0 Loss: 0.00043231688323430717\n",
      "Iteration: 10130 Training Accuracy: 0.96875 Loss: 0.001574470428749919\n",
      "Iteration: 10140 Training Accuracy: 0.984375 Loss: 0.0015180158661678433\n",
      "Iteration: 10150 Training Accuracy: 1.0 Loss: 0.000885315821506083\n",
      "Iteration: 10160 Training Accuracy: 0.953125 Loss: 0.0016755880787968636\n",
      "Iteration: 10170 Training Accuracy: 1.0 Loss: 0.00021425861632451415\n",
      "Iteration: 10180 Training Accuracy: 0.9375 Loss: 0.003143422771245241\n",
      "Iteration: 10190 Training Accuracy: 0.953125 Loss: 0.0018991805845871568\n",
      "Iteration: 10200 Training Accuracy: 0.96875 Loss: 0.000694651622325182\n",
      "Iteration: 10210 Training Accuracy: 0.984375 Loss: 0.0011677708243951201\n",
      "Iteration: 10220 Training Accuracy: 0.96875 Loss: 0.0019294695230200887\n",
      "Iteration: 10230 Training Accuracy: 0.984375 Loss: 0.0010827146470546722\n",
      "Iteration: 10240 Training Accuracy: 1.0 Loss: 0.00014389454736374319\n",
      "Iteration: 10250 Training Accuracy: 1.0 Loss: 0.00021249153360258788\n",
      "Iteration: 10260 Training Accuracy: 0.984375 Loss: 0.0008154496317729354\n",
      "Iteration: 10270 Training Accuracy: 0.984375 Loss: 0.0008018708322197199\n",
      "Iteration: 10280 Training Accuracy: 0.984375 Loss: 0.0006586108356714249\n",
      "Iteration: 10290 Training Accuracy: 0.984375 Loss: 0.0009991402039304376\n",
      "Iteration: 10300 Training Accuracy: 0.984375 Loss: 0.0005734235164709389\n",
      "Iteration: 10310 Training Accuracy: 0.984375 Loss: 0.0007508416892960668\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9215\n",
      "epoch: 11\n",
      "Iteration: 10320 Training Accuracy: 0.96875 Loss: 0.0008756967727094889\n",
      "Iteration: 10330 Training Accuracy: 0.953125 Loss: 0.0017117103561758995\n",
      "Iteration: 10340 Training Accuracy: 0.96875 Loss: 0.0011910841567441821\n",
      "Iteration: 10350 Training Accuracy: 0.984375 Loss: 0.0006216781912371516\n",
      "Iteration: 10360 Training Accuracy: 0.984375 Loss: 0.0014179195277392864\n",
      "Iteration: 10370 Training Accuracy: 0.96875 Loss: 0.0011241266038268805\n",
      "Iteration: 10380 Training Accuracy: 1.0 Loss: 0.00026356367743574083\n",
      "Iteration: 10390 Training Accuracy: 0.984375 Loss: 0.0012064543552696705\n",
      "Iteration: 10400 Training Accuracy: 1.0 Loss: 7.188577728811651e-05\n",
      "Iteration: 10410 Training Accuracy: 0.984375 Loss: 0.0008329189149662852\n",
      "Iteration: 10420 Training Accuracy: 0.953125 Loss: 0.0024168812669813633\n",
      "Iteration: 10430 Training Accuracy: 1.0 Loss: 0.0003901947056874633\n",
      "Iteration: 10440 Training Accuracy: 1.0 Loss: 0.00031412349198944867\n",
      "Iteration: 10450 Training Accuracy: 0.984375 Loss: 0.002183379139751196\n",
      "Iteration: 10460 Training Accuracy: 1.0 Loss: 0.0004314871912356466\n",
      "Iteration: 10470 Training Accuracy: 1.0 Loss: 0.0009194804588332772\n",
      "Iteration: 10480 Training Accuracy: 1.0 Loss: 0.00044952071039006114\n",
      "Iteration: 10490 Training Accuracy: 0.96875 Loss: 0.0010427427478134632\n",
      "Iteration: 10500 Training Accuracy: 0.984375 Loss: 0.0013814434641972184\n",
      "Iteration: 10510 Training Accuracy: 1.0 Loss: 0.00039439904503524303\n",
      "Iteration: 10520 Training Accuracy: 1.0 Loss: 0.0005361846997402608\n",
      "Iteration: 10530 Training Accuracy: 1.0 Loss: 0.00025236926740035415\n",
      "Iteration: 10540 Training Accuracy: 0.96875 Loss: 0.001822725054807961\n",
      "Iteration: 10550 Training Accuracy: 0.984375 Loss: 0.001888054539449513\n",
      "Iteration: 10560 Training Accuracy: 1.0 Loss: 0.0003294808557257056\n",
      "Iteration: 10570 Training Accuracy: 0.953125 Loss: 0.0011533484794199467\n",
      "Iteration: 10580 Training Accuracy: 0.984375 Loss: 0.0008250244427472353\n",
      "Iteration: 10590 Training Accuracy: 0.96875 Loss: 0.0014699280727654696\n",
      "Iteration: 10600 Training Accuracy: 0.96875 Loss: 0.0014473098563030362\n",
      "Iteration: 10610 Training Accuracy: 1.0 Loss: 0.00040746020385995507\n",
      "Iteration: 10620 Training Accuracy: 1.0 Loss: 0.00034697522642090917\n",
      "Iteration: 10630 Training Accuracy: 0.96875 Loss: 0.0011299733305349946\n",
      "Iteration: 10640 Training Accuracy: 0.96875 Loss: 0.0013701663119718432\n",
      "Iteration: 10650 Training Accuracy: 1.0 Loss: 0.0004353863187134266\n",
      "Iteration: 10660 Training Accuracy: 1.0 Loss: 0.00038650701753795147\n",
      "Iteration: 10670 Training Accuracy: 0.96875 Loss: 0.001155639998614788\n",
      "Iteration: 10680 Training Accuracy: 1.0 Loss: 0.00028140447102487087\n",
      "Iteration: 10690 Training Accuracy: 1.0 Loss: 0.00046164693776518106\n",
      "Iteration: 10700 Training Accuracy: 0.953125 Loss: 0.002211206592619419\n",
      "Iteration: 10710 Training Accuracy: 0.984375 Loss: 0.0006308169104158878\n",
      "Iteration: 10720 Training Accuracy: 0.953125 Loss: 0.0014121964341029525\n",
      "Iteration: 10730 Training Accuracy: 0.921875 Loss: 0.0029759567696601152\n",
      "Iteration: 10740 Training Accuracy: 0.984375 Loss: 0.0007649416802451015\n",
      "Iteration: 10750 Training Accuracy: 1.0 Loss: 0.00030994226108305156\n",
      "Iteration: 10760 Training Accuracy: 1.0 Loss: 0.00039356149500235915\n",
      "Iteration: 10770 Training Accuracy: 0.96875 Loss: 0.0008924554567784071\n",
      "Iteration: 10780 Training Accuracy: 0.984375 Loss: 0.0007308661006391048\n",
      "Iteration: 10790 Training Accuracy: 1.0 Loss: 0.00012957965373061597\n",
      "Iteration: 10800 Training Accuracy: 0.984375 Loss: 0.0005818083882331848\n",
      "Iteration: 10810 Training Accuracy: 0.984375 Loss: 0.0007062768563628197\n",
      "Iteration: 10820 Training Accuracy: 0.984375 Loss: 0.0014519081450998783\n",
      "Iteration: 10830 Training Accuracy: 0.96875 Loss: 0.0015703572425991297\n",
      "Iteration: 10840 Training Accuracy: 0.96875 Loss: 0.0016538938507437706\n",
      "Iteration: 10850 Training Accuracy: 0.96875 Loss: 0.0009245872497558594\n",
      "Iteration: 10860 Training Accuracy: 0.984375 Loss: 0.0006179257179610431\n",
      "Iteration: 10870 Training Accuracy: 1.0 Loss: 0.0005296031013131142\n",
      "Iteration: 10880 Training Accuracy: 1.0 Loss: 0.0002964625018648803\n",
      "Iteration: 10890 Training Accuracy: 0.984375 Loss: 0.0006482538883574307\n",
      "Iteration: 10900 Training Accuracy: 0.984375 Loss: 0.00037384178722277284\n",
      "Iteration: 10910 Training Accuracy: 0.984375 Loss: 0.00037117162719368935\n",
      "Iteration: 10920 Training Accuracy: 0.96875 Loss: 0.0014031247701495886\n",
      "Iteration: 10930 Training Accuracy: 1.0 Loss: 0.0006486332276836038\n",
      "Iteration: 10940 Training Accuracy: 1.0 Loss: 0.0002135962131433189\n",
      "Iteration: 10950 Training Accuracy: 0.96875 Loss: 0.0009383070282638073\n",
      "Iteration: 10960 Training Accuracy: 0.984375 Loss: 0.0005257207667455077\n",
      "Iteration: 10970 Training Accuracy: 1.0 Loss: 0.0005511168856173754\n",
      "Iteration: 10980 Training Accuracy: 0.984375 Loss: 0.000498417008202523\n",
      "Iteration: 10990 Training Accuracy: 1.0 Loss: 0.00023008354764897376\n",
      "Iteration: 11000 Training Accuracy: 0.953125 Loss: 0.0013102444354444742\n",
      "Iteration: 11010 Training Accuracy: 0.984375 Loss: 0.000688790634740144\n",
      "Iteration: 11020 Training Accuracy: 0.984375 Loss: 0.0014765033265575767\n",
      "Iteration: 11030 Training Accuracy: 0.96875 Loss: 0.0013728048652410507\n",
      "Iteration: 11040 Training Accuracy: 1.0 Loss: 0.00030556973069906235\n",
      "Iteration: 11050 Training Accuracy: 0.984375 Loss: 0.0008966255118139088\n",
      "Iteration: 11060 Training Accuracy: 1.0 Loss: 0.00015773325867485255\n",
      "Iteration: 11070 Training Accuracy: 1.0 Loss: 0.00046685931738466024\n",
      "Iteration: 11080 Training Accuracy: 1.0 Loss: 0.0003914611297659576\n",
      "Iteration: 11090 Training Accuracy: 1.0 Loss: 0.0007710769423283637\n",
      "Iteration: 11100 Training Accuracy: 1.0 Loss: 0.00017630810907576233\n",
      "Iteration: 11110 Training Accuracy: 0.984375 Loss: 0.0010507280239835382\n",
      "Iteration: 11120 Training Accuracy: 0.984375 Loss: 0.0005572132067754865\n",
      "Iteration: 11130 Training Accuracy: 1.0 Loss: 0.00017799838678911328\n",
      "Iteration: 11140 Training Accuracy: 1.0 Loss: 0.00030442140996456146\n",
      "Iteration: 11150 Training Accuracy: 0.96875 Loss: 0.0020186547189950943\n",
      "Iteration: 11160 Training Accuracy: 0.984375 Loss: 0.0009058714495040476\n",
      "Iteration: 11170 Training Accuracy: 0.96875 Loss: 0.0010291151702404022\n",
      "Iteration: 11180 Training Accuracy: 1.0 Loss: 0.000226871226914227\n",
      "Iteration: 11190 Training Accuracy: 0.984375 Loss: 0.0007195317884907126\n",
      "Iteration: 11200 Training Accuracy: 0.984375 Loss: 0.0008573770173825324\n",
      "Iteration: 11210 Training Accuracy: 1.0 Loss: 0.0003280930104665458\n",
      "Iteration: 11220 Training Accuracy: 0.984375 Loss: 0.0007784200133755803\n",
      "Iteration: 11230 Training Accuracy: 0.9375 Loss: 0.0022933187428861856\n",
      "Iteration: 11240 Training Accuracy: 1.0 Loss: 0.00043052586261183023\n",
      "Iteration: 11250 Training Accuracy: 1.0 Loss: 0.000491035170853138\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9223333333333333\n",
      "epoch: 12\n",
      "Iteration: 11260 Training Accuracy: 1.0 Loss: 0.001186187844723463\n",
      "Iteration: 11270 Training Accuracy: 0.984375 Loss: 0.0021022034343332052\n",
      "Iteration: 11280 Training Accuracy: 1.0 Loss: 0.00030551094096153975\n",
      "Iteration: 11290 Training Accuracy: 0.984375 Loss: 0.0011007569264620543\n",
      "Iteration: 11300 Training Accuracy: 1.0 Loss: 0.00019013551354873925\n",
      "Iteration: 11310 Training Accuracy: 0.953125 Loss: 0.002191009931266308\n",
      "Iteration: 11320 Training Accuracy: 0.984375 Loss: 0.0006806676974520087\n",
      "Iteration: 11330 Training Accuracy: 0.984375 Loss: 0.00092575594317168\n",
      "Iteration: 11340 Training Accuracy: 1.0 Loss: 0.00019385229097679257\n",
      "Iteration: 11350 Training Accuracy: 0.96875 Loss: 0.0006789493490941823\n",
      "Iteration: 11360 Training Accuracy: 1.0 Loss: 0.0006123748025856912\n",
      "Iteration: 11370 Training Accuracy: 0.953125 Loss: 0.0008688615052960813\n",
      "Iteration: 11380 Training Accuracy: 1.0 Loss: 0.0005417380016297102\n",
      "Iteration: 11390 Training Accuracy: 0.953125 Loss: 0.0015767004806548357\n",
      "Iteration: 11400 Training Accuracy: 0.984375 Loss: 0.0008240429451689124\n",
      "Iteration: 11410 Training Accuracy: 1.0 Loss: 0.00021134456619620323\n",
      "Iteration: 11420 Training Accuracy: 1.0 Loss: 0.0003942744806408882\n",
      "Iteration: 11430 Training Accuracy: 0.96875 Loss: 0.0008832762250676751\n",
      "Iteration: 11440 Training Accuracy: 0.96875 Loss: 0.001716514234431088\n",
      "Iteration: 11450 Training Accuracy: 0.984375 Loss: 0.0009648766717873514\n",
      "Iteration: 11460 Training Accuracy: 1.0 Loss: 6.32329611107707e-05\n",
      "Iteration: 11470 Training Accuracy: 0.96875 Loss: 0.0006252143648453057\n",
      "Iteration: 11480 Training Accuracy: 0.96875 Loss: 0.0011452530743554235\n",
      "Iteration: 11490 Training Accuracy: 0.984375 Loss: 0.0007687696488574147\n",
      "Iteration: 11500 Training Accuracy: 0.984375 Loss: 0.0013646278530359268\n",
      "Iteration: 11510 Training Accuracy: 1.0 Loss: 0.00021500210277736187\n",
      "Iteration: 11520 Training Accuracy: 1.0 Loss: 0.00028853066032752395\n",
      "Iteration: 11530 Training Accuracy: 1.0 Loss: 0.00017605478933546692\n",
      "Iteration: 11540 Training Accuracy: 0.953125 Loss: 0.001463519874960184\n",
      "Iteration: 11550 Training Accuracy: 0.96875 Loss: 0.001004728372208774\n",
      "Iteration: 11560 Training Accuracy: 1.0 Loss: 0.0002484091091901064\n",
      "Iteration: 11570 Training Accuracy: 0.984375 Loss: 0.0007919555064290762\n",
      "Iteration: 11580 Training Accuracy: 1.0 Loss: 0.0008013069164007902\n",
      "Iteration: 11590 Training Accuracy: 1.0 Loss: 0.00038996816147118807\n",
      "Iteration: 11600 Training Accuracy: 0.984375 Loss: 0.0012369483010843396\n",
      "Iteration: 11610 Training Accuracy: 1.0 Loss: 7.080589421093464e-05\n",
      "Iteration: 11620 Training Accuracy: 0.984375 Loss: 0.0007535695331171155\n",
      "Iteration: 11630 Training Accuracy: 0.984375 Loss: 0.000919646059628576\n",
      "Iteration: 11640 Training Accuracy: 0.984375 Loss: 0.0008918905514292419\n",
      "Iteration: 11650 Training Accuracy: 0.96875 Loss: 0.0009490410448051989\n",
      "Iteration: 11660 Training Accuracy: 1.0 Loss: 0.0001989486627280712\n",
      "Iteration: 11670 Training Accuracy: 1.0 Loss: 0.0003584770020097494\n",
      "Iteration: 11680 Training Accuracy: 1.0 Loss: 0.00017179515270981938\n",
      "Iteration: 11690 Training Accuracy: 1.0 Loss: 0.00019513291772454977\n",
      "Iteration: 11700 Training Accuracy: 0.953125 Loss: 0.002970318542793393\n",
      "Iteration: 11710 Training Accuracy: 1.0 Loss: 0.0005741263157688081\n",
      "Iteration: 11720 Training Accuracy: 0.984375 Loss: 0.0005776039906777442\n",
      "Iteration: 11730 Training Accuracy: 1.0 Loss: 0.0005279974429868162\n",
      "Iteration: 11740 Training Accuracy: 0.953125 Loss: 0.0013785089831799269\n",
      "Iteration: 11750 Training Accuracy: 1.0 Loss: 0.0003137578023597598\n",
      "Iteration: 11760 Training Accuracy: 0.953125 Loss: 0.0025681299157440662\n",
      "Iteration: 11770 Training Accuracy: 0.96875 Loss: 0.0007241531275212765\n",
      "Iteration: 11780 Training Accuracy: 0.984375 Loss: 0.0009412533836439252\n",
      "Iteration: 11790 Training Accuracy: 1.0 Loss: 0.0005794429453089833\n",
      "Iteration: 11800 Training Accuracy: 1.0 Loss: 0.00032409263076260686\n",
      "Iteration: 11810 Training Accuracy: 1.0 Loss: 0.0004751425876747817\n",
      "Iteration: 11820 Training Accuracy: 0.984375 Loss: 0.00046140654012560844\n",
      "Iteration: 11830 Training Accuracy: 0.984375 Loss: 0.0006607711547985673\n",
      "Iteration: 11840 Training Accuracy: 1.0 Loss: 1.0492229193914682e-05\n",
      "Iteration: 11850 Training Accuracy: 1.0 Loss: 0.0003392467915546149\n",
      "Iteration: 11860 Training Accuracy: 1.0 Loss: 0.00041786424117162824\n",
      "Iteration: 11870 Training Accuracy: 0.984375 Loss: 0.001077535911463201\n",
      "Iteration: 11880 Training Accuracy: 0.984375 Loss: 0.0012828155886381865\n",
      "Iteration: 11890 Training Accuracy: 0.96875 Loss: 0.0010424077045172453\n",
      "Iteration: 11900 Training Accuracy: 1.0 Loss: 0.00028270905022509396\n",
      "Iteration: 11910 Training Accuracy: 1.0 Loss: 0.0004453679721336812\n",
      "Iteration: 11920 Training Accuracy: 0.984375 Loss: 0.000689428299665451\n",
      "Iteration: 11930 Training Accuracy: 1.0 Loss: 0.0002385404077358544\n",
      "Iteration: 11940 Training Accuracy: 0.984375 Loss: 0.0004623918212018907\n",
      "Iteration: 11950 Training Accuracy: 1.0 Loss: 0.00018436199752613902\n",
      "Iteration: 11960 Training Accuracy: 0.984375 Loss: 0.0012006324250251055\n",
      "Iteration: 11970 Training Accuracy: 0.96875 Loss: 0.0011251814430579543\n",
      "Iteration: 11980 Training Accuracy: 0.953125 Loss: 0.0018570579122751951\n",
      "Iteration: 11990 Training Accuracy: 0.984375 Loss: 0.0010640752734616399\n",
      "Iteration: 12000 Training Accuracy: 0.984375 Loss: 0.0009368881001137197\n",
      "Iteration: 12010 Training Accuracy: 1.0 Loss: 0.00029399985214695334\n",
      "Iteration: 12020 Training Accuracy: 0.984375 Loss: 0.0017549173207953572\n",
      "Iteration: 12030 Training Accuracy: 0.984375 Loss: 0.0013500849017873406\n",
      "Iteration: 12040 Training Accuracy: 0.984375 Loss: 0.0011277211597189307\n",
      "Iteration: 12050 Training Accuracy: 0.984375 Loss: 0.000834707054309547\n",
      "Iteration: 12060 Training Accuracy: 0.984375 Loss: 0.0007606602739542723\n",
      "Iteration: 12070 Training Accuracy: 0.984375 Loss: 0.0006590164266526699\n",
      "Iteration: 12080 Training Accuracy: 1.0 Loss: 0.0003913095570169389\n",
      "Iteration: 12090 Training Accuracy: 0.984375 Loss: 0.0011626769555732608\n",
      "Iteration: 12100 Training Accuracy: 0.953125 Loss: 0.0012180136982351542\n",
      "Iteration: 12110 Training Accuracy: 1.0 Loss: 0.000616211153101176\n",
      "Iteration: 12120 Training Accuracy: 0.984375 Loss: 0.0009204616653732955\n",
      "Iteration: 12130 Training Accuracy: 0.96875 Loss: 0.0008678545709699392\n",
      "Iteration: 12140 Training Accuracy: 0.96875 Loss: 0.0010142619721591473\n",
      "Iteration: 12150 Training Accuracy: 0.953125 Loss: 0.0017676582792773843\n",
      "Iteration: 12160 Training Accuracy: 1.0 Loss: 0.0002939580590464175\n",
      "Iteration: 12170 Training Accuracy: 0.984375 Loss: 0.0005068572936579585\n",
      "Iteration: 12180 Training Accuracy: 1.0 Loss: 0.00067548593506217\n",
      "Iteration: 12190 Training Accuracy: 1.0 Loss: 0.00016858591698110104\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.923\n",
      "epoch: 13\n",
      "Iteration: 12200 Training Accuracy: 1.0 Loss: 0.0004575039492920041\n",
      "Iteration: 12210 Training Accuracy: 1.0 Loss: 0.00038761357427574694\n",
      "Iteration: 12220 Training Accuracy: 1.0 Loss: 0.00035849492996931076\n",
      "Iteration: 12230 Training Accuracy: 0.984375 Loss: 0.0014506189618259668\n",
      "Iteration: 12240 Training Accuracy: 1.0 Loss: 0.00033634790452197194\n",
      "Iteration: 12250 Training Accuracy: 0.984375 Loss: 0.0004991767345927656\n",
      "Iteration: 12260 Training Accuracy: 1.0 Loss: 0.0014231938403099775\n",
      "Iteration: 12270 Training Accuracy: 0.953125 Loss: 0.0022165956906974316\n",
      "Iteration: 12280 Training Accuracy: 0.96875 Loss: 0.001716085709631443\n",
      "Iteration: 12290 Training Accuracy: 0.96875 Loss: 0.0011322591453790665\n",
      "Iteration: 12300 Training Accuracy: 0.953125 Loss: 0.0017975317314267159\n",
      "Iteration: 12310 Training Accuracy: 1.0 Loss: 0.0002771130530163646\n",
      "Iteration: 12320 Training Accuracy: 0.96875 Loss: 0.0016270549967885017\n",
      "Iteration: 12330 Training Accuracy: 1.0 Loss: 0.00027767338906414807\n",
      "Iteration: 12340 Training Accuracy: 1.0 Loss: 0.0005433488404378295\n",
      "Iteration: 12350 Training Accuracy: 0.96875 Loss: 0.001458852901123464\n",
      "Iteration: 12360 Training Accuracy: 1.0 Loss: 0.00025021290639415383\n",
      "Iteration: 12370 Training Accuracy: 0.984375 Loss: 0.0007911290740594268\n",
      "Iteration: 12380 Training Accuracy: 0.984375 Loss: 0.0006965718348510563\n",
      "Iteration: 12390 Training Accuracy: 0.984375 Loss: 0.0004931185394525528\n",
      "Iteration: 12400 Training Accuracy: 0.984375 Loss: 0.0008769951527938247\n",
      "Iteration: 12410 Training Accuracy: 0.96875 Loss: 0.0015878567937761545\n",
      "Iteration: 12420 Training Accuracy: 0.984375 Loss: 0.0009682700037956238\n",
      "Iteration: 12430 Training Accuracy: 0.984375 Loss: 0.00047614233335480094\n",
      "Iteration: 12440 Training Accuracy: 1.0 Loss: 0.0006086460198275745\n",
      "Iteration: 12450 Training Accuracy: 1.0 Loss: 0.0002066727029159665\n",
      "Iteration: 12460 Training Accuracy: 1.0 Loss: 0.0006626122049055994\n",
      "Iteration: 12470 Training Accuracy: 0.984375 Loss: 0.0006923901382833719\n",
      "Iteration: 12480 Training Accuracy: 1.0 Loss: 8.98466314538382e-05\n",
      "Iteration: 12490 Training Accuracy: 1.0 Loss: 0.0005168289644643664\n",
      "Iteration: 12500 Training Accuracy: 1.0 Loss: 0.00043294118950143456\n",
      "Iteration: 12510 Training Accuracy: 0.984375 Loss: 0.000970669905655086\n",
      "Iteration: 12520 Training Accuracy: 1.0 Loss: 0.00010445991938468069\n",
      "Iteration: 12530 Training Accuracy: 0.953125 Loss: 0.0030157328583300114\n",
      "Iteration: 12540 Training Accuracy: 1.0 Loss: 0.0003221742808818817\n",
      "Iteration: 12550 Training Accuracy: 0.984375 Loss: 0.00031340186251327395\n",
      "Iteration: 12560 Training Accuracy: 1.0 Loss: 0.00022092772996984422\n",
      "Iteration: 12570 Training Accuracy: 0.984375 Loss: 0.0006591315614059567\n",
      "Iteration: 12580 Training Accuracy: 0.953125 Loss: 0.0014965186128392816\n",
      "Iteration: 12590 Training Accuracy: 1.0 Loss: 0.000314952660119161\n",
      "Iteration: 12600 Training Accuracy: 1.0 Loss: 0.0002633875992614776\n",
      "Iteration: 12610 Training Accuracy: 1.0 Loss: 0.0006414871895685792\n",
      "Iteration: 12620 Training Accuracy: 1.0 Loss: 0.000506939715705812\n",
      "Iteration: 12630 Training Accuracy: 0.984375 Loss: 0.0006004787283018231\n",
      "Iteration: 12640 Training Accuracy: 1.0 Loss: 0.00046460545854642987\n",
      "Iteration: 12650 Training Accuracy: 1.0 Loss: 0.0002788766869343817\n",
      "Iteration: 12660 Training Accuracy: 1.0 Loss: 0.0004263028677087277\n",
      "Iteration: 12670 Training Accuracy: 0.984375 Loss: 0.0008000582456588745\n",
      "Iteration: 12680 Training Accuracy: 1.0 Loss: 0.0005478180828504264\n",
      "Iteration: 12690 Training Accuracy: 0.984375 Loss: 0.0005433150799944997\n",
      "Iteration: 12700 Training Accuracy: 0.96875 Loss: 0.0022010314278304577\n",
      "Iteration: 12710 Training Accuracy: 1.0 Loss: 0.0003231147420592606\n",
      "Iteration: 12720 Training Accuracy: 0.984375 Loss: 0.0006085419445298612\n",
      "Iteration: 12730 Training Accuracy: 0.96875 Loss: 0.0011884353589266539\n",
      "Iteration: 12740 Training Accuracy: 0.984375 Loss: 0.0010944592067971826\n",
      "Iteration: 12750 Training Accuracy: 1.0 Loss: 9.487642091698945e-05\n",
      "Iteration: 12760 Training Accuracy: 1.0 Loss: 0.00011017003271263093\n",
      "Iteration: 12770 Training Accuracy: 1.0 Loss: 0.00048760930076241493\n",
      "Iteration: 12780 Training Accuracy: 1.0 Loss: 0.00043521824409253895\n",
      "Iteration: 12790 Training Accuracy: 0.984375 Loss: 0.0006508404039777815\n",
      "Iteration: 12800 Training Accuracy: 1.0 Loss: 0.0004068397101946175\n",
      "Iteration: 12810 Training Accuracy: 1.0 Loss: 0.00016353715909644961\n",
      "Iteration: 12820 Training Accuracy: 1.0 Loss: 0.0001117544888984412\n",
      "Iteration: 12830 Training Accuracy: 1.0 Loss: 0.00037745339795947075\n",
      "Iteration: 12840 Training Accuracy: 0.96875 Loss: 0.001134155667386949\n",
      "Iteration: 12850 Training Accuracy: 1.0 Loss: 0.00038707349449396133\n",
      "Iteration: 12860 Training Accuracy: 0.984375 Loss: 0.0006127302185632288\n",
      "Iteration: 12870 Training Accuracy: 1.0 Loss: 0.00046961705083958805\n",
      "Iteration: 12880 Training Accuracy: 0.984375 Loss: 0.0013360187876969576\n",
      "Iteration: 12890 Training Accuracy: 1.0 Loss: 0.0005475716898217797\n",
      "Iteration: 12900 Training Accuracy: 1.0 Loss: 0.00017914670752361417\n",
      "Iteration: 12910 Training Accuracy: 0.984375 Loss: 0.0011859445367008448\n",
      "Iteration: 12920 Training Accuracy: 0.984375 Loss: 0.00077438895823434\n",
      "Iteration: 12930 Training Accuracy: 0.953125 Loss: 0.0018153213895857334\n",
      "Iteration: 12940 Training Accuracy: 0.96875 Loss: 0.0013954425230622292\n",
      "Iteration: 12950 Training Accuracy: 0.96875 Loss: 0.0009627444087527692\n",
      "Iteration: 12960 Training Accuracy: 0.984375 Loss: 0.0005385397234931588\n",
      "Iteration: 12970 Training Accuracy: 0.984375 Loss: 0.0007267718319781125\n",
      "Iteration: 12980 Training Accuracy: 1.0 Loss: 0.0003443066671025008\n",
      "Iteration: 12990 Training Accuracy: 0.984375 Loss: 0.0003837409312836826\n",
      "Iteration: 13000 Training Accuracy: 1.0 Loss: 0.00029415980679914355\n",
      "Iteration: 13010 Training Accuracy: 0.96875 Loss: 0.001666238415054977\n",
      "Iteration: 13020 Training Accuracy: 0.984375 Loss: 0.0005243696505203843\n",
      "Iteration: 13030 Training Accuracy: 1.0 Loss: 0.0003834280651062727\n",
      "Iteration: 13040 Training Accuracy: 0.984375 Loss: 0.0005048360908403993\n",
      "Iteration: 13050 Training Accuracy: 0.953125 Loss: 0.0016528547275811434\n",
      "Iteration: 13060 Training Accuracy: 0.984375 Loss: 0.0005519362166523933\n",
      "Iteration: 13070 Training Accuracy: 1.0 Loss: 0.0003853581438306719\n",
      "Iteration: 13080 Training Accuracy: 0.96875 Loss: 0.0008943453431129456\n",
      "Iteration: 13090 Training Accuracy: 1.0 Loss: 0.00031140827923081815\n",
      "Iteration: 13100 Training Accuracy: 1.0 Loss: 0.0005590292857959867\n",
      "Iteration: 13110 Training Accuracy: 0.984375 Loss: 0.0009805236477404833\n",
      "Iteration: 13120 Training Accuracy: 1.0 Loss: 0.000504298775922507\n",
      "Iteration: 13130 Training Accuracy: 0.96875 Loss: 0.0013346591731533408\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9223333333333333\n",
      "epoch: 14\n",
      "Iteration: 13140 Training Accuracy: 1.0 Loss: 0.0003831881331279874\n",
      "Iteration: 13150 Training Accuracy: 1.0 Loss: 0.00032755930442363024\n",
      "Iteration: 13160 Training Accuracy: 0.984375 Loss: 0.0015262013766914606\n",
      "Iteration: 13170 Training Accuracy: 1.0 Loss: 0.0005294869770295918\n",
      "Iteration: 13180 Training Accuracy: 0.96875 Loss: 0.0009005244355648756\n",
      "Iteration: 13190 Training Accuracy: 1.0 Loss: 0.0002286226226715371\n",
      "Iteration: 13200 Training Accuracy: 0.96875 Loss: 0.0017120616976171732\n",
      "Iteration: 13210 Training Accuracy: 0.9375 Loss: 0.001244241138920188\n",
      "Iteration: 13220 Training Accuracy: 0.96875 Loss: 0.001491455128416419\n",
      "Iteration: 13230 Training Accuracy: 1.0 Loss: 0.00032463070238009095\n",
      "Iteration: 13240 Training Accuracy: 0.984375 Loss: 0.0007310651708394289\n",
      "Iteration: 13250 Training Accuracy: 0.96875 Loss: 0.0012647353578358889\n",
      "Iteration: 13260 Training Accuracy: 1.0 Loss: 0.00019473709107842296\n",
      "Iteration: 13270 Training Accuracy: 0.984375 Loss: 0.0013975468464195728\n",
      "Iteration: 13280 Training Accuracy: 1.0 Loss: 0.0008079071994870901\n",
      "Iteration: 13290 Training Accuracy: 0.984375 Loss: 0.0006523472256958485\n",
      "Iteration: 13300 Training Accuracy: 0.96875 Loss: 0.0007142624235711992\n",
      "Iteration: 13310 Training Accuracy: 0.984375 Loss: 0.000718685972969979\n",
      "Iteration: 13320 Training Accuracy: 1.0 Loss: 0.00044630406773649156\n",
      "Iteration: 13330 Training Accuracy: 1.0 Loss: 0.00030684712692163885\n",
      "Iteration: 13340 Training Accuracy: 1.0 Loss: 0.0001266814215341583\n",
      "Iteration: 13350 Training Accuracy: 0.984375 Loss: 0.0006616438622586429\n",
      "Iteration: 13360 Training Accuracy: 0.96875 Loss: 0.001108080497942865\n",
      "Iteration: 13370 Training Accuracy: 0.984375 Loss: 0.0008496735827066004\n",
      "Iteration: 13380 Training Accuracy: 0.96875 Loss: 0.0010570910526439548\n",
      "Iteration: 13390 Training Accuracy: 0.96875 Loss: 0.0006300067761912942\n",
      "Iteration: 13400 Training Accuracy: 0.984375 Loss: 0.0014142933068796992\n",
      "Iteration: 13410 Training Accuracy: 0.96875 Loss: 0.0007701715803705156\n",
      "Iteration: 13420 Training Accuracy: 0.984375 Loss: 0.0007198007660917938\n",
      "Iteration: 13430 Training Accuracy: 1.0 Loss: 0.00037679943488910794\n",
      "Iteration: 13440 Training Accuracy: 1.0 Loss: 0.0004000070330221206\n",
      "Iteration: 13450 Training Accuracy: 0.96875 Loss: 0.0025858236476778984\n",
      "Iteration: 13460 Training Accuracy: 1.0 Loss: 0.00043876978452317417\n",
      "Iteration: 13470 Training Accuracy: 1.0 Loss: 6.961879262235016e-05\n",
      "Iteration: 13480 Training Accuracy: 0.984375 Loss: 0.00031331414356827736\n",
      "Iteration: 13490 Training Accuracy: 1.0 Loss: 0.0003839655255433172\n",
      "Iteration: 13500 Training Accuracy: 0.96875 Loss: 0.0011846852721646428\n",
      "Iteration: 13510 Training Accuracy: 0.984375 Loss: 0.0007232954376377165\n",
      "Iteration: 13520 Training Accuracy: 0.984375 Loss: 0.0007685875752940774\n",
      "Iteration: 13530 Training Accuracy: 0.984375 Loss: 0.0008344952948391438\n",
      "Iteration: 13540 Training Accuracy: 0.984375 Loss: 0.0010112497257068753\n",
      "Iteration: 13550 Training Accuracy: 1.0 Loss: 8.21296707727015e-05\n",
      "Iteration: 13560 Training Accuracy: 0.984375 Loss: 0.0013283557491376996\n",
      "Iteration: 13570 Training Accuracy: 1.0 Loss: 0.0006254836916923523\n",
      "Iteration: 13580 Training Accuracy: 0.96875 Loss: 0.0009687721612863243\n",
      "Iteration: 13590 Training Accuracy: 1.0 Loss: 9.712576138554141e-05\n",
      "Iteration: 13600 Training Accuracy: 1.0 Loss: 0.0004477160400711\n",
      "Iteration: 13610 Training Accuracy: 0.984375 Loss: 0.0007419020403176546\n",
      "Iteration: 13620 Training Accuracy: 0.984375 Loss: 0.0011898516677320004\n",
      "Iteration: 13630 Training Accuracy: 1.0 Loss: 0.0002836067578755319\n",
      "Iteration: 13640 Training Accuracy: 0.984375 Loss: 0.0006548885139636695\n",
      "Iteration: 13650 Training Accuracy: 1.0 Loss: 0.0003822816361207515\n",
      "Iteration: 13660 Training Accuracy: 1.0 Loss: 0.0004903216613456607\n",
      "Iteration: 13670 Training Accuracy: 0.96875 Loss: 0.0012631507124751806\n",
      "Iteration: 13680 Training Accuracy: 0.984375 Loss: 0.0013589912559837103\n",
      "Iteration: 13690 Training Accuracy: 0.984375 Loss: 0.0010040471097454429\n",
      "Iteration: 13700 Training Accuracy: 1.0 Loss: 0.00030447315657511353\n",
      "Iteration: 13710 Training Accuracy: 0.984375 Loss: 0.0006779725663363934\n",
      "Iteration: 13720 Training Accuracy: 1.0 Loss: 0.0005925076548010111\n",
      "Iteration: 13730 Training Accuracy: 1.0 Loss: 0.0005382878007367253\n",
      "Iteration: 13740 Training Accuracy: 1.0 Loss: 0.00037524764775298536\n",
      "Iteration: 13750 Training Accuracy: 1.0 Loss: 0.0002852227189578116\n",
      "Iteration: 13760 Training Accuracy: 1.0 Loss: 0.00037035602144896984\n",
      "Iteration: 13770 Training Accuracy: 0.984375 Loss: 0.0011351723223924637\n",
      "Iteration: 13780 Training Accuracy: 1.0 Loss: 0.00010691964416764677\n",
      "Iteration: 13790 Training Accuracy: 0.96875 Loss: 0.0011678134324029088\n",
      "Iteration: 13800 Training Accuracy: 0.96875 Loss: 0.001974650425836444\n",
      "Iteration: 13810 Training Accuracy: 1.0 Loss: 8.128317131195217e-05\n",
      "Iteration: 13820 Training Accuracy: 1.0 Loss: 0.00027878154651261866\n",
      "Iteration: 13830 Training Accuracy: 0.984375 Loss: 0.0002753834705799818\n",
      "Iteration: 13840 Training Accuracy: 0.984375 Loss: 0.0004521281807683408\n",
      "Iteration: 13850 Training Accuracy: 0.984375 Loss: 0.0007042047800496221\n",
      "Iteration: 13860 Training Accuracy: 1.0 Loss: 0.00038888584822416306\n",
      "Iteration: 13870 Training Accuracy: 0.984375 Loss: 0.0012335566570982337\n",
      "Iteration: 13880 Training Accuracy: 0.96875 Loss: 0.0008494151988998055\n",
      "Iteration: 13890 Training Accuracy: 1.0 Loss: 0.0006351001793518662\n",
      "Iteration: 13900 Training Accuracy: 0.984375 Loss: 0.0009743347764015198\n",
      "Iteration: 13910 Training Accuracy: 0.96875 Loss: 0.0012498634168878198\n",
      "Iteration: 13920 Training Accuracy: 0.984375 Loss: 0.0007223470602184534\n",
      "Iteration: 13930 Training Accuracy: 1.0 Loss: 0.0005474460776895285\n",
      "Iteration: 13940 Training Accuracy: 1.0 Loss: 0.0005440945969894528\n",
      "Iteration: 13950 Training Accuracy: 0.96875 Loss: 0.0009339430835098028\n",
      "Iteration: 13960 Training Accuracy: 1.0 Loss: 4.9933230911847204e-05\n",
      "Iteration: 13970 Training Accuracy: 1.0 Loss: 0.0002145062608178705\n",
      "Iteration: 13980 Training Accuracy: 0.984375 Loss: 0.0011290563270449638\n",
      "Iteration: 13990 Training Accuracy: 0.984375 Loss: 0.0009909794898703694\n",
      "Iteration: 14000 Training Accuracy: 0.984375 Loss: 0.0005980051937513053\n",
      "Iteration: 14010 Training Accuracy: 0.984375 Loss: 0.0006440585129894316\n",
      "Iteration: 14020 Training Accuracy: 0.984375 Loss: 0.000984875252470374\n",
      "Iteration: 14030 Training Accuracy: 1.0 Loss: 0.00016186512948479503\n",
      "Iteration: 14040 Training Accuracy: 1.0 Loss: 0.0001950035511981696\n",
      "Iteration: 14050 Training Accuracy: 1.0 Loss: 0.0005056122899986804\n",
      "Iteration: 14060 Training Accuracy: 0.984375 Loss: 0.0006362776039168239\n",
      "Iteration: 14070 Training Accuracy: 1.0 Loss: 0.00016613675688859075\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9248333333333333\n",
      "epoch: 15\n",
      "Iteration: 14080 Training Accuracy: 0.953125 Loss: 0.00141505419742316\n",
      "Iteration: 14090 Training Accuracy: 1.0 Loss: 0.00018543897022027522\n",
      "Iteration: 14100 Training Accuracy: 0.96875 Loss: 0.0013235552469268441\n",
      "Iteration: 14110 Training Accuracy: 0.96875 Loss: 0.0008846306591294706\n",
      "Iteration: 14120 Training Accuracy: 0.984375 Loss: 0.0011134627275168896\n",
      "Iteration: 14130 Training Accuracy: 1.0 Loss: 0.0007262258441187441\n",
      "Iteration: 14140 Training Accuracy: 0.984375 Loss: 0.0004322077729739249\n",
      "Iteration: 14150 Training Accuracy: 0.984375 Loss: 0.0009315205970779061\n",
      "Iteration: 14160 Training Accuracy: 1.0 Loss: 0.00038343147025443614\n",
      "Iteration: 14170 Training Accuracy: 1.0 Loss: 0.0005785029497928917\n",
      "Iteration: 14180 Training Accuracy: 1.0 Loss: 0.00027583789778873324\n",
      "Iteration: 14190 Training Accuracy: 1.0 Loss: 0.00044620566768571734\n",
      "Iteration: 14200 Training Accuracy: 0.96875 Loss: 0.0013550079893320799\n",
      "Iteration: 14210 Training Accuracy: 0.984375 Loss: 0.0016983039677143097\n",
      "Iteration: 14220 Training Accuracy: 1.0 Loss: 0.0006590343546122313\n",
      "Iteration: 14230 Training Accuracy: 1.0 Loss: 0.00015103637997526675\n",
      "Iteration: 14240 Training Accuracy: 0.984375 Loss: 0.0012584456708282232\n",
      "Iteration: 14250 Training Accuracy: 0.984375 Loss: 0.0012812630739063025\n",
      "Iteration: 14260 Training Accuracy: 1.0 Loss: 0.0003208025300409645\n",
      "Iteration: 14270 Training Accuracy: 0.953125 Loss: 0.0021089031361043453\n",
      "Iteration: 14280 Training Accuracy: 1.0 Loss: 0.00026401091599836946\n",
      "Iteration: 14290 Training Accuracy: 1.0 Loss: 0.0005024484707973897\n",
      "Iteration: 14300 Training Accuracy: 0.984375 Loss: 0.0012410904746502638\n",
      "Iteration: 14310 Training Accuracy: 0.984375 Loss: 0.001112898695282638\n",
      "Iteration: 14320 Training Accuracy: 1.0 Loss: 0.0004867333918809891\n",
      "Iteration: 14330 Training Accuracy: 1.0 Loss: 0.0001948175922734663\n",
      "Iteration: 14340 Training Accuracy: 1.0 Loss: 0.00015794934006407857\n",
      "Iteration: 14350 Training Accuracy: 0.984375 Loss: 0.0007610960747115314\n",
      "Iteration: 14360 Training Accuracy: 1.0 Loss: 0.0004146243736613542\n",
      "Iteration: 14370 Training Accuracy: 0.96875 Loss: 0.0011702592018991709\n",
      "Iteration: 14380 Training Accuracy: 0.96875 Loss: 0.0032960898242890835\n",
      "Iteration: 14390 Training Accuracy: 0.984375 Loss: 0.0008509786566719413\n",
      "Iteration: 14400 Training Accuracy: 0.984375 Loss: 0.0006321773980744183\n",
      "Iteration: 14410 Training Accuracy: 1.0 Loss: 9.114733256865293e-05\n",
      "Iteration: 14420 Training Accuracy: 1.0 Loss: 0.00033894041553139687\n",
      "Iteration: 14430 Training Accuracy: 0.984375 Loss: 0.00037084551877342165\n",
      "Iteration: 14440 Training Accuracy: 1.0 Loss: 0.0006106734508648515\n",
      "Iteration: 14450 Training Accuracy: 1.0 Loss: 0.000823474139906466\n",
      "Iteration: 14460 Training Accuracy: 0.984375 Loss: 0.0009797830134630203\n",
      "Iteration: 14470 Training Accuracy: 0.984375 Loss: 0.0010507525876164436\n",
      "Iteration: 14480 Training Accuracy: 0.984375 Loss: 0.0003287454310338944\n",
      "Iteration: 14490 Training Accuracy: 0.984375 Loss: 0.0008178635034710169\n",
      "Iteration: 14500 Training Accuracy: 0.984375 Loss: 0.0007529215072281659\n",
      "Iteration: 14510 Training Accuracy: 0.984375 Loss: 0.0005220277816988528\n",
      "Iteration: 14520 Training Accuracy: 0.96875 Loss: 0.000993056339211762\n",
      "Iteration: 14530 Training Accuracy: 0.96875 Loss: 0.000695343129336834\n",
      "Iteration: 14540 Training Accuracy: 1.0 Loss: 0.0003772523195948452\n",
      "Iteration: 14550 Training Accuracy: 1.0 Loss: 0.0005408156430348754\n",
      "Iteration: 14560 Training Accuracy: 1.0 Loss: 0.00023916222562547773\n",
      "Iteration: 14570 Training Accuracy: 0.984375 Loss: 0.0010936891194432974\n",
      "Iteration: 14580 Training Accuracy: 1.0 Loss: 0.00021391798509284854\n",
      "Iteration: 14590 Training Accuracy: 1.0 Loss: 0.00011763216753024608\n",
      "Iteration: 14600 Training Accuracy: 1.0 Loss: 0.00020949092868249863\n",
      "Iteration: 14610 Training Accuracy: 1.0 Loss: 0.00020806555403396487\n",
      "Iteration: 14620 Training Accuracy: 1.0 Loss: 0.00041948124999180436\n",
      "Iteration: 14630 Training Accuracy: 0.984375 Loss: 0.00037723808782175183\n",
      "Iteration: 14640 Training Accuracy: 0.96875 Loss: 0.0013880475889891386\n",
      "Iteration: 14650 Training Accuracy: 1.0 Loss: 0.0004249606281518936\n",
      "Iteration: 14660 Training Accuracy: 0.984375 Loss: 0.0004380806640256196\n",
      "Iteration: 14670 Training Accuracy: 0.984375 Loss: 0.00046482926700264215\n",
      "Iteration: 14680 Training Accuracy: 1.0 Loss: 0.00016696294187568128\n",
      "Iteration: 14690 Training Accuracy: 1.0 Loss: 0.0008183180470950902\n",
      "Iteration: 14700 Training Accuracy: 1.0 Loss: 0.00015588276437483728\n",
      "Iteration: 14710 Training Accuracy: 1.0 Loss: 0.00018534516857471317\n",
      "Iteration: 14720 Training Accuracy: 1.0 Loss: 0.0002548511838540435\n",
      "Iteration: 14730 Training Accuracy: 0.96875 Loss: 0.0011275720316916704\n",
      "Iteration: 14740 Training Accuracy: 1.0 Loss: 0.0003398173430468887\n",
      "Iteration: 14750 Training Accuracy: 0.984375 Loss: 0.0012143761850893497\n",
      "Iteration: 14760 Training Accuracy: 1.0 Loss: 0.00017057271907106042\n",
      "Iteration: 14770 Training Accuracy: 1.0 Loss: 0.0004603896813932806\n",
      "Iteration: 14780 Training Accuracy: 0.984375 Loss: 0.00027134266565553844\n",
      "Iteration: 14790 Training Accuracy: 0.984375 Loss: 0.0008253785781562328\n",
      "Iteration: 14800 Training Accuracy: 0.984375 Loss: 0.0014011766761541367\n",
      "Iteration: 14810 Training Accuracy: 1.0 Loss: 0.00030942377634346485\n",
      "Iteration: 14820 Training Accuracy: 0.96875 Loss: 0.0010326707269996405\n",
      "Iteration: 14830 Training Accuracy: 0.984375 Loss: 0.0011168557684868574\n",
      "Iteration: 14840 Training Accuracy: 1.0 Loss: 0.0005955045344308019\n",
      "Iteration: 14850 Training Accuracy: 0.96875 Loss: 0.0014934290666133165\n",
      "Iteration: 14860 Training Accuracy: 1.0 Loss: 7.952607847983018e-05\n",
      "Iteration: 14870 Training Accuracy: 0.96875 Loss: 0.0015387614257633686\n",
      "Iteration: 14880 Training Accuracy: 0.984375 Loss: 0.0009427216136828065\n",
      "Iteration: 14890 Training Accuracy: 1.0 Loss: 0.0004356564022600651\n",
      "Iteration: 14900 Training Accuracy: 0.96875 Loss: 0.0008166814222931862\n",
      "Iteration: 14910 Training Accuracy: 0.953125 Loss: 0.0016172213945537806\n",
      "Iteration: 14920 Training Accuracy: 0.984375 Loss: 0.0008982247090898454\n",
      "Iteration: 14930 Training Accuracy: 1.0 Loss: 9.946198406396434e-05\n",
      "Iteration: 14940 Training Accuracy: 1.0 Loss: 0.00017183659656438977\n",
      "Iteration: 14950 Training Accuracy: 1.0 Loss: 0.0003335927031002939\n",
      "Iteration: 14960 Training Accuracy: 1.0 Loss: 0.00047323512262664735\n",
      "Iteration: 14970 Training Accuracy: 1.0 Loss: 0.00047537399223074317\n",
      "Iteration: 14980 Training Accuracy: 0.984375 Loss: 0.0007754809339530766\n",
      "Iteration: 14990 Training Accuracy: 1.0 Loss: 0.00039864558493718505\n",
      "Iteration: 15000 Training Accuracy: 0.984375 Loss: 0.0006669983267784119\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9245\n",
      "epoch: 16\n",
      "Iteration: 15010 Training Accuracy: 1.0 Loss: 0.0004891910357400775\n",
      "Iteration: 15020 Training Accuracy: 0.984375 Loss: 0.0013982176315039396\n",
      "Iteration: 15030 Training Accuracy: 0.984375 Loss: 0.0007818362792022526\n",
      "Iteration: 15040 Training Accuracy: 0.984375 Loss: 0.0004991379100829363\n",
      "Iteration: 15050 Training Accuracy: 0.984375 Loss: 0.0010157128563150764\n",
      "Iteration: 15060 Training Accuracy: 0.96875 Loss: 0.0006845778552815318\n",
      "Iteration: 15070 Training Accuracy: 1.0 Loss: 0.0001862502540461719\n",
      "Iteration: 15080 Training Accuracy: 0.984375 Loss: 0.000725459074601531\n",
      "Iteration: 15090 Training Accuracy: 1.0 Loss: 3.4372424124740064e-05\n",
      "Iteration: 15100 Training Accuracy: 0.96875 Loss: 0.000581827131099999\n",
      "Iteration: 15110 Training Accuracy: 0.953125 Loss: 0.002026807051151991\n",
      "Iteration: 15120 Training Accuracy: 1.0 Loss: 0.00026060082018375397\n",
      "Iteration: 15130 Training Accuracy: 1.0 Loss: 0.00015968560182955116\n",
      "Iteration: 15140 Training Accuracy: 0.984375 Loss: 0.001898713642731309\n",
      "Iteration: 15150 Training Accuracy: 1.0 Loss: 0.00023183412849903107\n",
      "Iteration: 15160 Training Accuracy: 1.0 Loss: 0.0007813178235664964\n",
      "Iteration: 15170 Training Accuracy: 1.0 Loss: 0.0004539910005405545\n",
      "Iteration: 15180 Training Accuracy: 0.984375 Loss: 0.0005283149657770991\n",
      "Iteration: 15190 Training Accuracy: 0.984375 Loss: 0.0011513695353642106\n",
      "Iteration: 15200 Training Accuracy: 1.0 Loss: 0.00037318147951737046\n",
      "Iteration: 15210 Training Accuracy: 1.0 Loss: 0.00031007398501969874\n",
      "Iteration: 15220 Training Accuracy: 1.0 Loss: 0.00020473168115131557\n",
      "Iteration: 15230 Training Accuracy: 0.984375 Loss: 0.0011771395802497864\n",
      "Iteration: 15240 Training Accuracy: 0.984375 Loss: 0.0019366134656593204\n",
      "Iteration: 15250 Training Accuracy: 1.0 Loss: 0.00018038629787042737\n",
      "Iteration: 15260 Training Accuracy: 0.984375 Loss: 0.0007352210814133286\n",
      "Iteration: 15270 Training Accuracy: 0.984375 Loss: 0.00069860287476331\n",
      "Iteration: 15280 Training Accuracy: 0.984375 Loss: 0.0010348830837756395\n",
      "Iteration: 15290 Training Accuracy: 0.984375 Loss: 0.0011265355860814452\n",
      "Iteration: 15300 Training Accuracy: 1.0 Loss: 0.0003059969749301672\n",
      "Iteration: 15310 Training Accuracy: 1.0 Loss: 0.00031072343699634075\n",
      "Iteration: 15320 Training Accuracy: 0.984375 Loss: 0.0006599098560400307\n",
      "Iteration: 15330 Training Accuracy: 0.953125 Loss: 0.0010474846931174397\n",
      "Iteration: 15340 Training Accuracy: 1.0 Loss: 0.00033731330768205225\n",
      "Iteration: 15350 Training Accuracy: 1.0 Loss: 0.0002664065104909241\n",
      "Iteration: 15360 Training Accuracy: 0.984375 Loss: 0.0006251180311664939\n",
      "Iteration: 15370 Training Accuracy: 1.0 Loss: 0.0001982483227038756\n",
      "Iteration: 15380 Training Accuracy: 1.0 Loss: 0.00022345938486978412\n",
      "Iteration: 15390 Training Accuracy: 0.96875 Loss: 0.0016580645460635424\n",
      "Iteration: 15400 Training Accuracy: 1.0 Loss: 0.0005404924741014838\n",
      "Iteration: 15410 Training Accuracy: 0.96875 Loss: 0.0011040725512430072\n",
      "Iteration: 15420 Training Accuracy: 0.921875 Loss: 0.0020126886665821075\n",
      "Iteration: 15430 Training Accuracy: 1.0 Loss: 0.00046661958913318813\n",
      "Iteration: 15440 Training Accuracy: 1.0 Loss: 0.00023364461958408356\n",
      "Iteration: 15450 Training Accuracy: 1.0 Loss: 0.0003267601423431188\n",
      "Iteration: 15460 Training Accuracy: 0.984375 Loss: 0.0005251272232271731\n",
      "Iteration: 15470 Training Accuracy: 0.984375 Loss: 0.0004252948274370283\n",
      "Iteration: 15480 Training Accuracy: 1.0 Loss: 6.754678179277107e-05\n",
      "Iteration: 15490 Training Accuracy: 0.984375 Loss: 0.00034166889963671565\n",
      "Iteration: 15500 Training Accuracy: 0.984375 Loss: 0.00055782898562029\n",
      "Iteration: 15510 Training Accuracy: 0.984375 Loss: 0.0012573582353070378\n",
      "Iteration: 15520 Training Accuracy: 0.96875 Loss: 0.0009886269690468907\n",
      "Iteration: 15530 Training Accuracy: 0.96875 Loss: 0.001016617170535028\n",
      "Iteration: 15540 Training Accuracy: 0.96875 Loss: 0.0007165619172155857\n",
      "Iteration: 15550 Training Accuracy: 1.0 Loss: 0.0003558916214387864\n",
      "Iteration: 15560 Training Accuracy: 1.0 Loss: 0.000369958026567474\n",
      "Iteration: 15570 Training Accuracy: 1.0 Loss: 0.00016280775889754295\n",
      "Iteration: 15580 Training Accuracy: 0.984375 Loss: 0.0005038962117396295\n",
      "Iteration: 15590 Training Accuracy: 0.984375 Loss: 0.00028079719049856067\n",
      "Iteration: 15600 Training Accuracy: 1.0 Loss: 0.00026791688287630677\n",
      "Iteration: 15610 Training Accuracy: 0.96875 Loss: 0.001207113265991211\n",
      "Iteration: 15620 Training Accuracy: 1.0 Loss: 0.0004517461347859353\n",
      "Iteration: 15630 Training Accuracy: 1.0 Loss: 0.00014256517169997096\n",
      "Iteration: 15640 Training Accuracy: 1.0 Loss: 0.0005527842440642416\n",
      "Iteration: 15650 Training Accuracy: 1.0 Loss: 0.0003632781736087054\n",
      "Iteration: 15660 Training Accuracy: 1.0 Loss: 0.0002954840019810945\n",
      "Iteration: 15670 Training Accuracy: 1.0 Loss: 0.0003165131784044206\n",
      "Iteration: 15680 Training Accuracy: 1.0 Loss: 0.00010999824735336006\n",
      "Iteration: 15690 Training Accuracy: 0.953125 Loss: 0.0010477013420313597\n",
      "Iteration: 15700 Training Accuracy: 0.984375 Loss: 0.0005476113874465227\n",
      "Iteration: 15710 Training Accuracy: 0.984375 Loss: 0.0014477417571470141\n",
      "Iteration: 15720 Training Accuracy: 0.984375 Loss: 0.0009394497028551996\n",
      "Iteration: 15730 Training Accuracy: 1.0 Loss: 0.00022257196542341262\n",
      "Iteration: 15740 Training Accuracy: 0.984375 Loss: 0.00047723689931444824\n",
      "Iteration: 15750 Training Accuracy: 1.0 Loss: 0.0001213613068102859\n",
      "Iteration: 15760 Training Accuracy: 1.0 Loss: 0.00034977926407009363\n",
      "Iteration: 15770 Training Accuracy: 1.0 Loss: 0.0002202020405093208\n",
      "Iteration: 15780 Training Accuracy: 1.0 Loss: 0.0006317195948213339\n",
      "Iteration: 15790 Training Accuracy: 1.0 Loss: 0.0001391828991472721\n",
      "Iteration: 15800 Training Accuracy: 0.984375 Loss: 0.0009906854247674346\n",
      "Iteration: 15810 Training Accuracy: 1.0 Loss: 0.0003370358026586473\n",
      "Iteration: 15820 Training Accuracy: 1.0 Loss: 0.0001662680588196963\n",
      "Iteration: 15830 Training Accuracy: 1.0 Loss: 0.00028499215841293335\n",
      "Iteration: 15840 Training Accuracy: 0.96875 Loss: 0.0013485094532370567\n",
      "Iteration: 15850 Training Accuracy: 0.984375 Loss: 0.0007283554878085852\n",
      "Iteration: 15860 Training Accuracy: 1.0 Loss: 0.0006037438288331032\n",
      "Iteration: 15870 Training Accuracy: 1.0 Loss: 0.0002481190604157746\n",
      "Iteration: 15880 Training Accuracy: 1.0 Loss: 0.0004620863182935864\n",
      "Iteration: 15890 Training Accuracy: 0.984375 Loss: 0.0005978071130812168\n",
      "Iteration: 15900 Training Accuracy: 1.0 Loss: 0.00019407481886446476\n",
      "Iteration: 15910 Training Accuracy: 1.0 Loss: 0.00038525761920027435\n",
      "Iteration: 15920 Training Accuracy: 0.953125 Loss: 0.0018992458935827017\n",
      "Iteration: 15930 Training Accuracy: 1.0 Loss: 0.0002832503814715892\n",
      "Iteration: 15940 Training Accuracy: 1.0 Loss: 0.0003649437567219138\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9245\n",
      "epoch: 17\n",
      "Iteration: 15950 Training Accuracy: 0.984375 Loss: 0.0008017008658498526\n",
      "Iteration: 15960 Training Accuracy: 0.984375 Loss: 0.001435992307960987\n",
      "Iteration: 15970 Training Accuracy: 1.0 Loss: 0.00019164236437063664\n",
      "Iteration: 15980 Training Accuracy: 0.984375 Loss: 0.0009098383598029613\n",
      "Iteration: 15990 Training Accuracy: 1.0 Loss: 0.00014041110989637673\n",
      "Iteration: 16000 Training Accuracy: 0.96875 Loss: 0.0018621142953634262\n",
      "Iteration: 16010 Training Accuracy: 1.0 Loss: 0.0004725903272628784\n",
      "Iteration: 16020 Training Accuracy: 1.0 Loss: 0.0005892254994250834\n",
      "Iteration: 16030 Training Accuracy: 1.0 Loss: 0.00012416858226060867\n",
      "Iteration: 16040 Training Accuracy: 0.96875 Loss: 0.0006758464151062071\n",
      "Iteration: 16050 Training Accuracy: 1.0 Loss: 0.0006104550557211041\n",
      "Iteration: 16060 Training Accuracy: 0.984375 Loss: 0.0006596380262635648\n",
      "Iteration: 16070 Training Accuracy: 1.0 Loss: 0.00042420002864673734\n",
      "Iteration: 16080 Training Accuracy: 0.96875 Loss: 0.0013835113495588303\n",
      "Iteration: 16090 Training Accuracy: 0.984375 Loss: 0.0007834666175767779\n",
      "Iteration: 16100 Training Accuracy: 1.0 Loss: 0.0002598839346319437\n",
      "Iteration: 16110 Training Accuracy: 1.0 Loss: 0.0002491460181772709\n",
      "Iteration: 16120 Training Accuracy: 0.96875 Loss: 0.000735960784368217\n",
      "Iteration: 16130 Training Accuracy: 0.96875 Loss: 0.001312614418566227\n",
      "Iteration: 16140 Training Accuracy: 0.984375 Loss: 0.0007898154435679317\n",
      "Iteration: 16150 Training Accuracy: 1.0 Loss: 2.5976247343351133e-05\n",
      "Iteration: 16160 Training Accuracy: 0.984375 Loss: 0.0005913283093832433\n",
      "Iteration: 16170 Training Accuracy: 0.953125 Loss: 0.001036628964357078\n",
      "Iteration: 16180 Training Accuracy: 1.0 Loss: 0.00044158651144243777\n",
      "Iteration: 16190 Training Accuracy: 0.984375 Loss: 0.0012837606482207775\n",
      "Iteration: 16200 Training Accuracy: 1.0 Loss: 0.00015834407531656325\n",
      "Iteration: 16210 Training Accuracy: 1.0 Loss: 0.00020535480871330947\n",
      "Iteration: 16220 Training Accuracy: 1.0 Loss: 0.0002166010090149939\n",
      "Iteration: 16230 Training Accuracy: 0.96875 Loss: 0.0009007459739223123\n",
      "Iteration: 16240 Training Accuracy: 0.984375 Loss: 0.0007145461859181523\n",
      "Iteration: 16250 Training Accuracy: 1.0 Loss: 0.0002276638406328857\n",
      "Iteration: 16260 Training Accuracy: 0.96875 Loss: 0.0008105632150545716\n",
      "Iteration: 16270 Training Accuracy: 1.0 Loss: 0.0006243048119358718\n",
      "Iteration: 16280 Training Accuracy: 1.0 Loss: 0.000262713641859591\n",
      "Iteration: 16290 Training Accuracy: 0.984375 Loss: 0.0009369619656354189\n",
      "Iteration: 16300 Training Accuracy: 1.0 Loss: 5.182305903872475e-05\n",
      "Iteration: 16310 Training Accuracy: 0.984375 Loss: 0.0006392862414941192\n",
      "Iteration: 16320 Training Accuracy: 0.984375 Loss: 0.0006192671717144549\n",
      "Iteration: 16330 Training Accuracy: 0.984375 Loss: 0.0005775324534624815\n",
      "Iteration: 16340 Training Accuracy: 0.984375 Loss: 0.0006392609211616218\n",
      "Iteration: 16350 Training Accuracy: 1.0 Loss: 0.00016537662304472178\n",
      "Iteration: 16360 Training Accuracy: 1.0 Loss: 0.0003518137091305107\n",
      "Iteration: 16370 Training Accuracy: 1.0 Loss: 0.0001148048831964843\n",
      "Iteration: 16380 Training Accuracy: 1.0 Loss: 0.00014888474834151566\n",
      "Iteration: 16390 Training Accuracy: 0.984375 Loss: 0.002548995427787304\n",
      "Iteration: 16400 Training Accuracy: 1.0 Loss: 0.0003197910846211016\n",
      "Iteration: 16410 Training Accuracy: 1.0 Loss: 0.00045125908218324184\n",
      "Iteration: 16420 Training Accuracy: 1.0 Loss: 0.00042171822860836983\n",
      "Iteration: 16430 Training Accuracy: 0.96875 Loss: 0.0012583453208208084\n",
      "Iteration: 16440 Training Accuracy: 1.0 Loss: 0.00021713285241276026\n",
      "Iteration: 16450 Training Accuracy: 0.953125 Loss: 0.0023895089980214834\n",
      "Iteration: 16460 Training Accuracy: 0.984375 Loss: 0.0006287246360443532\n",
      "Iteration: 16470 Training Accuracy: 0.984375 Loss: 0.0006506541976705194\n",
      "Iteration: 16480 Training Accuracy: 1.0 Loss: 0.00047662213910371065\n",
      "Iteration: 16490 Training Accuracy: 1.0 Loss: 0.00018823369464371353\n",
      "Iteration: 16500 Training Accuracy: 1.0 Loss: 0.0004196545050945133\n",
      "Iteration: 16510 Training Accuracy: 1.0 Loss: 0.0003392304352018982\n",
      "Iteration: 16520 Training Accuracy: 0.984375 Loss: 0.0005012518377043307\n",
      "Iteration: 16530 Training Accuracy: 1.0 Loss: 7.418099357892061e-06\n",
      "Iteration: 16540 Training Accuracy: 1.0 Loss: 0.00031417515128850937\n",
      "Iteration: 16550 Training Accuracy: 1.0 Loss: 0.0003545148647390306\n",
      "Iteration: 16560 Training Accuracy: 0.984375 Loss: 0.0009858653647825122\n",
      "Iteration: 16570 Training Accuracy: 0.984375 Loss: 0.0008177175768651068\n",
      "Iteration: 16580 Training Accuracy: 0.984375 Loss: 0.000686156505253166\n",
      "Iteration: 16590 Training Accuracy: 1.0 Loss: 0.00020720453176181763\n",
      "Iteration: 16600 Training Accuracy: 1.0 Loss: 0.00030605250503867865\n",
      "Iteration: 16610 Training Accuracy: 0.984375 Loss: 0.0005668432568199933\n",
      "Iteration: 16620 Training Accuracy: 1.0 Loss: 0.00011696861474774778\n",
      "Iteration: 16630 Training Accuracy: 0.984375 Loss: 0.0004257720138411969\n",
      "Iteration: 16640 Training Accuracy: 1.0 Loss: 0.0001419909967808053\n",
      "Iteration: 16650 Training Accuracy: 0.984375 Loss: 0.0009454037644900382\n",
      "Iteration: 16660 Training Accuracy: 0.96875 Loss: 0.0008091739728115499\n",
      "Iteration: 16670 Training Accuracy: 0.984375 Loss: 0.0012510360684245825\n",
      "Iteration: 16680 Training Accuracy: 1.0 Loss: 0.0006628414848819375\n",
      "Iteration: 16690 Training Accuracy: 0.984375 Loss: 0.0008034845814108849\n",
      "Iteration: 16700 Training Accuracy: 1.0 Loss: 0.00029261360759846866\n",
      "Iteration: 16710 Training Accuracy: 0.984375 Loss: 0.0016860193572938442\n",
      "Iteration: 16720 Training Accuracy: 0.984375 Loss: 0.0011548439506441355\n",
      "Iteration: 16730 Training Accuracy: 1.0 Loss: 0.0006729074520990252\n",
      "Iteration: 16740 Training Accuracy: 0.984375 Loss: 0.0007000486366450787\n",
      "Iteration: 16750 Training Accuracy: 0.984375 Loss: 0.0005669785896316171\n",
      "Iteration: 16760 Training Accuracy: 1.0 Loss: 0.00031286131707020104\n",
      "Iteration: 16770 Training Accuracy: 1.0 Loss: 0.0002793471794575453\n",
      "Iteration: 16780 Training Accuracy: 0.984375 Loss: 0.0009374298388138413\n",
      "Iteration: 16790 Training Accuracy: 0.96875 Loss: 0.0008684542845003307\n",
      "Iteration: 16800 Training Accuracy: 1.0 Loss: 0.0004463630320969969\n",
      "Iteration: 16810 Training Accuracy: 0.984375 Loss: 0.0006499833543784916\n",
      "Iteration: 16820 Training Accuracy: 0.96875 Loss: 0.0007421735790558159\n",
      "Iteration: 16830 Training Accuracy: 1.0 Loss: 0.0006298660300672054\n",
      "Iteration: 16840 Training Accuracy: 0.96875 Loss: 0.0015453151427209377\n",
      "Iteration: 16850 Training Accuracy: 1.0 Loss: 0.00023008791322354227\n",
      "Iteration: 16860 Training Accuracy: 1.0 Loss: 0.0003361760755069554\n",
      "Iteration: 16870 Training Accuracy: 1.0 Loss: 0.0005193878896534443\n",
      "Iteration: 16880 Training Accuracy: 1.0 Loss: 0.0001313187531195581\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9246666666666666\n",
      "epoch: 18\n",
      "Iteration: 16890 Training Accuracy: 1.0 Loss: 0.00034441007301211357\n",
      "Iteration: 16900 Training Accuracy: 1.0 Loss: 0.0002947335597127676\n",
      "Iteration: 16910 Training Accuracy: 1.0 Loss: 0.0002314094454050064\n",
      "Iteration: 16920 Training Accuracy: 0.984375 Loss: 0.0014501545811071992\n",
      "Iteration: 16930 Training Accuracy: 1.0 Loss: 0.0002908901951741427\n",
      "Iteration: 16940 Training Accuracy: 0.984375 Loss: 0.000318120262818411\n",
      "Iteration: 16950 Training Accuracy: 0.984375 Loss: 0.0013737352564930916\n",
      "Iteration: 16960 Training Accuracy: 0.984375 Loss: 0.0015609747497364879\n",
      "Iteration: 16970 Training Accuracy: 0.96875 Loss: 0.0014921498950570822\n",
      "Iteration: 16980 Training Accuracy: 0.96875 Loss: 0.001123562571592629\n",
      "Iteration: 16990 Training Accuracy: 0.953125 Loss: 0.0016311963554471731\n",
      "Iteration: 17000 Training Accuracy: 0.984375 Loss: 0.0003105599316768348\n",
      "Iteration: 17010 Training Accuracy: 0.96875 Loss: 0.0015835072845220566\n",
      "Iteration: 17020 Training Accuracy: 1.0 Loss: 0.00019669131143018603\n",
      "Iteration: 17030 Training Accuracy: 1.0 Loss: 0.0004627089947462082\n",
      "Iteration: 17040 Training Accuracy: 0.96875 Loss: 0.001346344593912363\n",
      "Iteration: 17050 Training Accuracy: 1.0 Loss: 0.00018591724801808596\n",
      "Iteration: 17060 Training Accuracy: 0.984375 Loss: 0.0006275753839872777\n",
      "Iteration: 17070 Training Accuracy: 0.96875 Loss: 0.0006831905338913202\n",
      "Iteration: 17080 Training Accuracy: 0.984375 Loss: 0.0004771611129399389\n",
      "Iteration: 17090 Training Accuracy: 0.984375 Loss: 0.0007821907056495547\n",
      "Iteration: 17100 Training Accuracy: 0.953125 Loss: 0.0015300174709409475\n",
      "Iteration: 17110 Training Accuracy: 0.984375 Loss: 0.0009224460227414966\n",
      "Iteration: 17120 Training Accuracy: 0.984375 Loss: 0.0004802508046850562\n",
      "Iteration: 17130 Training Accuracy: 1.0 Loss: 0.0005051856860518456\n",
      "Iteration: 17140 Training Accuracy: 1.0 Loss: 0.00014475107309408486\n",
      "Iteration: 17150 Training Accuracy: 0.984375 Loss: 0.0005875129136256874\n",
      "Iteration: 17160 Training Accuracy: 0.984375 Loss: 0.0006985360523685813\n",
      "Iteration: 17170 Training Accuracy: 1.0 Loss: 8.101433923002332e-05\n",
      "Iteration: 17180 Training Accuracy: 1.0 Loss: 0.0004665914748329669\n",
      "Iteration: 17190 Training Accuracy: 1.0 Loss: 0.0003995953593403101\n",
      "Iteration: 17200 Training Accuracy: 0.984375 Loss: 0.0008726773085072637\n",
      "Iteration: 17210 Training Accuracy: 1.0 Loss: 9.30325040826574e-05\n",
      "Iteration: 17220 Training Accuracy: 0.953125 Loss: 0.002358249155804515\n",
      "Iteration: 17230 Training Accuracy: 1.0 Loss: 0.0002771805739030242\n",
      "Iteration: 17240 Training Accuracy: 1.0 Loss: 0.00020685503841377795\n",
      "Iteration: 17250 Training Accuracy: 1.0 Loss: 0.00019627579604275525\n",
      "Iteration: 17260 Training Accuracy: 0.984375 Loss: 0.0006249947473406792\n",
      "Iteration: 17270 Training Accuracy: 0.96875 Loss: 0.001127897179685533\n",
      "Iteration: 17280 Training Accuracy: 1.0 Loss: 0.00028510676929727197\n",
      "Iteration: 17290 Training Accuracy: 1.0 Loss: 0.00016480841441079974\n",
      "Iteration: 17300 Training Accuracy: 1.0 Loss: 0.000454229157185182\n",
      "Iteration: 17310 Training Accuracy: 1.0 Loss: 0.00039642085903324187\n",
      "Iteration: 17320 Training Accuracy: 1.0 Loss: 0.00045326430699788034\n",
      "Iteration: 17330 Training Accuracy: 1.0 Loss: 0.0003080070309806615\n",
      "Iteration: 17340 Training Accuracy: 1.0 Loss: 0.00019145969417877495\n",
      "Iteration: 17350 Training Accuracy: 1.0 Loss: 0.00027815060457214713\n",
      "Iteration: 17360 Training Accuracy: 1.0 Loss: 0.0006628058035857975\n",
      "Iteration: 17370 Training Accuracy: 1.0 Loss: 0.0004475425521377474\n",
      "Iteration: 17380 Training Accuracy: 1.0 Loss: 0.0005169167416170239\n",
      "Iteration: 17390 Training Accuracy: 0.96875 Loss: 0.0020936618093401194\n",
      "Iteration: 17400 Training Accuracy: 1.0 Loss: 0.00022141134832054377\n",
      "Iteration: 17410 Training Accuracy: 0.984375 Loss: 0.0005528457113541663\n",
      "Iteration: 17420 Training Accuracy: 0.984375 Loss: 0.0007869830005802214\n",
      "Iteration: 17430 Training Accuracy: 0.984375 Loss: 0.0011084803845733404\n",
      "Iteration: 17440 Training Accuracy: 1.0 Loss: 6.659880455117673e-05\n",
      "Iteration: 17450 Training Accuracy: 1.0 Loss: 7.201927655842155e-05\n",
      "Iteration: 17460 Training Accuracy: 1.0 Loss: 0.0004620321560651064\n",
      "Iteration: 17470 Training Accuracy: 1.0 Loss: 0.0003915856941603124\n",
      "Iteration: 17480 Training Accuracy: 0.984375 Loss: 0.0005203810869716108\n",
      "Iteration: 17490 Training Accuracy: 1.0 Loss: 0.00033087990595959127\n",
      "Iteration: 17500 Training Accuracy: 1.0 Loss: 0.00013474095612764359\n",
      "Iteration: 17510 Training Accuracy: 1.0 Loss: 0.00011362106306478381\n",
      "Iteration: 17520 Training Accuracy: 1.0 Loss: 0.0003610170679166913\n",
      "Iteration: 17530 Training Accuracy: 0.96875 Loss: 0.0009599251206964254\n",
      "Iteration: 17540 Training Accuracy: 1.0 Loss: 0.00035840971395373344\n",
      "Iteration: 17550 Training Accuracy: 0.984375 Loss: 0.0005292515270411968\n",
      "Iteration: 17560 Training Accuracy: 1.0 Loss: 0.0004384650383144617\n",
      "Iteration: 17570 Training Accuracy: 1.0 Loss: 0.0011249210219830275\n",
      "Iteration: 17580 Training Accuracy: 1.0 Loss: 0.00045630126260221004\n",
      "Iteration: 17590 Training Accuracy: 1.0 Loss: 0.000225167372263968\n",
      "Iteration: 17600 Training Accuracy: 0.984375 Loss: 0.0009431289508938789\n",
      "Iteration: 17610 Training Accuracy: 0.984375 Loss: 0.000499742804095149\n",
      "Iteration: 17620 Training Accuracy: 0.96875 Loss: 0.001545688253827393\n",
      "Iteration: 17630 Training Accuracy: 0.96875 Loss: 0.0012382289860397577\n",
      "Iteration: 17640 Training Accuracy: 0.984375 Loss: 0.0007021366618573666\n",
      "Iteration: 17650 Training Accuracy: 0.984375 Loss: 0.0005877060466445982\n",
      "Iteration: 17660 Training Accuracy: 0.984375 Loss: 0.0005599610158242285\n",
      "Iteration: 17670 Training Accuracy: 1.0 Loss: 0.00035970419412478805\n",
      "Iteration: 17680 Training Accuracy: 0.984375 Loss: 0.00038442612276412547\n",
      "Iteration: 17690 Training Accuracy: 1.0 Loss: 0.00019133889873046428\n",
      "Iteration: 17700 Training Accuracy: 0.984375 Loss: 0.0012612794525921345\n",
      "Iteration: 17710 Training Accuracy: 1.0 Loss: 0.000372508104192093\n",
      "Iteration: 17720 Training Accuracy: 1.0 Loss: 0.0003461934975348413\n",
      "Iteration: 17730 Training Accuracy: 0.984375 Loss: 0.0003872112138196826\n",
      "Iteration: 17740 Training Accuracy: 0.96875 Loss: 0.00137984415050596\n",
      "Iteration: 17750 Training Accuracy: 1.0 Loss: 0.0004682443104684353\n",
      "Iteration: 17760 Training Accuracy: 1.0 Loss: 0.0002748330880422145\n",
      "Iteration: 17770 Training Accuracy: 1.0 Loss: 0.000514032319188118\n",
      "Iteration: 17780 Training Accuracy: 1.0 Loss: 0.0002627663780003786\n",
      "Iteration: 17790 Training Accuracy: 1.0 Loss: 0.00045691695413552225\n",
      "Iteration: 17800 Training Accuracy: 0.984375 Loss: 0.0009716474451124668\n",
      "Iteration: 17810 Training Accuracy: 1.0 Loss: 0.0004119277582503855\n",
      "Iteration: 17820 Training Accuracy: 0.96875 Loss: 0.00138876645360142\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9248333333333333\n",
      "epoch: 19\n",
      "Iteration: 17830 Training Accuracy: 1.0 Loss: 0.00025711487978696823\n",
      "Iteration: 17840 Training Accuracy: 1.0 Loss: 0.00024347314320039004\n",
      "Iteration: 17850 Training Accuracy: 0.984375 Loss: 0.0014925103168934584\n",
      "Iteration: 17860 Training Accuracy: 1.0 Loss: 0.0004025932867079973\n",
      "Iteration: 17870 Training Accuracy: 1.0 Loss: 0.0006022987654432654\n",
      "Iteration: 17880 Training Accuracy: 1.0 Loss: 0.00017860109801404178\n",
      "Iteration: 17890 Training Accuracy: 0.96875 Loss: 0.0017751817358657718\n",
      "Iteration: 17900 Training Accuracy: 0.984375 Loss: 0.0010245381854474545\n",
      "Iteration: 17910 Training Accuracy: 0.96875 Loss: 0.0012461432488635182\n",
      "Iteration: 17920 Training Accuracy: 1.0 Loss: 0.0002592550590634346\n",
      "Iteration: 17930 Training Accuracy: 0.984375 Loss: 0.0006949823000468314\n",
      "Iteration: 17940 Training Accuracy: 0.984375 Loss: 0.0010958141647279263\n",
      "Iteration: 17950 Training Accuracy: 1.0 Loss: 0.00019292863726150244\n",
      "Iteration: 17960 Training Accuracy: 0.984375 Loss: 0.0012920720037072897\n",
      "Iteration: 17970 Training Accuracy: 1.0 Loss: 0.000752351654227823\n",
      "Iteration: 17980 Training Accuracy: 0.984375 Loss: 0.0005537736578844488\n",
      "Iteration: 17990 Training Accuracy: 0.984375 Loss: 0.0006101049948483706\n",
      "Iteration: 18000 Training Accuracy: 0.984375 Loss: 0.0005716336308978498\n",
      "Iteration: 18010 Training Accuracy: 1.0 Loss: 0.0003334489301778376\n",
      "Iteration: 18020 Training Accuracy: 1.0 Loss: 0.00024164786736946553\n",
      "Iteration: 18030 Training Accuracy: 1.0 Loss: 0.00011197223648196086\n",
      "Iteration: 18040 Training Accuracy: 0.984375 Loss: 0.000620851875282824\n",
      "Iteration: 18050 Training Accuracy: 0.96875 Loss: 0.0010655542137101293\n",
      "Iteration: 18060 Training Accuracy: 0.984375 Loss: 0.0006726226420141757\n",
      "Iteration: 18070 Training Accuracy: 0.96875 Loss: 0.0010055146412923932\n",
      "Iteration: 18080 Training Accuracy: 1.0 Loss: 0.0004995060153305531\n",
      "Iteration: 18090 Training Accuracy: 0.984375 Loss: 0.001368833938613534\n",
      "Iteration: 18100 Training Accuracy: 0.984375 Loss: 0.0007033428410068154\n",
      "Iteration: 18110 Training Accuracy: 0.984375 Loss: 0.0006127721862867475\n",
      "Iteration: 18120 Training Accuracy: 1.0 Loss: 0.0003968669625464827\n",
      "Iteration: 18130 Training Accuracy: 1.0 Loss: 0.00034978261101059616\n",
      "Iteration: 18140 Training Accuracy: 0.96875 Loss: 0.0024503988679498434\n",
      "Iteration: 18150 Training Accuracy: 1.0 Loss: 0.0003343041753396392\n",
      "Iteration: 18160 Training Accuracy: 1.0 Loss: 6.250216392800212e-05\n",
      "Iteration: 18170 Training Accuracy: 0.984375 Loss: 0.0002960127894766629\n",
      "Iteration: 18180 Training Accuracy: 1.0 Loss: 0.00035084059345535934\n",
      "Iteration: 18190 Training Accuracy: 0.984375 Loss: 0.001006023958325386\n",
      "Iteration: 18200 Training Accuracy: 0.984375 Loss: 0.0006801002891734242\n",
      "Iteration: 18210 Training Accuracy: 0.984375 Loss: 0.0007623619749210775\n",
      "Iteration: 18220 Training Accuracy: 0.984375 Loss: 0.0008021547691896558\n",
      "Iteration: 18230 Training Accuracy: 0.984375 Loss: 0.0009477448184043169\n",
      "Iteration: 18240 Training Accuracy: 1.0 Loss: 7.422729686368257e-05\n",
      "Iteration: 18250 Training Accuracy: 0.96875 Loss: 0.0012103066546842456\n",
      "Iteration: 18260 Training Accuracy: 1.0 Loss: 0.0005701165646314621\n",
      "Iteration: 18270 Training Accuracy: 0.96875 Loss: 0.0009252222953364253\n",
      "Iteration: 18280 Training Accuracy: 1.0 Loss: 9.939351002685726e-05\n",
      "Iteration: 18290 Training Accuracy: 1.0 Loss: 0.0003661680384539068\n",
      "Iteration: 18300 Training Accuracy: 1.0 Loss: 0.0006787106394767761\n",
      "Iteration: 18310 Training Accuracy: 0.984375 Loss: 0.0010519990464672446\n",
      "Iteration: 18320 Training Accuracy: 1.0 Loss: 0.0002566048060543835\n",
      "Iteration: 18330 Training Accuracy: 1.0 Loss: 0.0005129660712555051\n",
      "Iteration: 18340 Training Accuracy: 1.0 Loss: 0.00037523036007769406\n",
      "Iteration: 18350 Training Accuracy: 1.0 Loss: 0.00046976213343441486\n",
      "Iteration: 18360 Training Accuracy: 0.96875 Loss: 0.001203657011501491\n",
      "Iteration: 18370 Training Accuracy: 0.984375 Loss: 0.0013215815415605903\n",
      "Iteration: 18380 Training Accuracy: 0.984375 Loss: 0.0008168716449290514\n",
      "Iteration: 18390 Training Accuracy: 1.0 Loss: 0.0003079126472584903\n",
      "Iteration: 18400 Training Accuracy: 0.984375 Loss: 0.0006669285357929766\n",
      "Iteration: 18410 Training Accuracy: 1.0 Loss: 0.0004790460807271302\n",
      "Iteration: 18420 Training Accuracy: 1.0 Loss: 0.0005035398644395173\n",
      "Iteration: 18430 Training Accuracy: 0.984375 Loss: 0.00037577791954390705\n",
      "Iteration: 18440 Training Accuracy: 1.0 Loss: 0.00025966769317165017\n",
      "Iteration: 18450 Training Accuracy: 1.0 Loss: 0.00031462390325032175\n",
      "Iteration: 18460 Training Accuracy: 0.984375 Loss: 0.0011455102358013391\n",
      "Iteration: 18470 Training Accuracy: 1.0 Loss: 9.549826791044325e-05\n",
      "Iteration: 18480 Training Accuracy: 0.96875 Loss: 0.0010573569452390075\n",
      "Iteration: 18490 Training Accuracy: 0.96875 Loss: 0.0018815292278304696\n",
      "Iteration: 18500 Training Accuracy: 1.0 Loss: 7.205118890851736e-05\n",
      "Iteration: 18510 Training Accuracy: 1.0 Loss: 0.00023598353436682373\n",
      "Iteration: 18520 Training Accuracy: 0.984375 Loss: 0.0002624533954076469\n",
      "Iteration: 18530 Training Accuracy: 0.984375 Loss: 0.00045808375580236316\n",
      "Iteration: 18540 Training Accuracy: 0.984375 Loss: 0.0006273476756177843\n",
      "Iteration: 18550 Training Accuracy: 1.0 Loss: 0.00040151685243472457\n",
      "Iteration: 18560 Training Accuracy: 0.984375 Loss: 0.0012440261198207736\n",
      "Iteration: 18570 Training Accuracy: 0.96875 Loss: 0.0008965071174316108\n",
      "Iteration: 18580 Training Accuracy: 1.0 Loss: 0.0006467243656516075\n",
      "Iteration: 18590 Training Accuracy: 0.96875 Loss: 0.0010019202018156648\n",
      "Iteration: 18600 Training Accuracy: 0.96875 Loss: 0.00111456960439682\n",
      "Iteration: 18610 Training Accuracy: 0.984375 Loss: 0.000675346702337265\n",
      "Iteration: 18620 Training Accuracy: 1.0 Loss: 0.0005340073257684708\n",
      "Iteration: 18630 Training Accuracy: 1.0 Loss: 0.0005599152063950896\n",
      "Iteration: 18640 Training Accuracy: 0.96875 Loss: 0.000991488341242075\n",
      "Iteration: 18650 Training Accuracy: 1.0 Loss: 4.0412072848994285e-05\n",
      "Iteration: 18660 Training Accuracy: 1.0 Loss: 0.00022189099399838597\n",
      "Iteration: 18670 Training Accuracy: 0.984375 Loss: 0.0011840537190437317\n",
      "Iteration: 18680 Training Accuracy: 0.984375 Loss: 0.0009885650360956788\n",
      "Iteration: 18690 Training Accuracy: 0.984375 Loss: 0.000608442525845021\n",
      "Iteration: 18700 Training Accuracy: 0.984375 Loss: 0.0006316646467894316\n",
      "Iteration: 18710 Training Accuracy: 0.984375 Loss: 0.0010022815549746156\n",
      "Iteration: 18720 Training Accuracy: 1.0 Loss: 0.00016564028919674456\n",
      "Iteration: 18730 Training Accuracy: 1.0 Loss: 0.00018856785027310252\n",
      "Iteration: 18740 Training Accuracy: 1.0 Loss: 0.0005247954395599663\n",
      "Iteration: 18750 Training Accuracy: 0.984375 Loss: 0.0006383161526173353\n",
      "Iteration: 18760 Training Accuracy: 1.0 Loss: 0.00014252925757318735\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9246666666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.9896333333333334\n",
      "Validation Accuracy = 0.9246666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RESNET18_2(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use res net \n",
    "model = RESNET18_2()\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'full_resnet18_1_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.96875 Loss: 0.0012539136223495007\n",
      "Iteration: 20 Training Accuracy: 1.0 Loss: 0.00026964934659190476\n",
      "Iteration: 30 Training Accuracy: 0.984375 Loss: 0.001392528647556901\n",
      "Iteration: 40 Training Accuracy: 0.96875 Loss: 0.0009054119582287967\n",
      "Iteration: 50 Training Accuracy: 0.9375 Loss: 0.0019365593325346708\n",
      "Iteration: 60 Training Accuracy: 0.96875 Loss: 0.0011154626263305545\n",
      "Iteration: 70 Training Accuracy: 0.984375 Loss: 0.0006532897241413593\n",
      "Iteration: 80 Training Accuracy: 0.96875 Loss: 0.0012577921152114868\n",
      "Iteration: 90 Training Accuracy: 0.984375 Loss: 0.0007886758539825678\n",
      "Iteration: 100 Training Accuracy: 0.984375 Loss: 0.0005693825660273433\n",
      "Iteration: 110 Training Accuracy: 1.0 Loss: 0.0003563984646461904\n",
      "Iteration: 120 Training Accuracy: 0.984375 Loss: 0.00040823937160894275\n",
      "Iteration: 130 Training Accuracy: 0.953125 Loss: 0.0020719568710774183\n",
      "Iteration: 140 Training Accuracy: 0.953125 Loss: 0.0025311096105724573\n",
      "Iteration: 150 Training Accuracy: 0.984375 Loss: 0.0019895427394658327\n",
      "Iteration: 160 Training Accuracy: 1.0 Loss: 0.0002502019633539021\n",
      "Iteration: 170 Training Accuracy: 0.96875 Loss: 0.0017279023304581642\n",
      "Iteration: 180 Training Accuracy: 0.953125 Loss: 0.0016666569281369448\n",
      "Iteration: 190 Training Accuracy: 0.984375 Loss: 0.0008425869746133685\n",
      "Iteration: 200 Training Accuracy: 0.96875 Loss: 0.002543809125199914\n",
      "Iteration: 210 Training Accuracy: 1.0 Loss: 0.0006533174309879541\n",
      "Iteration: 220 Training Accuracy: 0.984375 Loss: 0.0007399223977699876\n",
      "Iteration: 230 Training Accuracy: 0.984375 Loss: 0.0012278183130547404\n",
      "Iteration: 240 Training Accuracy: 0.984375 Loss: 0.0012389887124300003\n",
      "Iteration: 250 Training Accuracy: 1.0 Loss: 0.0005703440983779728\n",
      "Iteration: 260 Training Accuracy: 1.0 Loss: 0.0004703029408119619\n",
      "Iteration: 270 Training Accuracy: 1.0 Loss: 0.00024913111701607704\n",
      "Iteration: 280 Training Accuracy: 0.984375 Loss: 0.0008206233032979071\n",
      "Iteration: 290 Training Accuracy: 0.96875 Loss: 0.0012396249221637845\n",
      "Iteration: 300 Training Accuracy: 0.921875 Loss: 0.0021479488350450993\n",
      "Iteration: 310 Training Accuracy: 0.9375 Loss: 0.0031274366192519665\n",
      "Iteration: 320 Training Accuracy: 0.984375 Loss: 0.0009230988216586411\n",
      "Iteration: 330 Training Accuracy: 0.96875 Loss: 0.0011985229793936014\n",
      "Iteration: 340 Training Accuracy: 1.0 Loss: 0.0001251839566975832\n",
      "Iteration: 350 Training Accuracy: 1.0 Loss: 0.00015400530537590384\n",
      "Iteration: 360 Training Accuracy: 0.96875 Loss: 0.0016462086932733655\n",
      "Iteration: 370 Training Accuracy: 0.96875 Loss: 0.001263427548110485\n",
      "Iteration: 380 Training Accuracy: 1.0 Loss: 0.0006217951886355877\n",
      "Iteration: 390 Training Accuracy: 0.96875 Loss: 0.0015890973154455423\n",
      "Iteration: 400 Training Accuracy: 0.984375 Loss: 0.0011126026511192322\n",
      "Iteration: 410 Training Accuracy: 0.96875 Loss: 0.0013255393132567406\n",
      "Iteration: 420 Training Accuracy: 0.984375 Loss: 0.0007247916073538363\n",
      "Iteration: 430 Training Accuracy: 0.96875 Loss: 0.0018351715989410877\n",
      "Iteration: 440 Training Accuracy: 0.96875 Loss: 0.0010590695310384035\n",
      "Iteration: 450 Training Accuracy: 0.953125 Loss: 0.0017358921468257904\n",
      "Iteration: 460 Training Accuracy: 0.984375 Loss: 0.0009523456683382392\n",
      "Iteration: 470 Training Accuracy: 0.984375 Loss: 0.0007004641811363399\n",
      "Iteration: 480 Training Accuracy: 0.96875 Loss: 0.0019273512298241258\n",
      "Iteration: 490 Training Accuracy: 0.984375 Loss: 0.0007273273076862097\n",
      "Iteration: 500 Training Accuracy: 0.921875 Loss: 0.0021881498396396637\n",
      "Iteration: 510 Training Accuracy: 0.96875 Loss: 0.0007309342036023736\n",
      "Iteration: 520 Training Accuracy: 0.984375 Loss: 0.0004711572837550193\n",
      "Iteration: 530 Training Accuracy: 0.953125 Loss: 0.0007886866806074977\n",
      "Iteration: 540 Training Accuracy: 0.984375 Loss: 0.0010190187022089958\n",
      "Iteration: 550 Training Accuracy: 0.984375 Loss: 0.0009023383026942611\n",
      "Iteration: 560 Training Accuracy: 0.984375 Loss: 0.0005063913413323462\n",
      "Iteration: 570 Training Accuracy: 0.96875 Loss: 0.0017800555797293782\n",
      "Iteration: 580 Training Accuracy: 0.984375 Loss: 0.0008914358913898468\n",
      "Iteration: 590 Training Accuracy: 1.0 Loss: 0.00042674303404055536\n",
      "Iteration: 600 Training Accuracy: 1.0 Loss: 0.0004347534559201449\n",
      "Iteration: 610 Training Accuracy: 1.0 Loss: 0.00044467367115430534\n",
      "Iteration: 620 Training Accuracy: 0.96875 Loss: 0.0015818264801055193\n",
      "Iteration: 630 Training Accuracy: 1.0 Loss: 0.0001985897106351331\n",
      "Iteration: 640 Training Accuracy: 1.0 Loss: 0.000262930931057781\n",
      "Iteration: 650 Training Accuracy: 0.984375 Loss: 0.0008932941127568483\n",
      "Iteration: 660 Training Accuracy: 0.953125 Loss: 0.0018768107984215021\n",
      "Iteration: 670 Training Accuracy: 0.984375 Loss: 0.0006741672405041754\n",
      "Iteration: 680 Training Accuracy: 0.953125 Loss: 0.0018294937908649445\n",
      "Iteration: 690 Training Accuracy: 1.0 Loss: 0.00024621683405712247\n",
      "Iteration: 700 Training Accuracy: 0.9375 Loss: 0.0032732870895415545\n",
      "Iteration: 710 Training Accuracy: 0.984375 Loss: 0.00031706737354397774\n",
      "Iteration: 720 Training Accuracy: 0.953125 Loss: 0.0016012047417461872\n",
      "Iteration: 730 Training Accuracy: 0.9375 Loss: 0.0027912852820008993\n",
      "Iteration: 740 Training Accuracy: 0.984375 Loss: 0.0008926220471039414\n",
      "Iteration: 750 Training Accuracy: 0.953125 Loss: 0.0017145202727988362\n",
      "Iteration: 760 Training Accuracy: 0.953125 Loss: 0.002276657149195671\n",
      "Iteration: 770 Training Accuracy: 0.953125 Loss: 0.0020277989096939564\n",
      "Iteration: 780 Training Accuracy: 0.953125 Loss: 0.0018625942757353187\n",
      "Iteration: 790 Training Accuracy: 1.0 Loss: 0.00020121896523050964\n",
      "Iteration: 800 Training Accuracy: 0.921875 Loss: 0.004686907399445772\n",
      "Iteration: 810 Training Accuracy: 0.9375 Loss: 0.002124038990586996\n",
      "Iteration: 820 Training Accuracy: 0.96875 Loss: 0.0013233807403594255\n",
      "Iteration: 830 Training Accuracy: 0.9375 Loss: 0.0024138325825333595\n",
      "Iteration: 840 Training Accuracy: 0.96875 Loss: 0.0018007978796958923\n",
      "Iteration: 850 Training Accuracy: 0.984375 Loss: 0.0009659792413003743\n",
      "Iteration: 860 Training Accuracy: 1.0 Loss: 0.00024436129024252295\n",
      "Iteration: 870 Training Accuracy: 0.984375 Loss: 0.00032061481033451855\n",
      "Iteration: 880 Training Accuracy: 0.96875 Loss: 0.001020466210320592\n",
      "Iteration: 890 Training Accuracy: 0.96875 Loss: 0.0013176866341382265\n",
      "Iteration: 900 Training Accuracy: 1.0 Loss: 0.0005966196767985821\n",
      "Iteration: 910 Training Accuracy: 0.984375 Loss: 0.001235499046742916\n",
      "Iteration: 920 Training Accuracy: 1.0 Loss: 0.0003561197663657367\n",
      "Iteration: 930 Training Accuracy: 0.96875 Loss: 0.0016819338779896498\n",
      "Training Accuracy = 0.96875\n",
      "Validation Accuracy = 0.9075\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 0.96875 Loss: 0.0016686524031683803\n",
      "Iteration: 950 Training Accuracy: 0.953125 Loss: 0.0018910311628133059\n",
      "Iteration: 960 Training Accuracy: 0.953125 Loss: 0.0016491429414600134\n",
      "Iteration: 970 Training Accuracy: 0.984375 Loss: 0.0005419209483079612\n",
      "Iteration: 980 Training Accuracy: 0.953125 Loss: 0.0016339314170181751\n",
      "Iteration: 990 Training Accuracy: 0.96875 Loss: 0.0011938902316614985\n",
      "Iteration: 1000 Training Accuracy: 1.0 Loss: 0.00030578323639929295\n",
      "Iteration: 1010 Training Accuracy: 0.984375 Loss: 0.0005124501185491681\n",
      "Iteration: 1020 Training Accuracy: 1.0 Loss: 7.539005309808999e-05\n",
      "Iteration: 1030 Training Accuracy: 0.96875 Loss: 0.0009445190080441535\n",
      "Iteration: 1040 Training Accuracy: 0.96875 Loss: 0.0023450152948498726\n",
      "Iteration: 1050 Training Accuracy: 1.0 Loss: 0.0001842383062466979\n",
      "Iteration: 1060 Training Accuracy: 1.0 Loss: 0.0003530742251314223\n",
      "Iteration: 1070 Training Accuracy: 0.984375 Loss: 0.002320581115782261\n",
      "Iteration: 1080 Training Accuracy: 1.0 Loss: 0.0004863220383413136\n",
      "Iteration: 1090 Training Accuracy: 0.984375 Loss: 0.0006980475736781955\n",
      "Iteration: 1100 Training Accuracy: 1.0 Loss: 0.00039825253770686686\n",
      "Iteration: 1110 Training Accuracy: 0.984375 Loss: 0.0006662012310698628\n",
      "Iteration: 1120 Training Accuracy: 0.984375 Loss: 0.0014852338936179876\n",
      "Iteration: 1130 Training Accuracy: 0.984375 Loss: 0.0012260860530659556\n",
      "Iteration: 1140 Training Accuracy: 0.984375 Loss: 0.0005625223275274038\n",
      "Iteration: 1150 Training Accuracy: 1.0 Loss: 0.00020343621145002544\n",
      "Iteration: 1160 Training Accuracy: 0.953125 Loss: 0.0015689152060076594\n",
      "Iteration: 1170 Training Accuracy: 0.96875 Loss: 0.0011613888200372458\n",
      "Iteration: 1180 Training Accuracy: 0.984375 Loss: 0.0004230913473293185\n",
      "Iteration: 1190 Training Accuracy: 0.953125 Loss: 0.0012896815314888954\n",
      "Iteration: 1200 Training Accuracy: 0.984375 Loss: 0.0007777646533213556\n",
      "Iteration: 1210 Training Accuracy: 0.953125 Loss: 0.0018151231342926621\n",
      "Iteration: 1220 Training Accuracy: 0.984375 Loss: 0.0013108921702951193\n",
      "Iteration: 1230 Training Accuracy: 1.0 Loss: 0.0005045182770118117\n",
      "Iteration: 1240 Training Accuracy: 1.0 Loss: 0.00037479863385669887\n",
      "Iteration: 1250 Training Accuracy: 1.0 Loss: 0.0006829663761891425\n",
      "Iteration: 1260 Training Accuracy: 1.0 Loss: 0.000628336681984365\n",
      "Iteration: 1270 Training Accuracy: 1.0 Loss: 0.0002532675280235708\n",
      "Iteration: 1280 Training Accuracy: 1.0 Loss: 0.00010481078788870946\n",
      "Iteration: 1290 Training Accuracy: 0.984375 Loss: 0.0007446948438882828\n",
      "Iteration: 1300 Training Accuracy: 1.0 Loss: 0.00029428087873384356\n",
      "Iteration: 1310 Training Accuracy: 1.0 Loss: 0.00032677967101335526\n",
      "Iteration: 1320 Training Accuracy: 0.984375 Loss: 0.001081543043255806\n",
      "Iteration: 1330 Training Accuracy: 1.0 Loss: 0.0005193231045268476\n",
      "Iteration: 1340 Training Accuracy: 0.984375 Loss: 0.0007648238679394126\n",
      "Iteration: 1350 Training Accuracy: 0.9375 Loss: 0.0018375201616436243\n",
      "Iteration: 1360 Training Accuracy: 0.96875 Loss: 0.0007720374269410968\n",
      "Iteration: 1370 Training Accuracy: 1.0 Loss: 0.0004405340878292918\n",
      "Iteration: 1380 Training Accuracy: 1.0 Loss: 0.00034562230575829744\n",
      "Iteration: 1390 Training Accuracy: 0.96875 Loss: 0.0013893526047468185\n",
      "Iteration: 1400 Training Accuracy: 1.0 Loss: 0.0005528985057026148\n",
      "Iteration: 1410 Training Accuracy: 1.0 Loss: 0.00029141403501853347\n",
      "Iteration: 1420 Training Accuracy: 0.984375 Loss: 0.0006343030254356563\n",
      "Iteration: 1430 Training Accuracy: 0.96875 Loss: 0.0009068854269571602\n",
      "Iteration: 1440 Training Accuracy: 0.984375 Loss: 0.0012435695389285684\n",
      "Iteration: 1450 Training Accuracy: 0.96875 Loss: 0.002027673413977027\n",
      "Iteration: 1460 Training Accuracy: 0.984375 Loss: 0.0008813378517515957\n",
      "Iteration: 1470 Training Accuracy: 0.96875 Loss: 0.0009405437740497291\n",
      "Iteration: 1480 Training Accuracy: 0.96875 Loss: 0.0015334803611040115\n",
      "Iteration: 1490 Training Accuracy: 1.0 Loss: 0.000600153929553926\n",
      "Iteration: 1500 Training Accuracy: 1.0 Loss: 0.0003249142610002309\n",
      "Iteration: 1510 Training Accuracy: 0.984375 Loss: 0.0007442433852702379\n",
      "Iteration: 1520 Training Accuracy: 1.0 Loss: 0.000291668635327369\n",
      "Iteration: 1530 Training Accuracy: 0.984375 Loss: 0.0005940571427345276\n",
      "Iteration: 1540 Training Accuracy: 0.984375 Loss: 0.0008114738739095628\n",
      "Iteration: 1550 Training Accuracy: 1.0 Loss: 0.00047982099931687117\n",
      "Iteration: 1560 Training Accuracy: 1.0 Loss: 0.00031124093220569193\n",
      "Iteration: 1570 Training Accuracy: 0.953125 Loss: 0.001365147065371275\n",
      "Iteration: 1580 Training Accuracy: 0.96875 Loss: 0.0007645667647011578\n",
      "Iteration: 1590 Training Accuracy: 0.984375 Loss: 0.0004956371267326176\n",
      "Iteration: 1600 Training Accuracy: 0.96875 Loss: 0.0008579413406550884\n",
      "Iteration: 1610 Training Accuracy: 1.0 Loss: 6.240737275220454e-05\n",
      "Iteration: 1620 Training Accuracy: 0.984375 Loss: 0.0008864514529705048\n",
      "Iteration: 1630 Training Accuracy: 0.953125 Loss: 0.0011153618106618524\n",
      "Iteration: 1640 Training Accuracy: 0.96875 Loss: 0.0013786758063361049\n",
      "Iteration: 1650 Training Accuracy: 0.96875 Loss: 0.0009224821114912629\n",
      "Iteration: 1660 Training Accuracy: 0.984375 Loss: 0.0007365827914327383\n",
      "Iteration: 1670 Training Accuracy: 0.96875 Loss: 0.0009676163899712265\n",
      "Iteration: 1680 Training Accuracy: 1.0 Loss: 0.00019333107047714293\n",
      "Iteration: 1690 Training Accuracy: 0.96875 Loss: 0.0008976883254945278\n",
      "Iteration: 1700 Training Accuracy: 1.0 Loss: 0.0005665973294526339\n",
      "Iteration: 1710 Training Accuracy: 0.984375 Loss: 0.0011766935931518674\n",
      "Iteration: 1720 Training Accuracy: 1.0 Loss: 0.0001564875419717282\n",
      "Iteration: 1730 Training Accuracy: 0.96875 Loss: 0.0013835197314620018\n",
      "Iteration: 1740 Training Accuracy: 0.96875 Loss: 0.0008578195702284575\n",
      "Iteration: 1750 Training Accuracy: 0.984375 Loss: 0.0008029007585719228\n",
      "Iteration: 1760 Training Accuracy: 1.0 Loss: 0.0004027016693726182\n",
      "Iteration: 1770 Training Accuracy: 0.953125 Loss: 0.0015376820228993893\n",
      "Iteration: 1780 Training Accuracy: 0.96875 Loss: 0.00113282585516572\n",
      "Iteration: 1790 Training Accuracy: 0.96875 Loss: 0.0010066472459584475\n",
      "Iteration: 1800 Training Accuracy: 0.984375 Loss: 0.0006593905854970217\n",
      "Iteration: 1810 Training Accuracy: 0.984375 Loss: 0.00046596932224929333\n",
      "Iteration: 1820 Training Accuracy: 0.96875 Loss: 0.0009075404377654195\n",
      "Iteration: 1830 Training Accuracy: 0.984375 Loss: 0.0006688317516818643\n",
      "Iteration: 1840 Training Accuracy: 1.0 Loss: 0.00021542151807807386\n",
      "Iteration: 1850 Training Accuracy: 0.953125 Loss: 0.0018085413612425327\n",
      "Iteration: 1860 Training Accuracy: 0.984375 Loss: 0.001035172026604414\n",
      "Iteration: 1870 Training Accuracy: 1.0 Loss: 0.000476447690743953\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9058333333333334\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 0.96875 Loss: 0.001117590581998229\n",
      "Iteration: 1890 Training Accuracy: 0.96875 Loss: 0.0010220690164715052\n",
      "Iteration: 1900 Training Accuracy: 1.0 Loss: 0.00029394929879345\n",
      "Iteration: 1910 Training Accuracy: 0.984375 Loss: 0.0006444330792874098\n",
      "Iteration: 1920 Training Accuracy: 1.0 Loss: 0.00017239039880223572\n",
      "Iteration: 1930 Training Accuracy: 0.953125 Loss: 0.001495677512139082\n",
      "Iteration: 1940 Training Accuracy: 1.0 Loss: 0.0004019264015369117\n",
      "Iteration: 1950 Training Accuracy: 0.984375 Loss: 0.0005540208076126873\n",
      "Iteration: 1960 Training Accuracy: 1.0 Loss: 0.00012330997560638934\n",
      "Iteration: 1970 Training Accuracy: 1.0 Loss: 0.000286581227555871\n",
      "Iteration: 1980 Training Accuracy: 0.984375 Loss: 0.0005836003692820668\n",
      "Iteration: 1990 Training Accuracy: 1.0 Loss: 0.00041632461943663657\n",
      "Iteration: 2000 Training Accuracy: 1.0 Loss: 0.0006600540364161134\n",
      "Iteration: 2010 Training Accuracy: 0.984375 Loss: 0.0009210762218572199\n",
      "Iteration: 2020 Training Accuracy: 0.984375 Loss: 0.0004570732999127358\n",
      "Iteration: 2030 Training Accuracy: 1.0 Loss: 0.0001156217185780406\n",
      "Iteration: 2040 Training Accuracy: 0.984375 Loss: 0.0004530937294475734\n",
      "Iteration: 2050 Training Accuracy: 0.984375 Loss: 0.000992331188172102\n",
      "Iteration: 2060 Training Accuracy: 0.953125 Loss: 0.0013884310610592365\n",
      "Iteration: 2070 Training Accuracy: 1.0 Loss: 0.000515953463036567\n",
      "Iteration: 2080 Training Accuracy: 1.0 Loss: 2.2851552785141394e-05\n",
      "Iteration: 2090 Training Accuracy: 1.0 Loss: 0.0002849420125130564\n",
      "Iteration: 2100 Training Accuracy: 0.96875 Loss: 0.0008088238537311554\n",
      "Iteration: 2110 Training Accuracy: 1.0 Loss: 0.00036073377123102546\n",
      "Iteration: 2120 Training Accuracy: 0.984375 Loss: 0.001381136942654848\n",
      "Iteration: 2130 Training Accuracy: 1.0 Loss: 0.00015945240738801658\n",
      "Iteration: 2140 Training Accuracy: 1.0 Loss: 0.0002225907810498029\n",
      "Iteration: 2150 Training Accuracy: 1.0 Loss: 6.379640399245545e-05\n",
      "Iteration: 2160 Training Accuracy: 0.984375 Loss: 0.0008679507300257683\n",
      "Iteration: 2170 Training Accuracy: 1.0 Loss: 0.0004226863675285131\n",
      "Iteration: 2180 Training Accuracy: 1.0 Loss: 0.0001088153658201918\n",
      "Iteration: 2190 Training Accuracy: 0.984375 Loss: 0.0006591361598111689\n",
      "Iteration: 2200 Training Accuracy: 1.0 Loss: 0.0006803331198170781\n",
      "Iteration: 2210 Training Accuracy: 1.0 Loss: 0.0003334083594381809\n",
      "Iteration: 2220 Training Accuracy: 1.0 Loss: 0.0005652042455039918\n",
      "Iteration: 2230 Training Accuracy: 1.0 Loss: 4.619510582415387e-05\n",
      "Iteration: 2240 Training Accuracy: 0.984375 Loss: 0.0010107820853590965\n",
      "Iteration: 2250 Training Accuracy: 0.984375 Loss: 0.00035624767770059407\n",
      "Iteration: 2260 Training Accuracy: 1.0 Loss: 0.0004406569350976497\n",
      "Iteration: 2270 Training Accuracy: 1.0 Loss: 0.00046157819451764226\n",
      "Iteration: 2280 Training Accuracy: 1.0 Loss: 0.00010178200318478048\n",
      "Iteration: 2290 Training Accuracy: 1.0 Loss: 0.0004396873409859836\n",
      "Iteration: 2300 Training Accuracy: 1.0 Loss: 0.00017223009490408003\n",
      "Iteration: 2310 Training Accuracy: 0.984375 Loss: 0.0008620422449894249\n",
      "Iteration: 2320 Training Accuracy: 0.984375 Loss: 0.0024546117056161165\n",
      "Iteration: 2330 Training Accuracy: 1.0 Loss: 0.0002543273149058223\n",
      "Iteration: 2340 Training Accuracy: 1.0 Loss: 0.0005033462657593191\n",
      "Iteration: 2350 Training Accuracy: 1.0 Loss: 0.000339820166118443\n",
      "Iteration: 2360 Training Accuracy: 0.984375 Loss: 0.0011888609733432531\n",
      "Iteration: 2370 Training Accuracy: 1.0 Loss: 0.0001514225295977667\n",
      "Iteration: 2380 Training Accuracy: 0.96875 Loss: 0.001789717935025692\n",
      "Iteration: 2390 Training Accuracy: 0.984375 Loss: 0.0005231379764154553\n",
      "Iteration: 2400 Training Accuracy: 1.0 Loss: 0.0004253702354617417\n",
      "Iteration: 2410 Training Accuracy: 1.0 Loss: 0.0003016877162735909\n",
      "Iteration: 2420 Training Accuracy: 1.0 Loss: 0.0002248695818707347\n",
      "Iteration: 2430 Training Accuracy: 1.0 Loss: 0.00010489787382539362\n",
      "Iteration: 2440 Training Accuracy: 1.0 Loss: 0.00028928302344866097\n",
      "Iteration: 2450 Training Accuracy: 0.984375 Loss: 0.00036995369009673595\n",
      "Iteration: 2460 Training Accuracy: 1.0 Loss: 1.2294574844418094e-05\n",
      "Iteration: 2470 Training Accuracy: 1.0 Loss: 0.00019914429867640138\n",
      "Iteration: 2480 Training Accuracy: 0.96875 Loss: 0.0008274836000055075\n",
      "Iteration: 2490 Training Accuracy: 0.984375 Loss: 0.0011208507930859923\n",
      "Iteration: 2500 Training Accuracy: 0.96875 Loss: 0.0012574829161167145\n",
      "Iteration: 2510 Training Accuracy: 0.96875 Loss: 0.0010223775170743465\n",
      "Iteration: 2520 Training Accuracy: 1.0 Loss: 0.00013404861965682358\n",
      "Iteration: 2530 Training Accuracy: 1.0 Loss: 0.0004825353971682489\n",
      "Iteration: 2540 Training Accuracy: 0.984375 Loss: 0.0007687342585995793\n",
      "Iteration: 2550 Training Accuracy: 1.0 Loss: 0.0002876927610486746\n",
      "Iteration: 2560 Training Accuracy: 0.984375 Loss: 0.0009156924206763506\n",
      "Iteration: 2570 Training Accuracy: 0.984375 Loss: 0.00032815791200846434\n",
      "Iteration: 2580 Training Accuracy: 0.96875 Loss: 0.0014043404953554273\n",
      "Iteration: 2590 Training Accuracy: 0.96875 Loss: 0.0005392057937569916\n",
      "Iteration: 2600 Training Accuracy: 0.96875 Loss: 0.001388467033393681\n",
      "Iteration: 2610 Training Accuracy: 0.96875 Loss: 0.0015773905906826258\n",
      "Iteration: 2620 Training Accuracy: 0.96875 Loss: 0.0008431860478594899\n",
      "Iteration: 2630 Training Accuracy: 1.0 Loss: 0.0004970738664269447\n",
      "Iteration: 2640 Training Accuracy: 0.984375 Loss: 0.001329219900071621\n",
      "Iteration: 2650 Training Accuracy: 0.984375 Loss: 0.001144314999692142\n",
      "Iteration: 2660 Training Accuracy: 1.0 Loss: 0.0005087755271233618\n",
      "Iteration: 2670 Training Accuracy: 1.0 Loss: 0.000508495606482029\n",
      "Iteration: 2680 Training Accuracy: 1.0 Loss: 0.00047764298506081104\n",
      "Iteration: 2690 Training Accuracy: 1.0 Loss: 0.00027114441036246717\n",
      "Iteration: 2700 Training Accuracy: 1.0 Loss: 0.00044702127343043685\n",
      "Iteration: 2710 Training Accuracy: 0.96875 Loss: 0.0012479922734200954\n",
      "Iteration: 2720 Training Accuracy: 1.0 Loss: 0.0006035285186953843\n",
      "Iteration: 2730 Training Accuracy: 0.984375 Loss: 0.0007769970688968897\n",
      "Iteration: 2740 Training Accuracy: 0.984375 Loss: 0.0008283907664008439\n",
      "Iteration: 2750 Training Accuracy: 0.984375 Loss: 0.000623469240963459\n",
      "Iteration: 2760 Training Accuracy: 0.96875 Loss: 0.0009567798115313053\n",
      "Iteration: 2770 Training Accuracy: 0.984375 Loss: 0.0013298497069627047\n",
      "Iteration: 2780 Training Accuracy: 1.0 Loss: 0.00021627367823384702\n",
      "Iteration: 2790 Training Accuracy: 1.0 Loss: 0.00021652253053616732\n",
      "Iteration: 2800 Training Accuracy: 1.0 Loss: 0.0007706193719059229\n",
      "Iteration: 2810 Training Accuracy: 1.0 Loss: 0.00010455778101459146\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9105\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 0.984375 Loss: 0.0006722955731675029\n",
      "Iteration: 2830 Training Accuracy: 1.0 Loss: 0.00025805170298554003\n",
      "Iteration: 2840 Training Accuracy: 1.0 Loss: 0.0002961214922834188\n",
      "Iteration: 2850 Training Accuracy: 0.984375 Loss: 0.0009726161952130497\n",
      "Iteration: 2860 Training Accuracy: 1.0 Loss: 0.00013614377530757338\n",
      "Iteration: 2870 Training Accuracy: 1.0 Loss: 0.00014938060485292226\n",
      "Iteration: 2880 Training Accuracy: 1.0 Loss: 0.0008103683358058333\n",
      "Iteration: 2890 Training Accuracy: 0.96875 Loss: 0.0015768249286338687\n",
      "Iteration: 2900 Training Accuracy: 0.984375 Loss: 0.0008463404374197125\n",
      "Iteration: 2910 Training Accuracy: 0.984375 Loss: 0.00044987117871642113\n",
      "Iteration: 2920 Training Accuracy: 0.984375 Loss: 0.0006797725800424814\n",
      "Iteration: 2930 Training Accuracy: 1.0 Loss: 0.000217508029891178\n",
      "Iteration: 2940 Training Accuracy: 0.96875 Loss: 0.0009771824115887284\n",
      "Iteration: 2950 Training Accuracy: 1.0 Loss: 0.00018273942987434566\n",
      "Iteration: 2960 Training Accuracy: 1.0 Loss: 0.00040198295027948916\n",
      "Iteration: 2970 Training Accuracy: 0.96875 Loss: 0.0009464340400882065\n",
      "Iteration: 2980 Training Accuracy: 1.0 Loss: 0.00021740663214586675\n",
      "Iteration: 2990 Training Accuracy: 0.984375 Loss: 0.0007389897364191711\n",
      "Iteration: 3000 Training Accuracy: 1.0 Loss: 0.0003276356146670878\n",
      "Iteration: 3010 Training Accuracy: 1.0 Loss: 0.00023090574541129172\n",
      "Iteration: 3020 Training Accuracy: 0.984375 Loss: 0.0005887795123271644\n",
      "Iteration: 3030 Training Accuracy: 0.984375 Loss: 0.0008451701141893864\n",
      "Iteration: 3040 Training Accuracy: 0.984375 Loss: 0.0010546597186475992\n",
      "Iteration: 3050 Training Accuracy: 0.984375 Loss: 0.00035227328771725297\n",
      "Iteration: 3060 Training Accuracy: 1.0 Loss: 0.0003952241677325219\n",
      "Iteration: 3070 Training Accuracy: 1.0 Loss: 0.00010055331949843094\n",
      "Iteration: 3080 Training Accuracy: 1.0 Loss: 0.0004775988054461777\n",
      "Iteration: 3090 Training Accuracy: 1.0 Loss: 0.00039827547152526677\n",
      "Iteration: 3100 Training Accuracy: 1.0 Loss: 4.333413016865961e-05\n",
      "Iteration: 3110 Training Accuracy: 1.0 Loss: 0.00040248993900604546\n",
      "Iteration: 3120 Training Accuracy: 0.984375 Loss: 0.0007980929221957922\n",
      "Iteration: 3130 Training Accuracy: 1.0 Loss: 0.0003866650222335011\n",
      "Iteration: 3140 Training Accuracy: 1.0 Loss: 6.435234536183998e-05\n",
      "Iteration: 3150 Training Accuracy: 0.96875 Loss: 0.0018903010059148073\n",
      "Iteration: 3160 Training Accuracy: 1.0 Loss: 0.00012948940275236964\n",
      "Iteration: 3170 Training Accuracy: 1.0 Loss: 0.00012888813216704875\n",
      "Iteration: 3180 Training Accuracy: 1.0 Loss: 7.741378067294136e-05\n",
      "Iteration: 3190 Training Accuracy: 1.0 Loss: 0.0001613388303667307\n",
      "Iteration: 3200 Training Accuracy: 0.984375 Loss: 0.0007869342807680368\n",
      "Iteration: 3210 Training Accuracy: 1.0 Loss: 0.00021927169291302562\n",
      "Iteration: 3220 Training Accuracy: 1.0 Loss: 6.47984998067841e-05\n",
      "Iteration: 3230 Training Accuracy: 0.96875 Loss: 0.0007084549870342016\n",
      "Iteration: 3240 Training Accuracy: 0.984375 Loss: 0.0004915745812468231\n",
      "Iteration: 3250 Training Accuracy: 0.984375 Loss: 0.00043276092037558556\n",
      "Iteration: 3260 Training Accuracy: 1.0 Loss: 0.0001222452410729602\n",
      "Iteration: 3270 Training Accuracy: 1.0 Loss: 0.000273216690402478\n",
      "Iteration: 3280 Training Accuracy: 1.0 Loss: 0.00020056494395248592\n",
      "Iteration: 3290 Training Accuracy: 1.0 Loss: 0.0005894599016755819\n",
      "Iteration: 3300 Training Accuracy: 1.0 Loss: 0.000288687035208568\n",
      "Iteration: 3310 Training Accuracy: 1.0 Loss: 0.00021217142057139426\n",
      "Iteration: 3320 Training Accuracy: 0.96875 Loss: 0.001297632697969675\n",
      "Iteration: 3330 Training Accuracy: 0.984375 Loss: 0.0004184809513390064\n",
      "Iteration: 3340 Training Accuracy: 1.0 Loss: 0.0005188760114833713\n",
      "Iteration: 3350 Training Accuracy: 0.984375 Loss: 0.0005605531041510403\n",
      "Iteration: 3360 Training Accuracy: 0.984375 Loss: 0.0013634368078783154\n",
      "Iteration: 3370 Training Accuracy: 1.0 Loss: 4.257716136635281e-05\n",
      "Iteration: 3380 Training Accuracy: 1.0 Loss: 0.00016797486750874668\n",
      "Iteration: 3390 Training Accuracy: 1.0 Loss: 0.0004469427512958646\n",
      "Iteration: 3400 Training Accuracy: 1.0 Loss: 6.979698082432151e-05\n",
      "Iteration: 3410 Training Accuracy: 1.0 Loss: 0.00026878947392106056\n",
      "Iteration: 3420 Training Accuracy: 0.984375 Loss: 0.00043323179124854505\n",
      "Iteration: 3430 Training Accuracy: 1.0 Loss: 8.390858420170844e-05\n",
      "Iteration: 3440 Training Accuracy: 1.0 Loss: 9.142658382188529e-05\n",
      "Iteration: 3450 Training Accuracy: 1.0 Loss: 0.00015998557501006871\n",
      "Iteration: 3460 Training Accuracy: 1.0 Loss: 0.0005570737994275987\n",
      "Iteration: 3470 Training Accuracy: 1.0 Loss: 0.00030837045051157475\n",
      "Iteration: 3480 Training Accuracy: 0.984375 Loss: 0.0004385002830531448\n",
      "Iteration: 3490 Training Accuracy: 1.0 Loss: 0.0003073127882089466\n",
      "Iteration: 3500 Training Accuracy: 0.984375 Loss: 0.001096151303499937\n",
      "Iteration: 3510 Training Accuracy: 1.0 Loss: 0.0003791789058595896\n",
      "Iteration: 3520 Training Accuracy: 1.0 Loss: 4.6370376367121935e-05\n",
      "Iteration: 3530 Training Accuracy: 0.96875 Loss: 0.0013561217347159982\n",
      "Iteration: 3540 Training Accuracy: 1.0 Loss: 0.00032318022567778826\n",
      "Iteration: 3550 Training Accuracy: 0.96875 Loss: 0.00121747562661767\n",
      "Iteration: 3560 Training Accuracy: 0.984375 Loss: 0.0010869259713217616\n",
      "Iteration: 3570 Training Accuracy: 1.0 Loss: 0.0003940151073038578\n",
      "Iteration: 3580 Training Accuracy: 1.0 Loss: 0.00019569380674511194\n",
      "Iteration: 3590 Training Accuracy: 0.984375 Loss: 0.0004708435444626957\n",
      "Iteration: 3600 Training Accuracy: 1.0 Loss: 0.00018457935948390514\n",
      "Iteration: 3610 Training Accuracy: 1.0 Loss: 0.00027343546389602125\n",
      "Iteration: 3620 Training Accuracy: 1.0 Loss: 0.0004167006991337985\n",
      "Iteration: 3630 Training Accuracy: 0.984375 Loss: 0.000663077924400568\n",
      "Iteration: 3640 Training Accuracy: 0.984375 Loss: 0.0005571273504756391\n",
      "Iteration: 3650 Training Accuracy: 1.0 Loss: 0.0003445191541686654\n",
      "Iteration: 3660 Training Accuracy: 1.0 Loss: 0.000220950081711635\n",
      "Iteration: 3670 Training Accuracy: 1.0 Loss: 0.0006204293458722532\n",
      "Iteration: 3680 Training Accuracy: 0.984375 Loss: 0.00045938388211652637\n",
      "Iteration: 3690 Training Accuracy: 1.0 Loss: 0.00019130404689349234\n",
      "Iteration: 3700 Training Accuracy: 0.984375 Loss: 0.0007563602994196117\n",
      "Iteration: 3710 Training Accuracy: 0.96875 Loss: 0.0015550738899037242\n",
      "Iteration: 3720 Training Accuracy: 0.984375 Loss: 0.0011636161943897605\n",
      "Iteration: 3730 Training Accuracy: 0.96875 Loss: 0.0012830538908019662\n",
      "Iteration: 3740 Training Accuracy: 0.96875 Loss: 0.0011504419380798936\n",
      "Iteration: 3750 Training Accuracy: 0.984375 Loss: 0.0003602762590162456\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9136666666666666\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 1.0 Loss: 0.0002249526878586039\n",
      "Iteration: 3770 Training Accuracy: 1.0 Loss: 0.00018277140043210238\n",
      "Iteration: 3780 Training Accuracy: 0.984375 Loss: 0.0010987224522978067\n",
      "Iteration: 3790 Training Accuracy: 1.0 Loss: 0.00027583830524235964\n",
      "Iteration: 3800 Training Accuracy: 0.984375 Loss: 0.0010242790449410677\n",
      "Iteration: 3810 Training Accuracy: 1.0 Loss: 0.00010888566612266004\n",
      "Iteration: 3820 Training Accuracy: 0.953125 Loss: 0.002153439912945032\n",
      "Iteration: 3830 Training Accuracy: 1.0 Loss: 0.0002754796005319804\n",
      "Iteration: 3840 Training Accuracy: 1.0 Loss: 0.00033380871172994375\n",
      "Iteration: 3850 Training Accuracy: 1.0 Loss: 0.00030747134587727487\n",
      "Iteration: 3860 Training Accuracy: 0.984375 Loss: 0.0004801113100256771\n",
      "Iteration: 3870 Training Accuracy: 1.0 Loss: 0.00045624375343322754\n",
      "Iteration: 3880 Training Accuracy: 1.0 Loss: 5.314709051162936e-05\n",
      "Iteration: 3890 Training Accuracy: 0.984375 Loss: 0.0007709016790613532\n",
      "Iteration: 3900 Training Accuracy: 1.0 Loss: 0.0005333012668415904\n",
      "Iteration: 3910 Training Accuracy: 0.984375 Loss: 0.00044435731251724064\n",
      "Iteration: 3920 Training Accuracy: 1.0 Loss: 0.00013905917876400054\n",
      "Iteration: 3930 Training Accuracy: 0.984375 Loss: 0.0005299541517160833\n",
      "Iteration: 3940 Training Accuracy: 1.0 Loss: 0.00025731424102559686\n",
      "Iteration: 3950 Training Accuracy: 1.0 Loss: 0.00021726441627833992\n",
      "Iteration: 3960 Training Accuracy: 1.0 Loss: 5.0439044571248814e-05\n",
      "Iteration: 3970 Training Accuracy: 1.0 Loss: 0.0004239566333126277\n",
      "Iteration: 3980 Training Accuracy: 0.984375 Loss: 0.0005130270728841424\n",
      "Iteration: 3990 Training Accuracy: 0.984375 Loss: 0.0004848507232964039\n",
      "Iteration: 4000 Training Accuracy: 0.984375 Loss: 0.0006851236685179174\n",
      "Iteration: 4010 Training Accuracy: 1.0 Loss: 0.0002242513874080032\n",
      "Iteration: 4020 Training Accuracy: 0.96875 Loss: 0.0018517362186685205\n",
      "Iteration: 4030 Training Accuracy: 1.0 Loss: 0.00031627260614186525\n",
      "Iteration: 4040 Training Accuracy: 1.0 Loss: 0.00011842852109111845\n",
      "Iteration: 4050 Training Accuracy: 1.0 Loss: 0.00010417253361083567\n",
      "Iteration: 4060 Training Accuracy: 1.0 Loss: 0.00014488471788354218\n",
      "Iteration: 4070 Training Accuracy: 0.96875 Loss: 0.001575026661157608\n",
      "Iteration: 4080 Training Accuracy: 1.0 Loss: 6.366148591041565e-05\n",
      "Iteration: 4090 Training Accuracy: 1.0 Loss: 0.00011928485037060454\n",
      "Iteration: 4100 Training Accuracy: 0.984375 Loss: 0.00032948251464404166\n",
      "Iteration: 4110 Training Accuracy: 1.0 Loss: 0.00012960078311152756\n",
      "Iteration: 4120 Training Accuracy: 0.984375 Loss: 0.0005814825417473912\n",
      "Iteration: 4130 Training Accuracy: 1.0 Loss: 0.00042507966281846166\n",
      "Iteration: 4140 Training Accuracy: 0.984375 Loss: 0.0009930923115462065\n",
      "Iteration: 4150 Training Accuracy: 1.0 Loss: 0.00014292937703430653\n",
      "Iteration: 4160 Training Accuracy: 0.984375 Loss: 0.0005645659985020757\n",
      "Iteration: 4170 Training Accuracy: 1.0 Loss: 0.00011444064148236066\n",
      "Iteration: 4180 Training Accuracy: 0.984375 Loss: 0.0007970330188982189\n",
      "Iteration: 4190 Training Accuracy: 1.0 Loss: 0.0004363919433671981\n",
      "Iteration: 4200 Training Accuracy: 0.984375 Loss: 0.0003460403822828084\n",
      "Iteration: 4210 Training Accuracy: 1.0 Loss: 9.918286377796903e-05\n",
      "Iteration: 4220 Training Accuracy: 1.0 Loss: 0.0001680994319031015\n",
      "Iteration: 4230 Training Accuracy: 0.96875 Loss: 0.0007336300914175808\n",
      "Iteration: 4240 Training Accuracy: 0.953125 Loss: 0.001461076783016324\n",
      "Iteration: 4250 Training Accuracy: 1.0 Loss: 0.00024953827960416675\n",
      "Iteration: 4260 Training Accuracy: 1.0 Loss: 0.00017848654533736408\n",
      "Iteration: 4270 Training Accuracy: 0.984375 Loss: 0.0006146403611637652\n",
      "Iteration: 4280 Training Accuracy: 1.0 Loss: 0.0004942512605339289\n",
      "Iteration: 4290 Training Accuracy: 0.984375 Loss: 0.0010361287277191877\n",
      "Iteration: 4300 Training Accuracy: 0.984375 Loss: 0.0010921810753643513\n",
      "Iteration: 4310 Training Accuracy: 1.0 Loss: 0.00028830324299633503\n",
      "Iteration: 4320 Training Accuracy: 1.0 Loss: 0.00025991274742409587\n",
      "Iteration: 4330 Training Accuracy: 1.0 Loss: 0.0005224017659202218\n",
      "Iteration: 4340 Training Accuracy: 1.0 Loss: 0.00031578942434862256\n",
      "Iteration: 4350 Training Accuracy: 0.984375 Loss: 0.0006128749228082597\n",
      "Iteration: 4360 Training Accuracy: 1.0 Loss: 0.00016418246377725154\n",
      "Iteration: 4370 Training Accuracy: 1.0 Loss: 0.0002723736106418073\n",
      "Iteration: 4380 Training Accuracy: 1.0 Loss: 0.0002892105549108237\n",
      "Iteration: 4390 Training Accuracy: 0.984375 Loss: 0.0008096868405118585\n",
      "Iteration: 4400 Training Accuracy: 1.0 Loss: 0.0001708093477645889\n",
      "Iteration: 4410 Training Accuracy: 0.984375 Loss: 0.0006251285085454583\n",
      "Iteration: 4420 Training Accuracy: 0.984375 Loss: 0.000720857409760356\n",
      "Iteration: 4430 Training Accuracy: 1.0 Loss: 3.4270688047399744e-05\n",
      "Iteration: 4440 Training Accuracy: 1.0 Loss: 0.0003851836663670838\n",
      "Iteration: 4450 Training Accuracy: 1.0 Loss: 5.1873088523279876e-05\n",
      "Iteration: 4460 Training Accuracy: 0.96875 Loss: 0.0006344040739350021\n",
      "Iteration: 4470 Training Accuracy: 0.984375 Loss: 0.0005385450785979629\n",
      "Iteration: 4480 Training Accuracy: 0.96875 Loss: 0.0006217315094545484\n",
      "Iteration: 4490 Training Accuracy: 0.984375 Loss: 0.0009709728183224797\n",
      "Iteration: 4500 Training Accuracy: 0.96875 Loss: 0.0009519422892481089\n",
      "Iteration: 4510 Training Accuracy: 1.0 Loss: 0.0002231758990092203\n",
      "Iteration: 4520 Training Accuracy: 1.0 Loss: 0.00023686807253398\n",
      "Iteration: 4530 Training Accuracy: 1.0 Loss: 0.00033761432860046625\n",
      "Iteration: 4540 Training Accuracy: 1.0 Loss: 0.00046062160981819034\n",
      "Iteration: 4550 Training Accuracy: 1.0 Loss: 0.00034340802812948823\n",
      "Iteration: 4560 Training Accuracy: 0.984375 Loss: 0.0005900983233004808\n",
      "Iteration: 4570 Training Accuracy: 1.0 Loss: 0.0005972943035885692\n",
      "Iteration: 4580 Training Accuracy: 1.0 Loss: 8.424412226304412e-05\n",
      "Iteration: 4590 Training Accuracy: 1.0 Loss: 0.0001165561843663454\n",
      "Iteration: 4600 Training Accuracy: 0.984375 Loss: 0.0006675921613350511\n",
      "Iteration: 4610 Training Accuracy: 0.984375 Loss: 0.0005566876498050988\n",
      "Iteration: 4620 Training Accuracy: 1.0 Loss: 0.00031375669641420245\n",
      "Iteration: 4630 Training Accuracy: 0.984375 Loss: 0.000430419750045985\n",
      "Iteration: 4640 Training Accuracy: 0.984375 Loss: 0.0007202320266515017\n",
      "Iteration: 4650 Training Accuracy: 1.0 Loss: 6.380250852089375e-05\n",
      "Iteration: 4660 Training Accuracy: 1.0 Loss: 8.569282363168895e-05\n",
      "Iteration: 4670 Training Accuracy: 1.0 Loss: 0.00021947083587292582\n",
      "Iteration: 4680 Training Accuracy: 1.0 Loss: 0.0004642546409741044\n",
      "Iteration: 4690 Training Accuracy: 1.0 Loss: 0.00021286883566062897\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9148333333333334\n",
      "epoch: 5\n",
      "Iteration: 4700 Training Accuracy: 0.96875 Loss: 0.0009407199686393142\n",
      "Iteration: 4710 Training Accuracy: 1.0 Loss: 9.731743921292946e-05\n",
      "Iteration: 4720 Training Accuracy: 1.0 Loss: 0.00040842179441824555\n",
      "Iteration: 4730 Training Accuracy: 1.0 Loss: 9.213972953148186e-05\n",
      "Iteration: 4740 Training Accuracy: 0.984375 Loss: 0.0008814517641440034\n",
      "Iteration: 4750 Training Accuracy: 1.0 Loss: 0.00045030799810774624\n",
      "Iteration: 4760 Training Accuracy: 0.984375 Loss: 0.0010626423172652721\n",
      "Iteration: 4770 Training Accuracy: 1.0 Loss: 0.00017228577053174376\n",
      "Iteration: 4780 Training Accuracy: 1.0 Loss: 9.697710629552603e-05\n",
      "Iteration: 4790 Training Accuracy: 1.0 Loss: 0.00021900674619246274\n",
      "Iteration: 4800 Training Accuracy: 1.0 Loss: 0.0001491429575253278\n",
      "Iteration: 4810 Training Accuracy: 1.0 Loss: 0.00021676186588592827\n",
      "Iteration: 4820 Training Accuracy: 0.984375 Loss: 0.0007854515570215881\n",
      "Iteration: 4830 Training Accuracy: 0.984375 Loss: 0.0019634219352155924\n",
      "Iteration: 4840 Training Accuracy: 0.984375 Loss: 0.0007996541098691523\n",
      "Iteration: 4850 Training Accuracy: 1.0 Loss: 0.0001272806985070929\n",
      "Iteration: 4860 Training Accuracy: 0.984375 Loss: 0.0006973910494707525\n",
      "Iteration: 4870 Training Accuracy: 0.984375 Loss: 0.0008579004788771272\n",
      "Iteration: 4880 Training Accuracy: 1.0 Loss: 0.00017104053404182196\n",
      "Iteration: 4890 Training Accuracy: 0.96875 Loss: 0.0010240367846563458\n",
      "Iteration: 4900 Training Accuracy: 1.0 Loss: 0.00011200393782928586\n",
      "Iteration: 4910 Training Accuracy: 1.0 Loss: 0.0003233623574487865\n",
      "Iteration: 4920 Training Accuracy: 1.0 Loss: 0.00045423145638778806\n",
      "Iteration: 4930 Training Accuracy: 0.984375 Loss: 0.0007267474429681897\n",
      "Iteration: 4940 Training Accuracy: 1.0 Loss: 0.0002583922469057143\n",
      "Iteration: 4950 Training Accuracy: 1.0 Loss: 8.568432531319559e-05\n",
      "Iteration: 4960 Training Accuracy: 1.0 Loss: 9.003408194985241e-05\n",
      "Iteration: 4970 Training Accuracy: 1.0 Loss: 0.00010338241554563865\n",
      "Iteration: 4980 Training Accuracy: 1.0 Loss: 0.0002172876411350444\n",
      "Iteration: 4990 Training Accuracy: 1.0 Loss: 0.00022724173322785646\n",
      "Iteration: 5000 Training Accuracy: 0.96875 Loss: 0.0008189567015506327\n",
      "Iteration: 5010 Training Accuracy: 1.0 Loss: 0.00023611327924299985\n",
      "Iteration: 5020 Training Accuracy: 1.0 Loss: 7.292389636859298e-05\n",
      "Iteration: 5030 Training Accuracy: 1.0 Loss: 5.276525916997343e-05\n",
      "Iteration: 5040 Training Accuracy: 1.0 Loss: 0.00014984668814577162\n",
      "Iteration: 5050 Training Accuracy: 1.0 Loss: 0.0003250353620387614\n",
      "Iteration: 5060 Training Accuracy: 1.0 Loss: 0.00010765347542474046\n",
      "Iteration: 5070 Training Accuracy: 1.0 Loss: 0.0002485284931026399\n",
      "Iteration: 5080 Training Accuracy: 0.984375 Loss: 0.0003993521095253527\n",
      "Iteration: 5090 Training Accuracy: 1.0 Loss: 0.00015028593770693988\n",
      "Iteration: 5100 Training Accuracy: 1.0 Loss: 0.00018757734505925328\n",
      "Iteration: 5110 Training Accuracy: 1.0 Loss: 0.0003574813308659941\n",
      "Iteration: 5120 Training Accuracy: 1.0 Loss: 0.00012895363033749163\n",
      "Iteration: 5130 Training Accuracy: 1.0 Loss: 4.044808883918449e-05\n",
      "Iteration: 5140 Training Accuracy: 1.0 Loss: 0.0002370054426137358\n",
      "Iteration: 5150 Training Accuracy: 0.984375 Loss: 0.0004178695962764323\n",
      "Iteration: 5160 Training Accuracy: 1.0 Loss: 0.00020777390454895794\n",
      "Iteration: 5170 Training Accuracy: 1.0 Loss: 0.0001858588366303593\n",
      "Iteration: 5180 Training Accuracy: 1.0 Loss: 0.00018300165538676083\n",
      "Iteration: 5190 Training Accuracy: 0.984375 Loss: 0.0006081384490244091\n",
      "Iteration: 5200 Training Accuracy: 1.0 Loss: 0.00010528576967772096\n",
      "Iteration: 5210 Training Accuracy: 1.0 Loss: 8.672615513205528e-05\n",
      "Iteration: 5220 Training Accuracy: 1.0 Loss: 0.00010233990906272084\n",
      "Iteration: 5230 Training Accuracy: 0.984375 Loss: 0.0008133464143611491\n",
      "Iteration: 5240 Training Accuracy: 0.984375 Loss: 0.00033476122189313173\n",
      "Iteration: 5250 Training Accuracy: 1.0 Loss: 0.00019672958296723664\n",
      "Iteration: 5260 Training Accuracy: 0.984375 Loss: 0.0008313012658618391\n",
      "Iteration: 5270 Training Accuracy: 1.0 Loss: 0.00018487768829800189\n",
      "Iteration: 5280 Training Accuracy: 1.0 Loss: 0.00016612766194157302\n",
      "Iteration: 5290 Training Accuracy: 1.0 Loss: 0.0001870892010629177\n",
      "Iteration: 5300 Training Accuracy: 1.0 Loss: 4.291428194846958e-05\n",
      "Iteration: 5310 Training Accuracy: 1.0 Loss: 0.0003824685700237751\n",
      "Iteration: 5320 Training Accuracy: 1.0 Loss: 7.681229908484966e-05\n",
      "Iteration: 5330 Training Accuracy: 1.0 Loss: 7.875631854403764e-05\n",
      "Iteration: 5340 Training Accuracy: 1.0 Loss: 4.7703848395030946e-05\n",
      "Iteration: 5350 Training Accuracy: 0.984375 Loss: 0.0005709296092391014\n",
      "Iteration: 5360 Training Accuracy: 1.0 Loss: 0.00014245025522541255\n",
      "Iteration: 5370 Training Accuracy: 1.0 Loss: 0.00023346079979091883\n",
      "Iteration: 5380 Training Accuracy: 1.0 Loss: 4.292108860681765e-05\n",
      "Iteration: 5390 Training Accuracy: 1.0 Loss: 0.00020387605763971806\n",
      "Iteration: 5400 Training Accuracy: 1.0 Loss: 1.980275737878401e-05\n",
      "Iteration: 5410 Training Accuracy: 1.0 Loss: 0.00024112689425237477\n",
      "Iteration: 5420 Training Accuracy: 1.0 Loss: 0.0005893348134122789\n",
      "Iteration: 5430 Training Accuracy: 1.0 Loss: 8.46513721626252e-05\n",
      "Iteration: 5440 Training Accuracy: 0.984375 Loss: 0.0006082193576730788\n",
      "Iteration: 5450 Training Accuracy: 1.0 Loss: 0.00033745914697647095\n",
      "Iteration: 5460 Training Accuracy: 1.0 Loss: 0.00027351072640158236\n",
      "Iteration: 5470 Training Accuracy: 0.984375 Loss: 0.0006912864628247917\n",
      "Iteration: 5480 Training Accuracy: 1.0 Loss: 6.543260678881779e-05\n",
      "Iteration: 5490 Training Accuracy: 0.9375 Loss: 0.0015951907262206078\n",
      "Iteration: 5500 Training Accuracy: 0.984375 Loss: 0.0006223689415492117\n",
      "Iteration: 5510 Training Accuracy: 1.0 Loss: 0.0004780045128427446\n",
      "Iteration: 5520 Training Accuracy: 0.953125 Loss: 0.0016710840864107013\n",
      "Iteration: 5530 Training Accuracy: 0.96875 Loss: 0.0009953288827091455\n",
      "Iteration: 5540 Training Accuracy: 1.0 Loss: 0.00029000442009419203\n",
      "Iteration: 5550 Training Accuracy: 1.0 Loss: 0.00013327035412658006\n",
      "Iteration: 5560 Training Accuracy: 1.0 Loss: 0.0002761158102657646\n",
      "Iteration: 5570 Training Accuracy: 0.984375 Loss: 0.0002940984268207103\n",
      "Iteration: 5580 Training Accuracy: 1.0 Loss: 0.00028192682657390833\n",
      "Iteration: 5590 Training Accuracy: 0.984375 Loss: 0.00047338518197648227\n",
      "Iteration: 5600 Training Accuracy: 0.984375 Loss: 0.00039223849307745695\n",
      "Iteration: 5610 Training Accuracy: 1.0 Loss: 0.00013117551861796528\n",
      "Iteration: 5620 Training Accuracy: 0.984375 Loss: 0.0008615682017989457\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9158333333333334\n",
      "epoch: 6\n",
      "Iteration: 5630 Training Accuracy: 1.0 Loss: 0.0003069016383960843\n",
      "Iteration: 5640 Training Accuracy: 0.984375 Loss: 0.0007377362344413996\n",
      "Iteration: 5650 Training Accuracy: 0.984375 Loss: 0.00032774038845673203\n",
      "Iteration: 5660 Training Accuracy: 1.0 Loss: 0.0001896193716675043\n",
      "Iteration: 5670 Training Accuracy: 1.0 Loss: 0.0004828409873880446\n",
      "Iteration: 5680 Training Accuracy: 0.96875 Loss: 0.000710897846147418\n",
      "Iteration: 5690 Training Accuracy: 0.984375 Loss: 0.00032397150062024593\n",
      "Iteration: 5700 Training Accuracy: 0.984375 Loss: 0.0006100928294472396\n",
      "Iteration: 5710 Training Accuracy: 1.0 Loss: 1.4697108781547286e-05\n",
      "Iteration: 5720 Training Accuracy: 1.0 Loss: 0.00028775708051398396\n",
      "Iteration: 5730 Training Accuracy: 0.953125 Loss: 0.0013884149957448244\n",
      "Iteration: 5740 Training Accuracy: 1.0 Loss: 0.00019986866391263902\n",
      "Iteration: 5750 Training Accuracy: 1.0 Loss: 0.00018342958355788141\n",
      "Iteration: 5760 Training Accuracy: 0.984375 Loss: 0.0007819097372703254\n",
      "Iteration: 5770 Training Accuracy: 0.984375 Loss: 0.0011185144539922476\n",
      "Iteration: 5780 Training Accuracy: 0.984375 Loss: 0.0006528644589707255\n",
      "Iteration: 5790 Training Accuracy: 1.0 Loss: 0.00023471380700357258\n",
      "Iteration: 5800 Training Accuracy: 1.0 Loss: 8.335363236255944e-05\n",
      "Iteration: 5810 Training Accuracy: 0.96875 Loss: 0.0010243974393233657\n",
      "Iteration: 5820 Training Accuracy: 1.0 Loss: 5.5038730351952836e-05\n",
      "Iteration: 5830 Training Accuracy: 0.984375 Loss: 0.00038648705231025815\n",
      "Iteration: 5840 Training Accuracy: 1.0 Loss: 5.363934542401694e-05\n",
      "Iteration: 5850 Training Accuracy: 1.0 Loss: 0.0002631917013786733\n",
      "Iteration: 5860 Training Accuracy: 0.984375 Loss: 0.00039026254671625793\n",
      "Iteration: 5870 Training Accuracy: 1.0 Loss: 7.73905121604912e-05\n",
      "Iteration: 5880 Training Accuracy: 0.984375 Loss: 0.00040625815745443106\n",
      "Iteration: 5890 Training Accuracy: 1.0 Loss: 0.00015537659055553377\n",
      "Iteration: 5900 Training Accuracy: 1.0 Loss: 0.0003196455945726484\n",
      "Iteration: 5910 Training Accuracy: 1.0 Loss: 0.0006194260204210877\n",
      "Iteration: 5920 Training Accuracy: 1.0 Loss: 9.542590123601258e-05\n",
      "Iteration: 5930 Training Accuracy: 1.0 Loss: 0.0003348106984049082\n",
      "Iteration: 5940 Training Accuracy: 1.0 Loss: 0.0002031290059676394\n",
      "Iteration: 5950 Training Accuracy: 0.984375 Loss: 0.0003818750847131014\n",
      "Iteration: 5960 Training Accuracy: 1.0 Loss: 0.0001645475422265008\n",
      "Iteration: 5970 Training Accuracy: 0.984375 Loss: 0.0003318705130368471\n",
      "Iteration: 5980 Training Accuracy: 1.0 Loss: 0.00020598495029844344\n",
      "Iteration: 5990 Training Accuracy: 1.0 Loss: 6.229699647519737e-05\n",
      "Iteration: 6000 Training Accuracy: 1.0 Loss: 8.79224025993608e-05\n",
      "Iteration: 6010 Training Accuracy: 1.0 Loss: 0.0002583210007287562\n",
      "Iteration: 6020 Training Accuracy: 0.984375 Loss: 0.0003677282074932009\n",
      "Iteration: 6030 Training Accuracy: 1.0 Loss: 0.00028925167862325907\n",
      "Iteration: 6040 Training Accuracy: 0.984375 Loss: 0.0012538001174107194\n",
      "Iteration: 6050 Training Accuracy: 1.0 Loss: 0.0003806960303336382\n",
      "Iteration: 6060 Training Accuracy: 1.0 Loss: 0.0001885208475869149\n",
      "Iteration: 6070 Training Accuracy: 0.984375 Loss: 0.00047743928735144436\n",
      "Iteration: 6080 Training Accuracy: 0.984375 Loss: 0.0011311820708215237\n",
      "Iteration: 6090 Training Accuracy: 0.984375 Loss: 0.0004941808874718845\n",
      "Iteration: 6100 Training Accuracy: 1.0 Loss: 7.221012492664158e-05\n",
      "Iteration: 6110 Training Accuracy: 1.0 Loss: 0.000176705711055547\n",
      "Iteration: 6120 Training Accuracy: 0.984375 Loss: 0.0003907345235347748\n",
      "Iteration: 6130 Training Accuracy: 0.984375 Loss: 0.000649819674436003\n",
      "Iteration: 6140 Training Accuracy: 1.0 Loss: 0.00014586662291549146\n",
      "Iteration: 6150 Training Accuracy: 1.0 Loss: 0.0002797669149003923\n",
      "Iteration: 6160 Training Accuracy: 1.0 Loss: 0.00021243654191493988\n",
      "Iteration: 6170 Training Accuracy: 1.0 Loss: 0.0002035639772657305\n",
      "Iteration: 6180 Training Accuracy: 1.0 Loss: 0.0002261840709252283\n",
      "Iteration: 6190 Training Accuracy: 1.0 Loss: 0.00013349209621082991\n",
      "Iteration: 6200 Training Accuracy: 0.96875 Loss: 0.0007596323266625404\n",
      "Iteration: 6210 Training Accuracy: 1.0 Loss: 0.00026610339409671724\n",
      "Iteration: 6220 Training Accuracy: 1.0 Loss: 0.00017677832511253655\n",
      "Iteration: 6230 Training Accuracy: 1.0 Loss: 0.00018403917783871293\n",
      "Iteration: 6240 Training Accuracy: 1.0 Loss: 5.787121335742995e-05\n",
      "Iteration: 6250 Training Accuracy: 1.0 Loss: 0.0001119810258387588\n",
      "Iteration: 6260 Training Accuracy: 1.0 Loss: 0.0002890580799430609\n",
      "Iteration: 6270 Training Accuracy: 1.0 Loss: 0.00015348699525929987\n",
      "Iteration: 6280 Training Accuracy: 1.0 Loss: 0.0002020504034589976\n",
      "Iteration: 6290 Training Accuracy: 1.0 Loss: 7.963827374624088e-05\n",
      "Iteration: 6300 Training Accuracy: 1.0 Loss: 2.5886116418405436e-05\n",
      "Iteration: 6310 Training Accuracy: 1.0 Loss: 0.00018762884428724647\n",
      "Iteration: 6320 Training Accuracy: 1.0 Loss: 0.00021527406352106482\n",
      "Iteration: 6330 Training Accuracy: 1.0 Loss: 9.024355676956475e-05\n",
      "Iteration: 6340 Training Accuracy: 1.0 Loss: 0.0001060504509950988\n",
      "Iteration: 6350 Training Accuracy: 0.984375 Loss: 0.0004999362281523645\n",
      "Iteration: 6360 Training Accuracy: 1.0 Loss: 3.086046490352601e-05\n",
      "Iteration: 6370 Training Accuracy: 1.0 Loss: 7.451148849213496e-05\n",
      "Iteration: 6380 Training Accuracy: 1.0 Loss: 0.0002555669052526355\n",
      "Iteration: 6390 Training Accuracy: 1.0 Loss: 0.00016287849575746804\n",
      "Iteration: 6400 Training Accuracy: 0.984375 Loss: 0.0005616277339868248\n",
      "Iteration: 6410 Training Accuracy: 1.0 Loss: 1.105300361814443e-05\n",
      "Iteration: 6420 Training Accuracy: 0.984375 Loss: 0.0007955544861033559\n",
      "Iteration: 6430 Training Accuracy: 1.0 Loss: 0.00010398527956567705\n",
      "Iteration: 6440 Training Accuracy: 1.0 Loss: 3.6489898775471374e-05\n",
      "Iteration: 6450 Training Accuracy: 1.0 Loss: 0.0003307671577204019\n",
      "Iteration: 6460 Training Accuracy: 0.984375 Loss: 0.00037580501521006227\n",
      "Iteration: 6470 Training Accuracy: 0.984375 Loss: 0.00037404513568617404\n",
      "Iteration: 6480 Training Accuracy: 0.984375 Loss: 0.0010193989146500826\n",
      "Iteration: 6490 Training Accuracy: 1.0 Loss: 3.355056469445117e-05\n",
      "Iteration: 6500 Training Accuracy: 1.0 Loss: 7.245731831062585e-05\n",
      "Iteration: 6510 Training Accuracy: 1.0 Loss: 0.00040935364086180925\n",
      "Iteration: 6520 Training Accuracy: 1.0 Loss: 0.00015201963833533227\n",
      "Iteration: 6530 Training Accuracy: 1.0 Loss: 0.00012839998817071319\n",
      "Iteration: 6540 Training Accuracy: 0.984375 Loss: 0.0005794177995994687\n",
      "Iteration: 6550 Training Accuracy: 1.0 Loss: 1.8707592971622944e-05\n",
      "Iteration: 6560 Training Accuracy: 0.984375 Loss: 0.0007182533736340702\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9163333333333333\n",
      "epoch: 7\n",
      "Iteration: 6570 Training Accuracy: 0.984375 Loss: 0.0003588538966141641\n",
      "Iteration: 6580 Training Accuracy: 1.0 Loss: 0.0002303744840901345\n",
      "Iteration: 6590 Training Accuracy: 1.0 Loss: 2.96497019007802e-05\n",
      "Iteration: 6600 Training Accuracy: 1.0 Loss: 0.00026211151271127164\n",
      "Iteration: 6610 Training Accuracy: 1.0 Loss: 0.00028407611534930766\n",
      "Iteration: 6620 Training Accuracy: 1.0 Loss: 0.0005638946895487607\n",
      "Iteration: 6630 Training Accuracy: 1.0 Loss: 0.00019570894073694944\n",
      "Iteration: 6640 Training Accuracy: 1.0 Loss: 0.00026397290639579296\n",
      "Iteration: 6650 Training Accuracy: 1.0 Loss: 7.783767796354368e-05\n",
      "Iteration: 6660 Training Accuracy: 0.984375 Loss: 0.0007446471718139946\n",
      "Iteration: 6670 Training Accuracy: 0.984375 Loss: 0.0003408153133932501\n",
      "Iteration: 6680 Training Accuracy: 1.0 Loss: 0.00038060551742091775\n",
      "Iteration: 6690 Training Accuracy: 1.0 Loss: 0.0002361140213906765\n",
      "Iteration: 6700 Training Accuracy: 0.953125 Loss: 0.0016577328788116574\n",
      "Iteration: 6710 Training Accuracy: 1.0 Loss: 0.00017805588140618056\n",
      "Iteration: 6720 Training Accuracy: 1.0 Loss: 4.34869434684515e-05\n",
      "Iteration: 6730 Training Accuracy: 1.0 Loss: 7.915951573522761e-05\n",
      "Iteration: 6740 Training Accuracy: 1.0 Loss: 7.297725096577778e-05\n",
      "Iteration: 6750 Training Accuracy: 1.0 Loss: 0.00022599645308218896\n",
      "Iteration: 6760 Training Accuracy: 1.0 Loss: 0.0004774352128151804\n",
      "Iteration: 6770 Training Accuracy: 1.0 Loss: 3.966763415519381e-06\n",
      "Iteration: 6780 Training Accuracy: 1.0 Loss: 4.575091224978678e-05\n",
      "Iteration: 6790 Training Accuracy: 1.0 Loss: 0.00011932390771107748\n",
      "Iteration: 6800 Training Accuracy: 1.0 Loss: 0.00016147710266523063\n",
      "Iteration: 6810 Training Accuracy: 0.984375 Loss: 0.0005596106639131904\n",
      "Iteration: 6820 Training Accuracy: 1.0 Loss: 5.307169340085238e-05\n",
      "Iteration: 6830 Training Accuracy: 1.0 Loss: 7.097177149262279e-05\n",
      "Iteration: 6840 Training Accuracy: 1.0 Loss: 0.00011524195724632591\n",
      "Iteration: 6850 Training Accuracy: 1.0 Loss: 0.0001307720085605979\n",
      "Iteration: 6860 Training Accuracy: 0.984375 Loss: 0.0003640554496087134\n",
      "Iteration: 6870 Training Accuracy: 1.0 Loss: 9.178697655443102e-05\n",
      "Iteration: 6880 Training Accuracy: 0.984375 Loss: 0.00047405713121406734\n",
      "Iteration: 6890 Training Accuracy: 1.0 Loss: 0.00019580985826905817\n",
      "Iteration: 6900 Training Accuracy: 1.0 Loss: 8.112356590572745e-05\n",
      "Iteration: 6910 Training Accuracy: 0.984375 Loss: 0.00044262048322707415\n",
      "Iteration: 6920 Training Accuracy: 1.0 Loss: 9.19430658541387e-06\n",
      "Iteration: 6930 Training Accuracy: 0.984375 Loss: 0.00029891604208387434\n",
      "Iteration: 6940 Training Accuracy: 1.0 Loss: 0.00012307846918702126\n",
      "Iteration: 6950 Training Accuracy: 1.0 Loss: 0.0001731250958982855\n",
      "Iteration: 6960 Training Accuracy: 1.0 Loss: 0.00010969666618620977\n",
      "Iteration: 6970 Training Accuracy: 1.0 Loss: 4.311642624088563e-05\n",
      "Iteration: 6980 Training Accuracy: 1.0 Loss: 7.245560846058652e-05\n",
      "Iteration: 6990 Training Accuracy: 1.0 Loss: 8.829920989228413e-05\n",
      "Iteration: 7000 Training Accuracy: 1.0 Loss: 5.161245644558221e-05\n",
      "Iteration: 7010 Training Accuracy: 0.96875 Loss: 0.0027144707273691893\n",
      "Iteration: 7020 Training Accuracy: 1.0 Loss: 0.00024413899518549442\n",
      "Iteration: 7030 Training Accuracy: 1.0 Loss: 0.00021158518211450428\n",
      "Iteration: 7040 Training Accuracy: 1.0 Loss: 0.00010983728861901909\n",
      "Iteration: 7050 Training Accuracy: 0.96875 Loss: 0.0006376920500770211\n",
      "Iteration: 7060 Training Accuracy: 1.0 Loss: 0.00010456779273226857\n",
      "Iteration: 7070 Training Accuracy: 0.984375 Loss: 0.0008519162656739354\n",
      "Iteration: 7080 Training Accuracy: 1.0 Loss: 0.00027066306211054325\n",
      "Iteration: 7090 Training Accuracy: 1.0 Loss: 0.00014673997065983713\n",
      "Iteration: 7100 Training Accuracy: 1.0 Loss: 0.00012918032007291913\n",
      "Iteration: 7110 Training Accuracy: 1.0 Loss: 1.4789880879106931e-05\n",
      "Iteration: 7120 Training Accuracy: 1.0 Loss: 8.114132651826367e-05\n",
      "Iteration: 7130 Training Accuracy: 1.0 Loss: 0.00016603922995273024\n",
      "Iteration: 7140 Training Accuracy: 1.0 Loss: 7.25839490769431e-05\n",
      "Iteration: 7150 Training Accuracy: 1.0 Loss: 1.714790641926811e-06\n",
      "Iteration: 7160 Training Accuracy: 1.0 Loss: 5.528267865884118e-05\n",
      "Iteration: 7170 Training Accuracy: 1.0 Loss: 0.00025400350568816066\n",
      "Iteration: 7180 Training Accuracy: 1.0 Loss: 0.0003460919833742082\n",
      "Iteration: 7190 Training Accuracy: 1.0 Loss: 0.00028310352354310453\n",
      "Iteration: 7200 Training Accuracy: 1.0 Loss: 0.00010496209870325401\n",
      "Iteration: 7210 Training Accuracy: 1.0 Loss: 5.3566720453090966e-05\n",
      "Iteration: 7220 Training Accuracy: 1.0 Loss: 4.522852032096125e-05\n",
      "Iteration: 7230 Training Accuracy: 1.0 Loss: 6.177148316055536e-05\n",
      "Iteration: 7240 Training Accuracy: 1.0 Loss: 1.2807931852876209e-05\n",
      "Iteration: 7250 Training Accuracy: 1.0 Loss: 5.852910544490442e-05\n",
      "Iteration: 7260 Training Accuracy: 1.0 Loss: 1.331644944002619e-05\n",
      "Iteration: 7270 Training Accuracy: 1.0 Loss: 0.00024012467474676669\n",
      "Iteration: 7280 Training Accuracy: 0.984375 Loss: 0.00035555267822928727\n",
      "Iteration: 7290 Training Accuracy: 0.984375 Loss: 0.0005625070189125836\n",
      "Iteration: 7300 Training Accuracy: 1.0 Loss: 0.00016136113845277578\n",
      "Iteration: 7310 Training Accuracy: 0.984375 Loss: 0.0004979466903023422\n",
      "Iteration: 7320 Training Accuracy: 1.0 Loss: 3.978463064413518e-05\n",
      "Iteration: 7330 Training Accuracy: 0.96875 Loss: 0.0004968489520251751\n",
      "Iteration: 7340 Training Accuracy: 1.0 Loss: 0.00010950717842206359\n",
      "Iteration: 7350 Training Accuracy: 1.0 Loss: 0.00014166814798954874\n",
      "Iteration: 7360 Training Accuracy: 1.0 Loss: 4.373442061478272e-05\n",
      "Iteration: 7370 Training Accuracy: 1.0 Loss: 7.977815403137356e-05\n",
      "Iteration: 7380 Training Accuracy: 1.0 Loss: 7.504192581109237e-06\n",
      "Iteration: 7390 Training Accuracy: 1.0 Loss: 6.596408638870344e-05\n",
      "Iteration: 7400 Training Accuracy: 1.0 Loss: 0.0001537060597911477\n",
      "Iteration: 7410 Training Accuracy: 1.0 Loss: 0.00021265265240799636\n",
      "Iteration: 7420 Training Accuracy: 1.0 Loss: 7.207513408502564e-05\n",
      "Iteration: 7430 Training Accuracy: 1.0 Loss: 8.244408672908321e-05\n",
      "Iteration: 7440 Training Accuracy: 1.0 Loss: 6.456300616264343e-05\n",
      "Iteration: 7450 Training Accuracy: 1.0 Loss: 8.896455983631313e-05\n",
      "Iteration: 7460 Training Accuracy: 1.0 Loss: 0.00031031089019961655\n",
      "Iteration: 7470 Training Accuracy: 1.0 Loss: 5.91641801293008e-05\n",
      "Iteration: 7480 Training Accuracy: 1.0 Loss: 4.654648364521563e-05\n",
      "Iteration: 7490 Training Accuracy: 1.0 Loss: 7.186116999946535e-05\n",
      "Iteration: 7500 Training Accuracy: 1.0 Loss: 2.5516619643894956e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9275\n",
      "epoch: 8\n",
      "Iteration: 7510 Training Accuracy: 1.0 Loss: 0.0001298444694839418\n",
      "Iteration: 7520 Training Accuracy: 1.0 Loss: 5.6706609029788524e-05\n",
      "Iteration: 7530 Training Accuracy: 1.0 Loss: 1.4719730643264484e-05\n",
      "Iteration: 7540 Training Accuracy: 1.0 Loss: 0.0002828797441907227\n",
      "Iteration: 7550 Training Accuracy: 0.984375 Loss: 0.0002082009013975039\n",
      "Iteration: 7560 Training Accuracy: 1.0 Loss: 4.676655953517184e-05\n",
      "Iteration: 7570 Training Accuracy: 1.0 Loss: 0.000246161303948611\n",
      "Iteration: 7580 Training Accuracy: 1.0 Loss: 0.00019872515986207873\n",
      "Iteration: 7590 Training Accuracy: 0.984375 Loss: 0.00029052013996988535\n",
      "Iteration: 7600 Training Accuracy: 1.0 Loss: 0.0003031518426723778\n",
      "Iteration: 7610 Training Accuracy: 0.984375 Loss: 0.0003345690784044564\n",
      "Iteration: 7620 Training Accuracy: 1.0 Loss: 3.4311015042476356e-05\n",
      "Iteration: 7630 Training Accuracy: 0.984375 Loss: 0.0006835332023911178\n",
      "Iteration: 7640 Training Accuracy: 1.0 Loss: 1.8912298401119187e-05\n",
      "Iteration: 7650 Training Accuracy: 1.0 Loss: 7.747209019726142e-05\n",
      "Iteration: 7660 Training Accuracy: 0.984375 Loss: 0.0004200844850856811\n",
      "Iteration: 7670 Training Accuracy: 1.0 Loss: 8.977662218967453e-05\n",
      "Iteration: 7680 Training Accuracy: 1.0 Loss: 0.00017756223678588867\n",
      "Iteration: 7690 Training Accuracy: 1.0 Loss: 9.246533591067418e-05\n",
      "Iteration: 7700 Training Accuracy: 1.0 Loss: 7.097546040313318e-05\n",
      "Iteration: 7710 Training Accuracy: 1.0 Loss: 0.00021946098422631621\n",
      "Iteration: 7720 Training Accuracy: 1.0 Loss: 0.0003287821600679308\n",
      "Iteration: 7730 Training Accuracy: 0.984375 Loss: 0.0002989552158396691\n",
      "Iteration: 7740 Training Accuracy: 1.0 Loss: 7.101516530383378e-05\n",
      "Iteration: 7750 Training Accuracy: 1.0 Loss: 9.916795534081757e-05\n",
      "Iteration: 7760 Training Accuracy: 1.0 Loss: 3.7828664062544703e-05\n",
      "Iteration: 7770 Training Accuracy: 1.0 Loss: 0.00014562279102392495\n",
      "Iteration: 7780 Training Accuracy: 1.0 Loss: 7.214881770778447e-05\n",
      "Iteration: 7790 Training Accuracy: 1.0 Loss: 8.621470442449208e-06\n",
      "Iteration: 7800 Training Accuracy: 1.0 Loss: 7.773090328555554e-05\n",
      "Iteration: 7810 Training Accuracy: 1.0 Loss: 7.294044917216524e-05\n",
      "Iteration: 7820 Training Accuracy: 1.0 Loss: 0.0001220704725710675\n",
      "Iteration: 7830 Training Accuracy: 1.0 Loss: 7.81045946496306e-06\n",
      "Iteration: 7840 Training Accuracy: 1.0 Loss: 0.00035916990600526333\n",
      "Iteration: 7850 Training Accuracy: 1.0 Loss: 0.0001524531253380701\n",
      "Iteration: 7860 Training Accuracy: 1.0 Loss: 8.138180419337004e-05\n",
      "Iteration: 7870 Training Accuracy: 1.0 Loss: 5.379538197303191e-05\n",
      "Iteration: 7880 Training Accuracy: 1.0 Loss: 0.00024121347814798355\n",
      "Iteration: 7890 Training Accuracy: 1.0 Loss: 0.000218848668737337\n",
      "Iteration: 7900 Training Accuracy: 1.0 Loss: 1.7694730559014715e-05\n",
      "Iteration: 7910 Training Accuracy: 1.0 Loss: 3.6622470361180604e-05\n",
      "Iteration: 7920 Training Accuracy: 1.0 Loss: 4.507622725213878e-05\n",
      "Iteration: 7930 Training Accuracy: 0.984375 Loss: 0.0005509054753929377\n",
      "Iteration: 7940 Training Accuracy: 1.0 Loss: 8.880181121639907e-05\n",
      "Iteration: 7950 Training Accuracy: 1.0 Loss: 4.391059701447375e-05\n",
      "Iteration: 7960 Training Accuracy: 1.0 Loss: 1.6668176613165997e-05\n",
      "Iteration: 7970 Training Accuracy: 1.0 Loss: 2.7921527362195775e-05\n",
      "Iteration: 7980 Training Accuracy: 1.0 Loss: 0.00011867037392221391\n",
      "Iteration: 7990 Training Accuracy: 1.0 Loss: 3.6932953662471846e-05\n",
      "Iteration: 8000 Training Accuracy: 1.0 Loss: 6.0997423133812845e-05\n",
      "Iteration: 8010 Training Accuracy: 1.0 Loss: 0.00031990278512239456\n",
      "Iteration: 8020 Training Accuracy: 1.0 Loss: 0.00011910313332919031\n",
      "Iteration: 8030 Training Accuracy: 1.0 Loss: 0.00011718411406036466\n",
      "Iteration: 8040 Training Accuracy: 0.984375 Loss: 0.00022505725792143494\n",
      "Iteration: 8050 Training Accuracy: 1.0 Loss: 0.00018795757205225527\n",
      "Iteration: 8060 Training Accuracy: 1.0 Loss: 6.228753591130953e-06\n",
      "Iteration: 8070 Training Accuracy: 1.0 Loss: 7.372003892669454e-06\n",
      "Iteration: 8080 Training Accuracy: 1.0 Loss: 0.00010645760630723089\n",
      "Iteration: 8090 Training Accuracy: 1.0 Loss: 2.9188106054789387e-05\n",
      "Iteration: 8100 Training Accuracy: 1.0 Loss: 9.961856994777918e-05\n",
      "Iteration: 8110 Training Accuracy: 1.0 Loss: 4.8632165999151766e-05\n",
      "Iteration: 8120 Training Accuracy: 1.0 Loss: 2.8628814106923528e-05\n",
      "Iteration: 8130 Training Accuracy: 1.0 Loss: 1.1264285603829194e-05\n",
      "Iteration: 8140 Training Accuracy: 1.0 Loss: 0.00010288839985150844\n",
      "Iteration: 8150 Training Accuracy: 1.0 Loss: 0.00013890703849028796\n",
      "Iteration: 8160 Training Accuracy: 1.0 Loss: 6.749660678906366e-05\n",
      "Iteration: 8170 Training Accuracy: 1.0 Loss: 3.731429751496762e-05\n",
      "Iteration: 8180 Training Accuracy: 1.0 Loss: 0.00016535725444555283\n",
      "Iteration: 8190 Training Accuracy: 1.0 Loss: 0.0002476485096849501\n",
      "Iteration: 8200 Training Accuracy: 1.0 Loss: 0.000132836532429792\n",
      "Iteration: 8210 Training Accuracy: 1.0 Loss: 0.00016399483138229698\n",
      "Iteration: 8220 Training Accuracy: 1.0 Loss: 0.00023629877250641584\n",
      "Iteration: 8230 Training Accuracy: 1.0 Loss: 0.00010594322520773858\n",
      "Iteration: 8240 Training Accuracy: 1.0 Loss: 0.00019962475926149637\n",
      "Iteration: 8250 Training Accuracy: 0.984375 Loss: 0.0007115295738913119\n",
      "Iteration: 8260 Training Accuracy: 1.0 Loss: 0.00012524789781309664\n",
      "Iteration: 8270 Training Accuracy: 0.984375 Loss: 0.000302767613902688\n",
      "Iteration: 8280 Training Accuracy: 1.0 Loss: 8.089135371847078e-05\n",
      "Iteration: 8290 Training Accuracy: 0.984375 Loss: 0.00027818026137538254\n",
      "Iteration: 8300 Training Accuracy: 0.984375 Loss: 0.0002688991080503911\n",
      "Iteration: 8310 Training Accuracy: 1.0 Loss: 2.0266241335775703e-05\n",
      "Iteration: 8320 Training Accuracy: 1.0 Loss: 0.00011342488141963258\n",
      "Iteration: 8330 Training Accuracy: 1.0 Loss: 0.00011777890176745132\n",
      "Iteration: 8340 Training Accuracy: 1.0 Loss: 0.00010776935232570395\n",
      "Iteration: 8350 Training Accuracy: 1.0 Loss: 5.267009692033753e-05\n",
      "Iteration: 8360 Training Accuracy: 1.0 Loss: 0.00039443711284548044\n",
      "Iteration: 8370 Training Accuracy: 1.0 Loss: 7.035814633127302e-05\n",
      "Iteration: 8380 Training Accuracy: 1.0 Loss: 7.935537723824382e-05\n",
      "Iteration: 8390 Training Accuracy: 1.0 Loss: 0.00011625584011198953\n",
      "Iteration: 8400 Training Accuracy: 1.0 Loss: 3.754396311705932e-05\n",
      "Iteration: 8410 Training Accuracy: 1.0 Loss: 7.177243242040277e-05\n",
      "Iteration: 8420 Training Accuracy: 1.0 Loss: 0.00014198740245774388\n",
      "Iteration: 8430 Training Accuracy: 1.0 Loss: 5.3042294894112274e-05\n",
      "Iteration: 8440 Training Accuracy: 0.984375 Loss: 0.00032547840964980423\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9285\n",
      "epoch: 9\n",
      "Iteration: 8450 Training Accuracy: 1.0 Loss: 9.500115265836939e-05\n",
      "Iteration: 8460 Training Accuracy: 1.0 Loss: 2.6455931219970807e-05\n",
      "Iteration: 8470 Training Accuracy: 0.984375 Loss: 0.0005437598447315395\n",
      "Iteration: 8480 Training Accuracy: 1.0 Loss: 6.753680645488203e-05\n",
      "Iteration: 8490 Training Accuracy: 1.0 Loss: 9.801055421121418e-05\n",
      "Iteration: 8500 Training Accuracy: 1.0 Loss: 1.6290316125378013e-05\n",
      "Iteration: 8510 Training Accuracy: 1.0 Loss: 0.00038793543353676796\n",
      "Iteration: 8520 Training Accuracy: 1.0 Loss: 0.00017467173165641725\n",
      "Iteration: 8530 Training Accuracy: 1.0 Loss: 0.00022147211711853743\n",
      "Iteration: 8540 Training Accuracy: 1.0 Loss: 7.833927520550787e-05\n",
      "Iteration: 8550 Training Accuracy: 1.0 Loss: 5.7727454986888915e-05\n",
      "Iteration: 8560 Training Accuracy: 1.0 Loss: 0.00022935887682251632\n",
      "Iteration: 8570 Training Accuracy: 1.0 Loss: 7.249952614074573e-05\n",
      "Iteration: 8580 Training Accuracy: 1.0 Loss: 9.277002391172573e-05\n",
      "Iteration: 8590 Training Accuracy: 1.0 Loss: 0.00017923512496054173\n",
      "Iteration: 8600 Training Accuracy: 1.0 Loss: 2.822677924996242e-05\n",
      "Iteration: 8610 Training Accuracy: 1.0 Loss: 0.0001503790117567405\n",
      "Iteration: 8620 Training Accuracy: 1.0 Loss: 3.826909960480407e-05\n",
      "Iteration: 8630 Training Accuracy: 1.0 Loss: 6.448932981584221e-05\n",
      "Iteration: 8640 Training Accuracy: 1.0 Loss: 5.242469342192635e-05\n",
      "Iteration: 8650 Training Accuracy: 1.0 Loss: 2.242206028313376e-05\n",
      "Iteration: 8660 Training Accuracy: 1.0 Loss: 3.7282443372532725e-05\n",
      "Iteration: 8670 Training Accuracy: 1.0 Loss: 0.00017329970432911068\n",
      "Iteration: 8680 Training Accuracy: 1.0 Loss: 0.0001650087215239182\n",
      "Iteration: 8690 Training Accuracy: 1.0 Loss: 0.00016322404553648084\n",
      "Iteration: 8700 Training Accuracy: 1.0 Loss: 8.108549809549004e-05\n",
      "Iteration: 8710 Training Accuracy: 1.0 Loss: 0.0001992143807001412\n",
      "Iteration: 8720 Training Accuracy: 1.0 Loss: 0.000184212505700998\n",
      "Iteration: 8730 Training Accuracy: 1.0 Loss: 4.0978382457979023e-05\n",
      "Iteration: 8740 Training Accuracy: 1.0 Loss: 6.952737021492794e-05\n",
      "Iteration: 8750 Training Accuracy: 1.0 Loss: 0.00012087938375771046\n",
      "Iteration: 8760 Training Accuracy: 1.0 Loss: 0.00032754725543782115\n",
      "Iteration: 8770 Training Accuracy: 1.0 Loss: 2.2106967662693933e-05\n",
      "Iteration: 8780 Training Accuracy: 1.0 Loss: 7.331502274610102e-06\n",
      "Iteration: 8790 Training Accuracy: 1.0 Loss: 3.524391649989411e-05\n",
      "Iteration: 8800 Training Accuracy: 1.0 Loss: 3.2370906410505995e-05\n",
      "Iteration: 8810 Training Accuracy: 1.0 Loss: 0.00024962681345641613\n",
      "Iteration: 8820 Training Accuracy: 1.0 Loss: 0.00010488275438547134\n",
      "Iteration: 8830 Training Accuracy: 1.0 Loss: 0.00016561977099627256\n",
      "Iteration: 8840 Training Accuracy: 1.0 Loss: 0.00021298302453942597\n",
      "Iteration: 8850 Training Accuracy: 1.0 Loss: 0.0002518166438676417\n",
      "Iteration: 8860 Training Accuracy: 1.0 Loss: 1.3339993529370986e-05\n",
      "Iteration: 8870 Training Accuracy: 1.0 Loss: 0.00010289352940162644\n",
      "Iteration: 8880 Training Accuracy: 1.0 Loss: 5.0057133194059134e-05\n",
      "Iteration: 8890 Training Accuracy: 1.0 Loss: 0.00035931862657889724\n",
      "Iteration: 8900 Training Accuracy: 1.0 Loss: 1.4538055438606534e-05\n",
      "Iteration: 8910 Training Accuracy: 1.0 Loss: 8.098434045678005e-05\n",
      "Iteration: 8920 Training Accuracy: 0.984375 Loss: 0.00027419463731348515\n",
      "Iteration: 8930 Training Accuracy: 1.0 Loss: 7.728439231868833e-05\n",
      "Iteration: 8940 Training Accuracy: 1.0 Loss: 4.345043998910114e-05\n",
      "Iteration: 8950 Training Accuracy: 1.0 Loss: 0.0001411494449712336\n",
      "Iteration: 8960 Training Accuracy: 1.0 Loss: 7.804159395163879e-05\n",
      "Iteration: 8970 Training Accuracy: 1.0 Loss: 0.00011770764831453562\n",
      "Iteration: 8980 Training Accuracy: 1.0 Loss: 0.00020954731735400856\n",
      "Iteration: 8990 Training Accuracy: 0.984375 Loss: 0.0006229369901120663\n",
      "Iteration: 9000 Training Accuracy: 1.0 Loss: 5.702691851183772e-05\n",
      "Iteration: 9010 Training Accuracy: 1.0 Loss: 0.00013836802099831402\n",
      "Iteration: 9020 Training Accuracy: 1.0 Loss: 4.318619176046923e-05\n",
      "Iteration: 9030 Training Accuracy: 1.0 Loss: 6.002251757308841e-05\n",
      "Iteration: 9040 Training Accuracy: 1.0 Loss: 0.00011333596194162965\n",
      "Iteration: 9050 Training Accuracy: 1.0 Loss: 9.785622387425974e-05\n",
      "Iteration: 9060 Training Accuracy: 1.0 Loss: 3.6123223253525794e-05\n",
      "Iteration: 9070 Training Accuracy: 1.0 Loss: 6.85362028889358e-05\n",
      "Iteration: 9080 Training Accuracy: 1.0 Loss: 0.0002711154520511627\n",
      "Iteration: 9090 Training Accuracy: 1.0 Loss: 8.857191460265312e-06\n",
      "Iteration: 9100 Training Accuracy: 1.0 Loss: 0.00012245729158166796\n",
      "Iteration: 9110 Training Accuracy: 1.0 Loss: 0.00025167904095724225\n",
      "Iteration: 9120 Training Accuracy: 1.0 Loss: 8.189344953279942e-06\n",
      "Iteration: 9130 Training Accuracy: 1.0 Loss: 3.4002507163677365e-05\n",
      "Iteration: 9140 Training Accuracy: 1.0 Loss: 2.044882785412483e-05\n",
      "Iteration: 9150 Training Accuracy: 1.0 Loss: 0.00015574270219076425\n",
      "Iteration: 9160 Training Accuracy: 1.0 Loss: 0.00025081521016545594\n",
      "Iteration: 9170 Training Accuracy: 1.0 Loss: 4.5865173888159916e-05\n",
      "Iteration: 9180 Training Accuracy: 1.0 Loss: 0.00016630410391371697\n",
      "Iteration: 9190 Training Accuracy: 1.0 Loss: 8.912655175663531e-05\n",
      "Iteration: 9200 Training Accuracy: 1.0 Loss: 0.00016263329598587006\n",
      "Iteration: 9210 Training Accuracy: 1.0 Loss: 8.738102769711986e-05\n",
      "Iteration: 9220 Training Accuracy: 1.0 Loss: 0.00014868845755700022\n",
      "Iteration: 9230 Training Accuracy: 1.0 Loss: 0.00011539296247065067\n",
      "Iteration: 9240 Training Accuracy: 1.0 Loss: 8.929761679610237e-05\n",
      "Iteration: 9250 Training Accuracy: 1.0 Loss: 0.000189002399565652\n",
      "Iteration: 9260 Training Accuracy: 0.984375 Loss: 0.0004468430415727198\n",
      "Iteration: 9270 Training Accuracy: 1.0 Loss: 3.732127424882492e-06\n",
      "Iteration: 9280 Training Accuracy: 1.0 Loss: 2.8067051971447654e-05\n",
      "Iteration: 9290 Training Accuracy: 1.0 Loss: 0.00016087680705823004\n",
      "Iteration: 9300 Training Accuracy: 1.0 Loss: 0.0002816616033669561\n",
      "Iteration: 9310 Training Accuracy: 1.0 Loss: 0.00019612637697719038\n",
      "Iteration: 9320 Training Accuracy: 1.0 Loss: 0.00013387718354351819\n",
      "Iteration: 9330 Training Accuracy: 1.0 Loss: 8.897388761397451e-05\n",
      "Iteration: 9340 Training Accuracy: 1.0 Loss: 3.238784120185301e-05\n",
      "Iteration: 9350 Training Accuracy: 1.0 Loss: 3.511133400024846e-05\n",
      "Iteration: 9360 Training Accuracy: 1.0 Loss: 6.923804903635755e-05\n",
      "Iteration: 9370 Training Accuracy: 1.0 Loss: 0.0001874786103144288\n",
      "Iteration: 9380 Training Accuracy: 1.0 Loss: 1.7502245100331493e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9281666666666667\n",
      "epoch: 10\n",
      "Iteration: 9390 Training Accuracy: 1.0 Loss: 0.0001409879478160292\n",
      "Iteration: 9400 Training Accuracy: 1.0 Loss: 1.7511936675873585e-05\n",
      "Iteration: 9410 Training Accuracy: 1.0 Loss: 0.0003650719881989062\n",
      "Iteration: 9420 Training Accuracy: 1.0 Loss: 3.7683868868043646e-05\n",
      "Iteration: 9430 Training Accuracy: 1.0 Loss: 9.295911877416074e-05\n",
      "Iteration: 9440 Training Accuracy: 1.0 Loss: 0.0001320785959251225\n",
      "Iteration: 9450 Training Accuracy: 1.0 Loss: 8.245743811130524e-05\n",
      "Iteration: 9460 Training Accuracy: 1.0 Loss: 0.00010904572991421446\n",
      "Iteration: 9470 Training Accuracy: 1.0 Loss: 6.177998875500634e-05\n",
      "Iteration: 9480 Training Accuracy: 1.0 Loss: 6.475260306615382e-05\n",
      "Iteration: 9490 Training Accuracy: 1.0 Loss: 6.863434100523591e-05\n",
      "Iteration: 9500 Training Accuracy: 1.0 Loss: 0.00010160342935705557\n",
      "Iteration: 9510 Training Accuracy: 1.0 Loss: 0.0002097990654874593\n",
      "Iteration: 9520 Training Accuracy: 1.0 Loss: 0.00031426234636455774\n",
      "Iteration: 9530 Training Accuracy: 1.0 Loss: 0.00014546934107784182\n",
      "Iteration: 9540 Training Accuracy: 1.0 Loss: 1.6682863133610226e-05\n",
      "Iteration: 9550 Training Accuracy: 1.0 Loss: 0.0001223735889652744\n",
      "Iteration: 9560 Training Accuracy: 1.0 Loss: 0.0002599311410449445\n",
      "Iteration: 9570 Training Accuracy: 1.0 Loss: 2.8242366170161404e-05\n",
      "Iteration: 9580 Training Accuracy: 1.0 Loss: 0.0003640913055278361\n",
      "Iteration: 9590 Training Accuracy: 1.0 Loss: 5.386444536270574e-05\n",
      "Iteration: 9600 Training Accuracy: 1.0 Loss: 7.585226558148861e-05\n",
      "Iteration: 9610 Training Accuracy: 1.0 Loss: 0.00026827657711692154\n",
      "Iteration: 9620 Training Accuracy: 1.0 Loss: 0.00025105109671130776\n",
      "Iteration: 9630 Training Accuracy: 1.0 Loss: 5.772299846285023e-05\n",
      "Iteration: 9640 Training Accuracy: 1.0 Loss: 7.661255949642509e-05\n",
      "Iteration: 9650 Training Accuracy: 1.0 Loss: 2.852080069715157e-05\n",
      "Iteration: 9660 Training Accuracy: 1.0 Loss: 0.00012826842430513352\n",
      "Iteration: 9670 Training Accuracy: 1.0 Loss: 0.00010576323256827891\n",
      "Iteration: 9680 Training Accuracy: 0.984375 Loss: 0.0003590935666579753\n",
      "Iteration: 9690 Training Accuracy: 0.984375 Loss: 0.0005959356785751879\n",
      "Iteration: 9700 Training Accuracy: 1.0 Loss: 7.526260014856234e-05\n",
      "Iteration: 9710 Training Accuracy: 1.0 Loss: 2.875633435905911e-05\n",
      "Iteration: 9720 Training Accuracy: 1.0 Loss: 1.192734362120973e-05\n",
      "Iteration: 9730 Training Accuracy: 1.0 Loss: 2.0542747733998112e-05\n",
      "Iteration: 9740 Training Accuracy: 1.0 Loss: 3.9339778595604e-05\n",
      "Iteration: 9750 Training Accuracy: 1.0 Loss: 5.070518818683922e-05\n",
      "Iteration: 9760 Training Accuracy: 1.0 Loss: 0.0003739972598850727\n",
      "Iteration: 9770 Training Accuracy: 0.984375 Loss: 0.00027239220798946917\n",
      "Iteration: 9780 Training Accuracy: 1.0 Loss: 5.826589767821133e-05\n",
      "Iteration: 9790 Training Accuracy: 1.0 Loss: 1.794660602172371e-05\n",
      "Iteration: 9800 Training Accuracy: 1.0 Loss: 0.00012624214286915958\n",
      "Iteration: 9810 Training Accuracy: 1.0 Loss: 9.355772635899484e-05\n",
      "Iteration: 9820 Training Accuracy: 1.0 Loss: 3.673675746540539e-05\n",
      "Iteration: 9830 Training Accuracy: 1.0 Loss: 0.00017961813136935234\n",
      "Iteration: 9840 Training Accuracy: 1.0 Loss: 2.4859806217136793e-05\n",
      "Iteration: 9850 Training Accuracy: 1.0 Loss: 5.296280869515613e-05\n",
      "Iteration: 9860 Training Accuracy: 1.0 Loss: 0.00010734633542597294\n",
      "Iteration: 9870 Training Accuracy: 1.0 Loss: 9.034456161316484e-05\n",
      "Iteration: 9880 Training Accuracy: 0.984375 Loss: 0.0004546745913103223\n",
      "Iteration: 9890 Training Accuracy: 1.0 Loss: 1.8893002561526373e-05\n",
      "Iteration: 9900 Training Accuracy: 1.0 Loss: 1.6736506950110197e-05\n",
      "Iteration: 9910 Training Accuracy: 1.0 Loss: 9.444672468816862e-05\n",
      "Iteration: 9920 Training Accuracy: 1.0 Loss: 2.1757903596153483e-05\n",
      "Iteration: 9930 Training Accuracy: 1.0 Loss: 4.850120240007527e-05\n",
      "Iteration: 9940 Training Accuracy: 1.0 Loss: 4.292402809369378e-05\n",
      "Iteration: 9950 Training Accuracy: 0.984375 Loss: 0.0005867633735761046\n",
      "Iteration: 9960 Training Accuracy: 1.0 Loss: 0.00010521347576286644\n",
      "Iteration: 9970 Training Accuracy: 1.0 Loss: 6.130742985988036e-05\n",
      "Iteration: 9980 Training Accuracy: 1.0 Loss: 0.00010074237070512027\n",
      "Iteration: 9990 Training Accuracy: 1.0 Loss: 3.941150862374343e-05\n",
      "Iteration: 10000 Training Accuracy: 1.0 Loss: 0.00020054932974744588\n",
      "Iteration: 10010 Training Accuracy: 1.0 Loss: 7.28468075976707e-05\n",
      "Iteration: 10020 Training Accuracy: 1.0 Loss: 4.096728298463859e-05\n",
      "Iteration: 10030 Training Accuracy: 1.0 Loss: 6.643868255196139e-05\n",
      "Iteration: 10040 Training Accuracy: 1.0 Loss: 0.0002571144432295114\n",
      "Iteration: 10050 Training Accuracy: 1.0 Loss: 4.589725722325966e-05\n",
      "Iteration: 10060 Training Accuracy: 1.0 Loss: 7.532873132731766e-05\n",
      "Iteration: 10070 Training Accuracy: 1.0 Loss: 7.3565352067817e-05\n",
      "Iteration: 10080 Training Accuracy: 1.0 Loss: 9.132574632531032e-05\n",
      "Iteration: 10090 Training Accuracy: 1.0 Loss: 4.136643838137388e-05\n",
      "Iteration: 10100 Training Accuracy: 1.0 Loss: 9.871958900475875e-05\n",
      "Iteration: 10110 Training Accuracy: 1.0 Loss: 0.0001472493604524061\n",
      "Iteration: 10120 Training Accuracy: 1.0 Loss: 2.3201089788926765e-05\n",
      "Iteration: 10130 Training Accuracy: 1.0 Loss: 0.0001033943917718716\n",
      "Iteration: 10140 Training Accuracy: 1.0 Loss: 1.9103175873169675e-05\n",
      "Iteration: 10150 Training Accuracy: 1.0 Loss: 8.976866229204461e-05\n",
      "Iteration: 10160 Training Accuracy: 1.0 Loss: 0.00028896357980556786\n",
      "Iteration: 10170 Training Accuracy: 1.0 Loss: 1.6603502444922924e-05\n",
      "Iteration: 10180 Training Accuracy: 1.0 Loss: 0.00022180532687343657\n",
      "Iteration: 10190 Training Accuracy: 1.0 Loss: 7.735990220680833e-05\n",
      "Iteration: 10200 Training Accuracy: 1.0 Loss: 0.00012164123472757638\n",
      "Iteration: 10210 Training Accuracy: 1.0 Loss: 6.226397817954421e-05\n",
      "Iteration: 10220 Training Accuracy: 1.0 Loss: 0.00031127536203712225\n",
      "Iteration: 10230 Training Accuracy: 1.0 Loss: 0.00013038679026067257\n",
      "Iteration: 10240 Training Accuracy: 1.0 Loss: 1.824186256271787e-05\n",
      "Iteration: 10250 Training Accuracy: 1.0 Loss: 3.836921678157523e-05\n",
      "Iteration: 10260 Training Accuracy: 1.0 Loss: 0.00011369819549145177\n",
      "Iteration: 10270 Training Accuracy: 1.0 Loss: 3.479838778730482e-05\n",
      "Iteration: 10280 Training Accuracy: 1.0 Loss: 0.00012128303933423012\n",
      "Iteration: 10290 Training Accuracy: 1.0 Loss: 0.00015358885866589844\n",
      "Iteration: 10300 Training Accuracy: 1.0 Loss: 0.00010820535680977628\n",
      "Iteration: 10310 Training Accuracy: 1.0 Loss: 2.7349480660632253e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9278333333333333\n",
      "epoch: 11\n",
      "Iteration: 10320 Training Accuracy: 1.0 Loss: 9.943630720954388e-05\n",
      "Iteration: 10330 Training Accuracy: 1.0 Loss: 0.0002261180052300915\n",
      "Iteration: 10340 Training Accuracy: 1.0 Loss: 0.0001341156312264502\n",
      "Iteration: 10350 Training Accuracy: 1.0 Loss: 6.220603972906247e-05\n",
      "Iteration: 10360 Training Accuracy: 1.0 Loss: 4.591166361933574e-05\n",
      "Iteration: 10370 Training Accuracy: 1.0 Loss: 6.991518603172153e-05\n",
      "Iteration: 10380 Training Accuracy: 1.0 Loss: 1.7836038750829175e-05\n",
      "Iteration: 10390 Training Accuracy: 1.0 Loss: 6.126621155999601e-05\n",
      "Iteration: 10400 Training Accuracy: 1.0 Loss: 4.957068995281588e-06\n",
      "Iteration: 10410 Training Accuracy: 1.0 Loss: 0.00014219313743524253\n",
      "Iteration: 10420 Training Accuracy: 1.0 Loss: 0.00037574683665297925\n",
      "Iteration: 10430 Training Accuracy: 1.0 Loss: 4.981132588000037e-05\n",
      "Iteration: 10440 Training Accuracy: 1.0 Loss: 2.0589881387422793e-05\n",
      "Iteration: 10450 Training Accuracy: 1.0 Loss: 0.0001985820708796382\n",
      "Iteration: 10460 Training Accuracy: 1.0 Loss: 4.159625314059667e-05\n",
      "Iteration: 10470 Training Accuracy: 1.0 Loss: 0.00018211593851447105\n",
      "Iteration: 10480 Training Accuracy: 1.0 Loss: 0.00017805950483307242\n",
      "Iteration: 10490 Training Accuracy: 1.0 Loss: 2.658144512679428e-05\n",
      "Iteration: 10500 Training Accuracy: 0.984375 Loss: 0.0008489143801853061\n",
      "Iteration: 10510 Training Accuracy: 1.0 Loss: 2.5739815100678243e-05\n",
      "Iteration: 10520 Training Accuracy: 1.0 Loss: 1.512557537353132e-05\n",
      "Iteration: 10530 Training Accuracy: 1.0 Loss: 4.980372614227235e-05\n",
      "Iteration: 10540 Training Accuracy: 1.0 Loss: 0.00021200114861130714\n",
      "Iteration: 10550 Training Accuracy: 1.0 Loss: 8.657592843519524e-05\n",
      "Iteration: 10560 Training Accuracy: 1.0 Loss: 3.5139852116117254e-05\n",
      "Iteration: 10570 Training Accuracy: 1.0 Loss: 0.00010790461965370923\n",
      "Iteration: 10580 Training Accuracy: 1.0 Loss: 5.2756025979761034e-05\n",
      "Iteration: 10590 Training Accuracy: 1.0 Loss: 8.673340198583901e-05\n",
      "Iteration: 10600 Training Accuracy: 1.0 Loss: 0.0001938298810273409\n",
      "Iteration: 10610 Training Accuracy: 1.0 Loss: 4.7285146138165146e-05\n",
      "Iteration: 10620 Training Accuracy: 1.0 Loss: 3.106348594883457e-05\n",
      "Iteration: 10630 Training Accuracy: 1.0 Loss: 0.00010725174797698855\n",
      "Iteration: 10640 Training Accuracy: 1.0 Loss: 0.0001787594665074721\n",
      "Iteration: 10650 Training Accuracy: 1.0 Loss: 3.677587301353924e-05\n",
      "Iteration: 10660 Training Accuracy: 1.0 Loss: 8.25691968202591e-05\n",
      "Iteration: 10670 Training Accuracy: 1.0 Loss: 3.370879858266562e-05\n",
      "Iteration: 10680 Training Accuracy: 1.0 Loss: 3.7012887332821265e-05\n",
      "Iteration: 10690 Training Accuracy: 1.0 Loss: 2.1114003175171092e-05\n",
      "Iteration: 10700 Training Accuracy: 1.0 Loss: 0.000261981796938926\n",
      "Iteration: 10710 Training Accuracy: 1.0 Loss: 0.00014320354966912419\n",
      "Iteration: 10720 Training Accuracy: 1.0 Loss: 0.00014413014287129045\n",
      "Iteration: 10730 Training Accuracy: 1.0 Loss: 0.0001956513151526451\n",
      "Iteration: 10740 Training Accuracy: 1.0 Loss: 6.4738022047095e-05\n",
      "Iteration: 10750 Training Accuracy: 1.0 Loss: 7.328409265028313e-05\n",
      "Iteration: 10760 Training Accuracy: 1.0 Loss: 8.314186561619863e-05\n",
      "Iteration: 10770 Training Accuracy: 1.0 Loss: 1.6704858353477903e-05\n",
      "Iteration: 10780 Training Accuracy: 1.0 Loss: 4.0124890801962465e-05\n",
      "Iteration: 10790 Training Accuracy: 1.0 Loss: 1.2828085345972795e-05\n",
      "Iteration: 10800 Training Accuracy: 1.0 Loss: 4.3951833504252136e-05\n",
      "Iteration: 10810 Training Accuracy: 1.0 Loss: 0.00010007964010583237\n",
      "Iteration: 10820 Training Accuracy: 1.0 Loss: 0.0003091295948252082\n",
      "Iteration: 10830 Training Accuracy: 1.0 Loss: 7.862296479288489e-05\n",
      "Iteration: 10840 Training Accuracy: 1.0 Loss: 0.00011702031770255417\n",
      "Iteration: 10850 Training Accuracy: 1.0 Loss: 0.00013494091399479657\n",
      "Iteration: 10860 Training Accuracy: 1.0 Loss: 6.409136403817683e-05\n",
      "Iteration: 10870 Training Accuracy: 1.0 Loss: 5.123433220433071e-05\n",
      "Iteration: 10880 Training Accuracy: 1.0 Loss: 1.8441502106725238e-05\n",
      "Iteration: 10890 Training Accuracy: 1.0 Loss: 8.763221558183432e-05\n",
      "Iteration: 10900 Training Accuracy: 1.0 Loss: 4.719692515209317e-05\n",
      "Iteration: 10910 Training Accuracy: 1.0 Loss: 3.77516626031138e-05\n",
      "Iteration: 10920 Training Accuracy: 1.0 Loss: 0.00010189118620473891\n",
      "Iteration: 10930 Training Accuracy: 1.0 Loss: 5.959669579169713e-05\n",
      "Iteration: 10940 Training Accuracy: 1.0 Loss: 4.4787848310079426e-05\n",
      "Iteration: 10950 Training Accuracy: 1.0 Loss: 0.00011088715109508485\n",
      "Iteration: 10960 Training Accuracy: 1.0 Loss: 7.02505130902864e-05\n",
      "Iteration: 10970 Training Accuracy: 1.0 Loss: 1.2814070942113176e-05\n",
      "Iteration: 10980 Training Accuracy: 1.0 Loss: 1.3022455277678091e-05\n",
      "Iteration: 10990 Training Accuracy: 1.0 Loss: 7.062837539706379e-05\n",
      "Iteration: 11000 Training Accuracy: 1.0 Loss: 8.258019079221413e-05\n",
      "Iteration: 11010 Training Accuracy: 1.0 Loss: 0.00018441418069414794\n",
      "Iteration: 11020 Training Accuracy: 0.984375 Loss: 0.0003047947830054909\n",
      "Iteration: 11030 Training Accuracy: 1.0 Loss: 0.00011753314174711704\n",
      "Iteration: 11040 Training Accuracy: 1.0 Loss: 3.896769703715108e-05\n",
      "Iteration: 11050 Training Accuracy: 1.0 Loss: 9.884953033179045e-05\n",
      "Iteration: 11060 Training Accuracy: 1.0 Loss: 1.2765059182129335e-05\n",
      "Iteration: 11070 Training Accuracy: 1.0 Loss: 5.518801845028065e-05\n",
      "Iteration: 11080 Training Accuracy: 1.0 Loss: 5.2079249144298956e-05\n",
      "Iteration: 11090 Training Accuracy: 1.0 Loss: 4.143629485042766e-05\n",
      "Iteration: 11100 Training Accuracy: 1.0 Loss: 6.485002086265013e-05\n",
      "Iteration: 11110 Training Accuracy: 1.0 Loss: 0.00019982732192147523\n",
      "Iteration: 11120 Training Accuracy: 1.0 Loss: 3.4503842471167445e-05\n",
      "Iteration: 11130 Training Accuracy: 1.0 Loss: 1.1702335541485809e-05\n",
      "Iteration: 11140 Training Accuracy: 1.0 Loss: 7.304698374355212e-05\n",
      "Iteration: 11150 Training Accuracy: 1.0 Loss: 0.00012931103992741555\n",
      "Iteration: 11160 Training Accuracy: 1.0 Loss: 0.0002965701278299093\n",
      "Iteration: 11170 Training Accuracy: 1.0 Loss: 6.865991599624977e-05\n",
      "Iteration: 11180 Training Accuracy: 1.0 Loss: 2.1910394934820943e-05\n",
      "Iteration: 11190 Training Accuracy: 1.0 Loss: 8.82179374457337e-05\n",
      "Iteration: 11200 Training Accuracy: 1.0 Loss: 8.681474719196558e-05\n",
      "Iteration: 11210 Training Accuracy: 1.0 Loss: 2.0563144062180072e-05\n",
      "Iteration: 11220 Training Accuracy: 1.0 Loss: 8.513846842106432e-05\n",
      "Iteration: 11230 Training Accuracy: 1.0 Loss: 0.0002620376762934029\n",
      "Iteration: 11240 Training Accuracy: 1.0 Loss: 2.9607443138957024e-05\n",
      "Iteration: 11250 Training Accuracy: 1.0 Loss: 5.019827949581668e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9283333333333333\n",
      "epoch: 12\n",
      "Iteration: 11260 Training Accuracy: 1.0 Loss: 8.205453923437744e-05\n",
      "Iteration: 11270 Training Accuracy: 1.0 Loss: 7.960558286868036e-05\n",
      "Iteration: 11280 Training Accuracy: 1.0 Loss: 6.100758764659986e-05\n",
      "Iteration: 11290 Training Accuracy: 1.0 Loss: 9.915202099364251e-05\n",
      "Iteration: 11300 Training Accuracy: 1.0 Loss: 1.1868495676026214e-05\n",
      "Iteration: 11310 Training Accuracy: 1.0 Loss: 0.00017322304483968765\n",
      "Iteration: 11320 Training Accuracy: 1.0 Loss: 9.345699800178409e-05\n",
      "Iteration: 11330 Training Accuracy: 1.0 Loss: 3.1661045795772225e-05\n",
      "Iteration: 11340 Training Accuracy: 1.0 Loss: 3.9993257814785466e-05\n",
      "Iteration: 11350 Training Accuracy: 1.0 Loss: 9.072108514374122e-05\n",
      "Iteration: 11360 Training Accuracy: 1.0 Loss: 7.105646363925189e-05\n",
      "Iteration: 11370 Training Accuracy: 1.0 Loss: 7.11726097506471e-05\n",
      "Iteration: 11380 Training Accuracy: 1.0 Loss: 9.132464765571058e-05\n",
      "Iteration: 11390 Training Accuracy: 0.984375 Loss: 0.0003348724276293069\n",
      "Iteration: 11400 Training Accuracy: 1.0 Loss: 0.00011221947352169082\n",
      "Iteration: 11410 Training Accuracy: 1.0 Loss: 3.3152420655824244e-05\n",
      "Iteration: 11420 Training Accuracy: 1.0 Loss: 2.117991061822977e-05\n",
      "Iteration: 11430 Training Accuracy: 1.0 Loss: 5.195596895646304e-05\n",
      "Iteration: 11440 Training Accuracy: 1.0 Loss: 0.00014699930034112185\n",
      "Iteration: 11450 Training Accuracy: 1.0 Loss: 8.422570681432262e-05\n",
      "Iteration: 11460 Training Accuracy: 1.0 Loss: 2.887258233386092e-06\n",
      "Iteration: 11470 Training Accuracy: 1.0 Loss: 0.00010703637963160872\n",
      "Iteration: 11480 Training Accuracy: 1.0 Loss: 9.056513954419643e-05\n",
      "Iteration: 11490 Training Accuracy: 1.0 Loss: 4.326036651036702e-05\n",
      "Iteration: 11500 Training Accuracy: 1.0 Loss: 0.00031880030292086303\n",
      "Iteration: 11510 Training Accuracy: 1.0 Loss: 2.0364746887935326e-05\n",
      "Iteration: 11520 Training Accuracy: 1.0 Loss: 3.9821457903599367e-05\n",
      "Iteration: 11530 Training Accuracy: 1.0 Loss: 2.7781188691733405e-05\n",
      "Iteration: 11540 Training Accuracy: 1.0 Loss: 6.47437191219069e-05\n",
      "Iteration: 11550 Training Accuracy: 1.0 Loss: 9.00551094673574e-05\n",
      "Iteration: 11560 Training Accuracy: 1.0 Loss: 4.436770541360602e-05\n",
      "Iteration: 11570 Training Accuracy: 1.0 Loss: 0.00014011675375513732\n",
      "Iteration: 11580 Training Accuracy: 1.0 Loss: 5.5114513088483363e-05\n",
      "Iteration: 11590 Training Accuracy: 1.0 Loss: 4.7448389523196965e-05\n",
      "Iteration: 11600 Training Accuracy: 1.0 Loss: 0.00025159461074508727\n",
      "Iteration: 11610 Training Accuracy: 1.0 Loss: 4.775776233145734e-06\n",
      "Iteration: 11620 Training Accuracy: 1.0 Loss: 6.87724314047955e-05\n",
      "Iteration: 11630 Training Accuracy: 1.0 Loss: 5.34299251739867e-05\n",
      "Iteration: 11640 Training Accuracy: 1.0 Loss: 7.640147669008002e-05\n",
      "Iteration: 11650 Training Accuracy: 1.0 Loss: 2.6764597350847907e-05\n",
      "Iteration: 11660 Training Accuracy: 1.0 Loss: 1.0290491445630323e-05\n",
      "Iteration: 11670 Training Accuracy: 1.0 Loss: 4.196946247247979e-05\n",
      "Iteration: 11680 Training Accuracy: 1.0 Loss: 3.1424111512023956e-05\n",
      "Iteration: 11690 Training Accuracy: 1.0 Loss: 2.2513358999276534e-05\n",
      "Iteration: 11700 Training Accuracy: 0.984375 Loss: 0.002261966932564974\n",
      "Iteration: 11710 Training Accuracy: 1.0 Loss: 3.8071673770900816e-05\n",
      "Iteration: 11720 Training Accuracy: 1.0 Loss: 0.00012630419223569334\n",
      "Iteration: 11730 Training Accuracy: 1.0 Loss: 3.86457541026175e-05\n",
      "Iteration: 11740 Training Accuracy: 1.0 Loss: 0.0002074110961984843\n",
      "Iteration: 11750 Training Accuracy: 1.0 Loss: 6.565774674527347e-05\n",
      "Iteration: 11760 Training Accuracy: 1.0 Loss: 0.0004981743404641747\n",
      "Iteration: 11770 Training Accuracy: 1.0 Loss: 0.00011403061216697097\n",
      "Iteration: 11780 Training Accuracy: 1.0 Loss: 7.91302154539153e-05\n",
      "Iteration: 11790 Training Accuracy: 1.0 Loss: 6.291779573075473e-05\n",
      "Iteration: 11800 Training Accuracy: 1.0 Loss: 1.0522916454647202e-05\n",
      "Iteration: 11810 Training Accuracy: 1.0 Loss: 6.603020301554352e-05\n",
      "Iteration: 11820 Training Accuracy: 1.0 Loss: 5.0906179239973426e-05\n",
      "Iteration: 11830 Training Accuracy: 1.0 Loss: 2.4342596589121968e-05\n",
      "Iteration: 11840 Training Accuracy: 1.0 Loss: 3.6579356788024597e-07\n",
      "Iteration: 11850 Training Accuracy: 1.0 Loss: 5.1670380344148725e-05\n",
      "Iteration: 11860 Training Accuracy: 1.0 Loss: 7.716185064055026e-05\n",
      "Iteration: 11870 Training Accuracy: 1.0 Loss: 0.0001980216766241938\n",
      "Iteration: 11880 Training Accuracy: 1.0 Loss: 0.00014440983068197966\n",
      "Iteration: 11890 Training Accuracy: 1.0 Loss: 7.165211718529463e-05\n",
      "Iteration: 11900 Training Accuracy: 1.0 Loss: 2.9924134651082568e-05\n",
      "Iteration: 11910 Training Accuracy: 1.0 Loss: 5.2155566663714126e-05\n",
      "Iteration: 11920 Training Accuracy: 1.0 Loss: 4.245581658324227e-05\n",
      "Iteration: 11930 Training Accuracy: 1.0 Loss: 7.095289220160339e-06\n",
      "Iteration: 11940 Training Accuracy: 1.0 Loss: 3.270864181104116e-05\n",
      "Iteration: 11950 Training Accuracy: 1.0 Loss: 6.974076768528903e-06\n",
      "Iteration: 11960 Training Accuracy: 1.0 Loss: 0.00012878270354121923\n",
      "Iteration: 11970 Training Accuracy: 1.0 Loss: 6.92919857101515e-05\n",
      "Iteration: 11980 Training Accuracy: 1.0 Loss: 0.00025063377688638866\n",
      "Iteration: 11990 Training Accuracy: 1.0 Loss: 9.51714173424989e-05\n",
      "Iteration: 12000 Training Accuracy: 1.0 Loss: 0.00020462888642214239\n",
      "Iteration: 12010 Training Accuracy: 1.0 Loss: 2.5647463189670816e-05\n",
      "Iteration: 12020 Training Accuracy: 1.0 Loss: 0.0002377663622610271\n",
      "Iteration: 12030 Training Accuracy: 1.0 Loss: 7.821238250471652e-05\n",
      "Iteration: 12040 Training Accuracy: 1.0 Loss: 7.53399363020435e-05\n",
      "Iteration: 12050 Training Accuracy: 1.0 Loss: 3.977656160714105e-05\n",
      "Iteration: 12060 Training Accuracy: 1.0 Loss: 6.91490204189904e-05\n",
      "Iteration: 12070 Training Accuracy: 1.0 Loss: 6.056769507267745e-06\n",
      "Iteration: 12080 Training Accuracy: 1.0 Loss: 5.307895116857253e-05\n",
      "Iteration: 12090 Training Accuracy: 1.0 Loss: 0.000139541836688295\n",
      "Iteration: 12100 Training Accuracy: 1.0 Loss: 0.00016162922838702798\n",
      "Iteration: 12110 Training Accuracy: 1.0 Loss: 6.200051575433463e-05\n",
      "Iteration: 12120 Training Accuracy: 1.0 Loss: 8.279505709651858e-05\n",
      "Iteration: 12130 Training Accuracy: 1.0 Loss: 5.389896978158504e-05\n",
      "Iteration: 12140 Training Accuracy: 1.0 Loss: 9.477730054641142e-05\n",
      "Iteration: 12150 Training Accuracy: 1.0 Loss: 0.0002555185928940773\n",
      "Iteration: 12160 Training Accuracy: 1.0 Loss: 4.4797656300943345e-05\n",
      "Iteration: 12170 Training Accuracy: 1.0 Loss: 4.737583367386833e-05\n",
      "Iteration: 12180 Training Accuracy: 1.0 Loss: 4.423944483278319e-05\n",
      "Iteration: 12190 Training Accuracy: 1.0 Loss: 2.447764745738823e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9285\n",
      "epoch: 13\n",
      "Iteration: 12200 Training Accuracy: 1.0 Loss: 6.356775702442974e-05\n",
      "Iteration: 12210 Training Accuracy: 1.0 Loss: 3.1593132007401437e-05\n",
      "Iteration: 12220 Training Accuracy: 1.0 Loss: 9.475023944105487e-06\n",
      "Iteration: 12230 Training Accuracy: 1.0 Loss: 0.00019832495308946818\n",
      "Iteration: 12240 Training Accuracy: 1.0 Loss: 0.0001269874192075804\n",
      "Iteration: 12250 Training Accuracy: 1.0 Loss: 3.454077886999585e-05\n",
      "Iteration: 12260 Training Accuracy: 1.0 Loss: 0.00015930458903312683\n",
      "Iteration: 12270 Training Accuracy: 1.0 Loss: 0.0001640210102777928\n",
      "Iteration: 12280 Training Accuracy: 1.0 Loss: 0.000134484056616202\n",
      "Iteration: 12290 Training Accuracy: 1.0 Loss: 0.0002735481539275497\n",
      "Iteration: 12300 Training Accuracy: 1.0 Loss: 0.00022480511688627303\n",
      "Iteration: 12310 Training Accuracy: 1.0 Loss: 2.295289777975995e-05\n",
      "Iteration: 12320 Training Accuracy: 0.984375 Loss: 0.0005562285659834743\n",
      "Iteration: 12330 Training Accuracy: 1.0 Loss: 1.7970301996683702e-05\n",
      "Iteration: 12340 Training Accuracy: 1.0 Loss: 6.404468149412423e-05\n",
      "Iteration: 12350 Training Accuracy: 1.0 Loss: 0.00020457859500311315\n",
      "Iteration: 12360 Training Accuracy: 1.0 Loss: 5.257209704723209e-05\n",
      "Iteration: 12370 Training Accuracy: 1.0 Loss: 0.000186796227353625\n",
      "Iteration: 12380 Training Accuracy: 1.0 Loss: 6.262710667215288e-05\n",
      "Iteration: 12390 Training Accuracy: 1.0 Loss: 6.00749408476986e-05\n",
      "Iteration: 12400 Training Accuracy: 1.0 Loss: 0.0001659445115365088\n",
      "Iteration: 12410 Training Accuracy: 1.0 Loss: 0.00015147133672144264\n",
      "Iteration: 12420 Training Accuracy: 0.984375 Loss: 0.00021460934658534825\n",
      "Iteration: 12430 Training Accuracy: 1.0 Loss: 8.365320536540821e-05\n",
      "Iteration: 12440 Training Accuracy: 1.0 Loss: 7.245234155561775e-05\n",
      "Iteration: 12450 Training Accuracy: 1.0 Loss: 2.4272743758047e-05\n",
      "Iteration: 12460 Training Accuracy: 1.0 Loss: 9.907021740218624e-05\n",
      "Iteration: 12470 Training Accuracy: 1.0 Loss: 6.102114275563508e-05\n",
      "Iteration: 12480 Training Accuracy: 1.0 Loss: 9.176474122796208e-06\n",
      "Iteration: 12490 Training Accuracy: 1.0 Loss: 5.8376986999064684e-05\n",
      "Iteration: 12500 Training Accuracy: 1.0 Loss: 5.5062660976545885e-05\n",
      "Iteration: 12510 Training Accuracy: 1.0 Loss: 6.195198511704803e-05\n",
      "Iteration: 12520 Training Accuracy: 1.0 Loss: 5.901060831092764e-06\n",
      "Iteration: 12530 Training Accuracy: 1.0 Loss: 0.0002507667522877455\n",
      "Iteration: 12540 Training Accuracy: 1.0 Loss: 0.00010510991705814376\n",
      "Iteration: 12550 Training Accuracy: 1.0 Loss: 4.954321411787532e-05\n",
      "Iteration: 12560 Training Accuracy: 1.0 Loss: 2.239121386082843e-05\n",
      "Iteration: 12570 Training Accuracy: 1.0 Loss: 0.0001488070993218571\n",
      "Iteration: 12580 Training Accuracy: 1.0 Loss: 0.0001920794602483511\n",
      "Iteration: 12590 Training Accuracy: 1.0 Loss: 2.5785411708056927e-05\n",
      "Iteration: 12600 Training Accuracy: 1.0 Loss: 3.8930218579480425e-05\n",
      "Iteration: 12610 Training Accuracy: 1.0 Loss: 3.532103437464684e-05\n",
      "Iteration: 12620 Training Accuracy: 1.0 Loss: 4.061138315591961e-05\n",
      "Iteration: 12630 Training Accuracy: 1.0 Loss: 5.2632134611485526e-05\n",
      "Iteration: 12640 Training Accuracy: 1.0 Loss: 3.1836007110541686e-05\n",
      "Iteration: 12650 Training Accuracy: 1.0 Loss: 7.677072972001042e-06\n",
      "Iteration: 12660 Training Accuracy: 1.0 Loss: 1.813197013689205e-05\n",
      "Iteration: 12670 Training Accuracy: 1.0 Loss: 7.201341213658452e-05\n",
      "Iteration: 12680 Training Accuracy: 1.0 Loss: 2.5757108232937753e-05\n",
      "Iteration: 12690 Training Accuracy: 1.0 Loss: 4.3799249397125095e-05\n",
      "Iteration: 12700 Training Accuracy: 1.0 Loss: 0.0001450506824767217\n",
      "Iteration: 12710 Training Accuracy: 1.0 Loss: 5.318549301591702e-05\n",
      "Iteration: 12720 Training Accuracy: 1.0 Loss: 7.759983418509364e-05\n",
      "Iteration: 12730 Training Accuracy: 1.0 Loss: 0.0001386476360494271\n",
      "Iteration: 12740 Training Accuracy: 1.0 Loss: 0.00013272832438815385\n",
      "Iteration: 12750 Training Accuracy: 1.0 Loss: 2.6120992515643593e-06\n",
      "Iteration: 12760 Training Accuracy: 1.0 Loss: 5.8152595556748565e-06\n",
      "Iteration: 12770 Training Accuracy: 1.0 Loss: 5.420050729298964e-05\n",
      "Iteration: 12780 Training Accuracy: 1.0 Loss: 2.5283567083533853e-05\n",
      "Iteration: 12790 Training Accuracy: 1.0 Loss: 7.909909618319944e-05\n",
      "Iteration: 12800 Training Accuracy: 1.0 Loss: 4.0187849663197994e-05\n",
      "Iteration: 12810 Training Accuracy: 1.0 Loss: 2.3272532416740432e-05\n",
      "Iteration: 12820 Training Accuracy: 1.0 Loss: 7.786236892570741e-06\n",
      "Iteration: 12830 Training Accuracy: 1.0 Loss: 7.204400026239455e-05\n",
      "Iteration: 12840 Training Accuracy: 1.0 Loss: 9.66679654084146e-05\n",
      "Iteration: 12850 Training Accuracy: 1.0 Loss: 5.911253538215533e-05\n",
      "Iteration: 12860 Training Accuracy: 1.0 Loss: 2.319047780474648e-05\n",
      "Iteration: 12870 Training Accuracy: 1.0 Loss: 0.00012342820991761982\n",
      "Iteration: 12880 Training Accuracy: 1.0 Loss: 0.0001513634924776852\n",
      "Iteration: 12890 Training Accuracy: 1.0 Loss: 6.493697583209723e-05\n",
      "Iteration: 12900 Training Accuracy: 1.0 Loss: 5.519910337170586e-05\n",
      "Iteration: 12910 Training Accuracy: 1.0 Loss: 0.00013740788563154638\n",
      "Iteration: 12920 Training Accuracy: 1.0 Loss: 7.83350333222188e-05\n",
      "Iteration: 12930 Training Accuracy: 1.0 Loss: 0.00015807845920789987\n",
      "Iteration: 12940 Training Accuracy: 0.984375 Loss: 0.0006258389330469072\n",
      "Iteration: 12950 Training Accuracy: 1.0 Loss: 9.796484664548188e-05\n",
      "Iteration: 12960 Training Accuracy: 0.984375 Loss: 0.00021440445561893284\n",
      "Iteration: 12970 Training Accuracy: 1.0 Loss: 5.318308467394672e-05\n",
      "Iteration: 12980 Training Accuracy: 1.0 Loss: 0.00014610265498049557\n",
      "Iteration: 12990 Training Accuracy: 0.984375 Loss: 0.00019674337818287313\n",
      "Iteration: 13000 Training Accuracy: 1.0 Loss: 1.3505510651157238e-05\n",
      "Iteration: 13010 Training Accuracy: 1.0 Loss: 8.349177369382232e-05\n",
      "Iteration: 13020 Training Accuracy: 1.0 Loss: 7.621861004736274e-05\n",
      "Iteration: 13030 Training Accuracy: 1.0 Loss: 7.861963240429759e-05\n",
      "Iteration: 13040 Training Accuracy: 1.0 Loss: 3.383864532224834e-05\n",
      "Iteration: 13050 Training Accuracy: 1.0 Loss: 0.00024123660114128143\n",
      "Iteration: 13060 Training Accuracy: 1.0 Loss: 4.6772354835411534e-05\n",
      "Iteration: 13070 Training Accuracy: 1.0 Loss: 4.807826189789921e-05\n",
      "Iteration: 13080 Training Accuracy: 1.0 Loss: 8.174451068043709e-05\n",
      "Iteration: 13090 Training Accuracy: 1.0 Loss: 3.526817090460099e-05\n",
      "Iteration: 13100 Training Accuracy: 1.0 Loss: 6.834522355347872e-05\n",
      "Iteration: 13110 Training Accuracy: 1.0 Loss: 8.681310282554477e-05\n",
      "Iteration: 13120 Training Accuracy: 1.0 Loss: 4.2570263758534566e-05\n",
      "Iteration: 13130 Training Accuracy: 1.0 Loss: 0.00014345718955155462\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.928\n",
      "epoch: 14\n",
      "Iteration: 13140 Training Accuracy: 1.0 Loss: 5.13482082169503e-05\n",
      "Iteration: 13150 Training Accuracy: 1.0 Loss: 2.0816911273868755e-05\n",
      "Iteration: 13160 Training Accuracy: 0.984375 Loss: 0.0003132699930574745\n",
      "Iteration: 13170 Training Accuracy: 1.0 Loss: 3.26415101881139e-05\n",
      "Iteration: 13180 Training Accuracy: 1.0 Loss: 9.169535769615322e-05\n",
      "Iteration: 13190 Training Accuracy: 1.0 Loss: 1.8175807781517506e-05\n",
      "Iteration: 13200 Training Accuracy: 1.0 Loss: 0.00029419909697026014\n",
      "Iteration: 13210 Training Accuracy: 1.0 Loss: 0.00012179002806078643\n",
      "Iteration: 13220 Training Accuracy: 1.0 Loss: 0.00015913161041680723\n",
      "Iteration: 13230 Training Accuracy: 1.0 Loss: 9.297406359110028e-05\n",
      "Iteration: 13240 Training Accuracy: 1.0 Loss: 5.107613105792552e-05\n",
      "Iteration: 13250 Training Accuracy: 1.0 Loss: 0.00017705850768834352\n",
      "Iteration: 13260 Training Accuracy: 1.0 Loss: 3.86862775485497e-05\n",
      "Iteration: 13270 Training Accuracy: 1.0 Loss: 7.317820563912392e-05\n",
      "Iteration: 13280 Training Accuracy: 1.0 Loss: 0.00010865299555007368\n",
      "Iteration: 13290 Training Accuracy: 1.0 Loss: 6.582208152394742e-05\n",
      "Iteration: 13300 Training Accuracy: 1.0 Loss: 0.00013841157488059253\n",
      "Iteration: 13310 Training Accuracy: 1.0 Loss: 4.1372200939804316e-05\n",
      "Iteration: 13320 Training Accuracy: 1.0 Loss: 0.0001403799105901271\n",
      "Iteration: 13330 Training Accuracy: 1.0 Loss: 3.3796059142332524e-05\n",
      "Iteration: 13340 Training Accuracy: 1.0 Loss: 6.875604867673246e-06\n",
      "Iteration: 13350 Training Accuracy: 1.0 Loss: 4.085206819581799e-05\n",
      "Iteration: 13360 Training Accuracy: 1.0 Loss: 0.00013631161709781736\n",
      "Iteration: 13370 Training Accuracy: 1.0 Loss: 0.0001702545996522531\n",
      "Iteration: 13380 Training Accuracy: 1.0 Loss: 0.00016008522652555257\n",
      "Iteration: 13390 Training Accuracy: 1.0 Loss: 5.79511106479913e-05\n",
      "Iteration: 13400 Training Accuracy: 1.0 Loss: 8.493289351463318e-05\n",
      "Iteration: 13410 Training Accuracy: 1.0 Loss: 0.00013164349365979433\n",
      "Iteration: 13420 Training Accuracy: 1.0 Loss: 4.1171268094331026e-05\n",
      "Iteration: 13430 Training Accuracy: 1.0 Loss: 5.474197678267956e-05\n",
      "Iteration: 13440 Training Accuracy: 1.0 Loss: 9.513308032182977e-05\n",
      "Iteration: 13450 Training Accuracy: 1.0 Loss: 0.00016032617713790387\n",
      "Iteration: 13460 Training Accuracy: 1.0 Loss: 2.5085162633331493e-05\n",
      "Iteration: 13470 Training Accuracy: 1.0 Loss: 7.475086476915749e-06\n",
      "Iteration: 13480 Training Accuracy: 1.0 Loss: 3.0814797355560586e-05\n",
      "Iteration: 13490 Training Accuracy: 1.0 Loss: 3.359416950843297e-05\n",
      "Iteration: 13500 Training Accuracy: 1.0 Loss: 0.0002056506200460717\n",
      "Iteration: 13510 Training Accuracy: 1.0 Loss: 6.327693699859083e-05\n",
      "Iteration: 13520 Training Accuracy: 1.0 Loss: 8.085462468443438e-05\n",
      "Iteration: 13530 Training Accuracy: 1.0 Loss: 0.0001399171305820346\n",
      "Iteration: 13540 Training Accuracy: 1.0 Loss: 0.00014944729628041387\n",
      "Iteration: 13550 Training Accuracy: 1.0 Loss: 1.3013494935876224e-05\n",
      "Iteration: 13560 Training Accuracy: 1.0 Loss: 7.181288674473763e-05\n",
      "Iteration: 13570 Training Accuracy: 1.0 Loss: 4.617715603671968e-05\n",
      "Iteration: 13580 Training Accuracy: 1.0 Loss: 0.00012616584717761725\n",
      "Iteration: 13590 Training Accuracy: 1.0 Loss: 7.895284397818614e-06\n",
      "Iteration: 13600 Training Accuracy: 1.0 Loss: 6.585145456483588e-05\n",
      "Iteration: 13610 Training Accuracy: 1.0 Loss: 0.00021872218349017203\n",
      "Iteration: 13620 Training Accuracy: 1.0 Loss: 6.561368354596198e-05\n",
      "Iteration: 13630 Training Accuracy: 1.0 Loss: 3.598690818762407e-05\n",
      "Iteration: 13640 Training Accuracy: 1.0 Loss: 9.250252333004028e-05\n",
      "Iteration: 13650 Training Accuracy: 1.0 Loss: 5.342969234334305e-05\n",
      "Iteration: 13660 Training Accuracy: 1.0 Loss: 6.289521115832031e-05\n",
      "Iteration: 13670 Training Accuracy: 1.0 Loss: 0.00017228189972229302\n",
      "Iteration: 13680 Training Accuracy: 1.0 Loss: 0.000474791246233508\n",
      "Iteration: 13690 Training Accuracy: 1.0 Loss: 4.9941230827244e-05\n",
      "Iteration: 13700 Training Accuracy: 1.0 Loss: 0.00012377402163110673\n",
      "Iteration: 13710 Training Accuracy: 1.0 Loss: 2.3772508939146064e-05\n",
      "Iteration: 13720 Training Accuracy: 1.0 Loss: 5.479635001393035e-05\n",
      "Iteration: 13730 Training Accuracy: 1.0 Loss: 7.49070240999572e-05\n",
      "Iteration: 13740 Training Accuracy: 1.0 Loss: 6.716968346154317e-05\n",
      "Iteration: 13750 Training Accuracy: 1.0 Loss: 5.425920971902087e-05\n",
      "Iteration: 13760 Training Accuracy: 1.0 Loss: 7.041735807433724e-05\n",
      "Iteration: 13770 Training Accuracy: 1.0 Loss: 0.00021281235967762768\n",
      "Iteration: 13780 Training Accuracy: 1.0 Loss: 1.1309968613204546e-05\n",
      "Iteration: 13790 Training Accuracy: 1.0 Loss: 6.492334796348587e-05\n",
      "Iteration: 13800 Training Accuracy: 1.0 Loss: 0.000164771918207407\n",
      "Iteration: 13810 Training Accuracy: 1.0 Loss: 5.507725290954113e-06\n",
      "Iteration: 13820 Training Accuracy: 1.0 Loss: 4.687087130150758e-05\n",
      "Iteration: 13830 Training Accuracy: 1.0 Loss: 1.2941188288095873e-05\n",
      "Iteration: 13840 Training Accuracy: 1.0 Loss: 9.3609924078919e-05\n",
      "Iteration: 13850 Training Accuracy: 1.0 Loss: 0.0001691117649897933\n",
      "Iteration: 13860 Training Accuracy: 1.0 Loss: 3.257774005760439e-05\n",
      "Iteration: 13870 Training Accuracy: 1.0 Loss: 7.774679397698492e-05\n",
      "Iteration: 13880 Training Accuracy: 1.0 Loss: 5.819060606881976e-05\n",
      "Iteration: 13890 Training Accuracy: 1.0 Loss: 9.756919462233782e-05\n",
      "Iteration: 13900 Training Accuracy: 1.0 Loss: 8.821632945910096e-05\n",
      "Iteration: 13910 Training Accuracy: 1.0 Loss: 9.754558413987979e-05\n",
      "Iteration: 13920 Training Accuracy: 1.0 Loss: 7.485199603252113e-05\n",
      "Iteration: 13930 Training Accuracy: 1.0 Loss: 8.313149737659842e-05\n",
      "Iteration: 13940 Training Accuracy: 1.0 Loss: 5.288714237394743e-05\n",
      "Iteration: 13950 Training Accuracy: 1.0 Loss: 0.00017993079381994903\n",
      "Iteration: 13960 Training Accuracy: 1.0 Loss: 2.2168014766066335e-06\n",
      "Iteration: 13970 Training Accuracy: 1.0 Loss: 2.153372042812407e-05\n",
      "Iteration: 13980 Training Accuracy: 1.0 Loss: 6.865202158223838e-05\n",
      "Iteration: 13990 Training Accuracy: 1.0 Loss: 0.00014830296277068555\n",
      "Iteration: 14000 Training Accuracy: 1.0 Loss: 0.00011040321987820789\n",
      "Iteration: 14010 Training Accuracy: 1.0 Loss: 0.0001220662088599056\n",
      "Iteration: 14020 Training Accuracy: 1.0 Loss: 6.429477070923895e-05\n",
      "Iteration: 14030 Training Accuracy: 1.0 Loss: 2.3295131541090086e-05\n",
      "Iteration: 14040 Training Accuracy: 1.0 Loss: 2.1771338651888072e-05\n",
      "Iteration: 14050 Training Accuracy: 1.0 Loss: 6.0034311900381e-05\n",
      "Iteration: 14060 Training Accuracy: 1.0 Loss: 0.00011679262388497591\n",
      "Iteration: 14070 Training Accuracy: 1.0 Loss: 1.410597724316176e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9285\n",
      "epoch: 15\n",
      "Iteration: 14080 Training Accuracy: 1.0 Loss: 0.00010542498785071075\n",
      "Iteration: 14090 Training Accuracy: 1.0 Loss: 1.621501724002883e-05\n",
      "Iteration: 14100 Training Accuracy: 1.0 Loss: 0.0003046159108635038\n",
      "Iteration: 14110 Training Accuracy: 1.0 Loss: 4.172289845882915e-05\n",
      "Iteration: 14120 Training Accuracy: 1.0 Loss: 7.76513188611716e-05\n",
      "Iteration: 14130 Training Accuracy: 1.0 Loss: 9.509159281151369e-05\n",
      "Iteration: 14140 Training Accuracy: 1.0 Loss: 8.357821207027882e-05\n",
      "Iteration: 14150 Training Accuracy: 1.0 Loss: 8.890950994100422e-05\n",
      "Iteration: 14160 Training Accuracy: 1.0 Loss: 5.563259401242249e-05\n",
      "Iteration: 14170 Training Accuracy: 1.0 Loss: 5.148566560819745e-05\n",
      "Iteration: 14180 Training Accuracy: 1.0 Loss: 3.5756682336796075e-05\n",
      "Iteration: 14190 Training Accuracy: 1.0 Loss: 6.58505960018374e-05\n",
      "Iteration: 14200 Training Accuracy: 1.0 Loss: 0.00015516616986133158\n",
      "Iteration: 14210 Training Accuracy: 1.0 Loss: 0.0002145644393749535\n",
      "Iteration: 14220 Training Accuracy: 1.0 Loss: 0.00011652101966319606\n",
      "Iteration: 14230 Training Accuracy: 1.0 Loss: 1.4710921277583111e-05\n",
      "Iteration: 14240 Training Accuracy: 1.0 Loss: 7.830926915630698e-05\n",
      "Iteration: 14250 Training Accuracy: 1.0 Loss: 0.00019476402667351067\n",
      "Iteration: 14260 Training Accuracy: 1.0 Loss: 2.043155109276995e-05\n",
      "Iteration: 14270 Training Accuracy: 1.0 Loss: 0.00028112856671214104\n",
      "Iteration: 14280 Training Accuracy: 1.0 Loss: 3.6952944356016815e-05\n",
      "Iteration: 14290 Training Accuracy: 1.0 Loss: 4.813601481146179e-05\n",
      "Iteration: 14300 Training Accuracy: 1.0 Loss: 0.0001799545279936865\n",
      "Iteration: 14310 Training Accuracy: 1.0 Loss: 0.00021758121147286147\n",
      "Iteration: 14320 Training Accuracy: 1.0 Loss: 5.1686154620256275e-05\n",
      "Iteration: 14330 Training Accuracy: 1.0 Loss: 5.152095764060505e-05\n",
      "Iteration: 14340 Training Accuracy: 1.0 Loss: 2.050495459116064e-05\n",
      "Iteration: 14350 Training Accuracy: 1.0 Loss: 0.00010827001824509352\n",
      "Iteration: 14360 Training Accuracy: 1.0 Loss: 7.288165215868503e-05\n",
      "Iteration: 14370 Training Accuracy: 1.0 Loss: 0.00012220986536704004\n",
      "Iteration: 14380 Training Accuracy: 1.0 Loss: 0.00043752999044954777\n",
      "Iteration: 14390 Training Accuracy: 1.0 Loss: 5.702732232748531e-05\n",
      "Iteration: 14400 Training Accuracy: 1.0 Loss: 2.3145979866967537e-05\n",
      "Iteration: 14410 Training Accuracy: 1.0 Loss: 6.305785518634366e-06\n",
      "Iteration: 14420 Training Accuracy: 1.0 Loss: 1.739121216814965e-05\n",
      "Iteration: 14430 Training Accuracy: 1.0 Loss: 1.933050953084603e-05\n",
      "Iteration: 14440 Training Accuracy: 1.0 Loss: 4.39041577919852e-05\n",
      "Iteration: 14450 Training Accuracy: 1.0 Loss: 0.00023455005430150777\n",
      "Iteration: 14460 Training Accuracy: 1.0 Loss: 0.0001946833945112303\n",
      "Iteration: 14470 Training Accuracy: 1.0 Loss: 4.6127552195684984e-05\n",
      "Iteration: 14480 Training Accuracy: 1.0 Loss: 1.882799006125424e-05\n",
      "Iteration: 14490 Training Accuracy: 1.0 Loss: 8.892008918337524e-05\n",
      "Iteration: 14500 Training Accuracy: 1.0 Loss: 9.02975007193163e-05\n",
      "Iteration: 14510 Training Accuracy: 1.0 Loss: 2.5766239559743553e-05\n",
      "Iteration: 14520 Training Accuracy: 1.0 Loss: 0.00011041713150916621\n",
      "Iteration: 14530 Training Accuracy: 1.0 Loss: 2.5040060791070573e-05\n",
      "Iteration: 14540 Training Accuracy: 1.0 Loss: 4.9571808631299064e-05\n",
      "Iteration: 14550 Training Accuracy: 1.0 Loss: 0.00011212767276447266\n",
      "Iteration: 14560 Training Accuracy: 1.0 Loss: 8.152035297825933e-05\n",
      "Iteration: 14570 Training Accuracy: 1.0 Loss: 0.0002867554721888155\n",
      "Iteration: 14580 Training Accuracy: 1.0 Loss: 2.3223261450766586e-05\n",
      "Iteration: 14590 Training Accuracy: 1.0 Loss: 1.2374595826258883e-05\n",
      "Iteration: 14600 Training Accuracy: 1.0 Loss: 3.6221805203240365e-05\n",
      "Iteration: 14610 Training Accuracy: 1.0 Loss: 2.538265471230261e-05\n",
      "Iteration: 14620 Training Accuracy: 1.0 Loss: 3.8658698031213135e-05\n",
      "Iteration: 14630 Training Accuracy: 1.0 Loss: 4.122004975215532e-05\n",
      "Iteration: 14640 Training Accuracy: 0.984375 Loss: 0.0005058313836343586\n",
      "Iteration: 14650 Training Accuracy: 1.0 Loss: 6.505503552034497e-05\n",
      "Iteration: 14660 Training Accuracy: 1.0 Loss: 6.214722816366702e-05\n",
      "Iteration: 14670 Training Accuracy: 1.0 Loss: 6.583214417332783e-05\n",
      "Iteration: 14680 Training Accuracy: 1.0 Loss: 1.9685281586134806e-05\n",
      "Iteration: 14690 Training Accuracy: 1.0 Loss: 0.00014804919192101806\n",
      "Iteration: 14700 Training Accuracy: 1.0 Loss: 5.7356352044735104e-05\n",
      "Iteration: 14710 Training Accuracy: 1.0 Loss: 2.5239583919756114e-05\n",
      "Iteration: 14720 Training Accuracy: 1.0 Loss: 5.5604999943170696e-05\n",
      "Iteration: 14730 Training Accuracy: 1.0 Loss: 0.00013223891437519342\n",
      "Iteration: 14740 Training Accuracy: 1.0 Loss: 3.416098115849309e-05\n",
      "Iteration: 14750 Training Accuracy: 1.0 Loss: 8.042175613809377e-05\n",
      "Iteration: 14760 Training Accuracy: 1.0 Loss: 5.7854507758747786e-05\n",
      "Iteration: 14770 Training Accuracy: 1.0 Loss: 4.691247158916667e-05\n",
      "Iteration: 14780 Training Accuracy: 1.0 Loss: 3.583514626370743e-05\n",
      "Iteration: 14790 Training Accuracy: 1.0 Loss: 8.523352153133601e-05\n",
      "Iteration: 14800 Training Accuracy: 1.0 Loss: 0.0001211208727909252\n",
      "Iteration: 14810 Training Accuracy: 1.0 Loss: 1.9913193682441488e-05\n",
      "Iteration: 14820 Training Accuracy: 1.0 Loss: 7.31773761799559e-05\n",
      "Iteration: 14830 Training Accuracy: 1.0 Loss: 1.6920232155825943e-05\n",
      "Iteration: 14840 Training Accuracy: 1.0 Loss: 7.324732723645866e-05\n",
      "Iteration: 14850 Training Accuracy: 1.0 Loss: 0.0002212249964941293\n",
      "Iteration: 14860 Training Accuracy: 1.0 Loss: 8.364773748326115e-06\n",
      "Iteration: 14870 Training Accuracy: 1.0 Loss: 0.00012521112512331456\n",
      "Iteration: 14880 Training Accuracy: 1.0 Loss: 5.5995384173002094e-05\n",
      "Iteration: 14890 Training Accuracy: 1.0 Loss: 6.395918899215758e-05\n",
      "Iteration: 14900 Training Accuracy: 1.0 Loss: 6.787799793528393e-05\n",
      "Iteration: 14910 Training Accuracy: 1.0 Loss: 0.00024198123719543219\n",
      "Iteration: 14920 Training Accuracy: 1.0 Loss: 7.520947110606357e-05\n",
      "Iteration: 14930 Training Accuracy: 1.0 Loss: 1.1231577445869334e-05\n",
      "Iteration: 14940 Training Accuracy: 1.0 Loss: 3.061829193029553e-05\n",
      "Iteration: 14950 Training Accuracy: 1.0 Loss: 6.73551403451711e-05\n",
      "Iteration: 14960 Training Accuracy: 1.0 Loss: 4.6652367018396035e-05\n",
      "Iteration: 14970 Training Accuracy: 1.0 Loss: 0.00010299796122126281\n",
      "Iteration: 14980 Training Accuracy: 1.0 Loss: 0.00010585994459688663\n",
      "Iteration: 14990 Training Accuracy: 1.0 Loss: 8.065975271165371e-05\n",
      "Iteration: 15000 Training Accuracy: 1.0 Loss: 2.301712447660975e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9288333333333333\n",
      "epoch: 16\n",
      "Iteration: 15010 Training Accuracy: 1.0 Loss: 7.364193152170628e-05\n",
      "Iteration: 15020 Training Accuracy: 1.0 Loss: 0.00017137419490609318\n",
      "Iteration: 15030 Training Accuracy: 1.0 Loss: 0.00014804210513830185\n",
      "Iteration: 15040 Training Accuracy: 1.0 Loss: 3.4754422813421115e-05\n",
      "Iteration: 15050 Training Accuracy: 1.0 Loss: 4.976680065738037e-05\n",
      "Iteration: 15060 Training Accuracy: 1.0 Loss: 6.510762614198029e-05\n",
      "Iteration: 15070 Training Accuracy: 1.0 Loss: 1.702759436739143e-05\n",
      "Iteration: 15080 Training Accuracy: 1.0 Loss: 5.262036938802339e-05\n",
      "Iteration: 15090 Training Accuracy: 1.0 Loss: 2.9486964194802567e-06\n",
      "Iteration: 15100 Training Accuracy: 1.0 Loss: 6.850948557257652e-05\n",
      "Iteration: 15110 Training Accuracy: 1.0 Loss: 0.0002952677896246314\n",
      "Iteration: 15120 Training Accuracy: 1.0 Loss: 5.1737079047597945e-05\n",
      "Iteration: 15130 Training Accuracy: 1.0 Loss: 1.4200029909261502e-05\n",
      "Iteration: 15140 Training Accuracy: 1.0 Loss: 0.00016373390099033713\n",
      "Iteration: 15150 Training Accuracy: 1.0 Loss: 3.95957067667041e-05\n",
      "Iteration: 15160 Training Accuracy: 1.0 Loss: 0.0001471472205594182\n",
      "Iteration: 15170 Training Accuracy: 1.0 Loss: 0.0001827512460295111\n",
      "Iteration: 15180 Training Accuracy: 1.0 Loss: 3.0095707188593224e-05\n",
      "Iteration: 15190 Training Accuracy: 0.984375 Loss: 0.0005788160488009453\n",
      "Iteration: 15200 Training Accuracy: 1.0 Loss: 1.9819828594336286e-05\n",
      "Iteration: 15210 Training Accuracy: 1.0 Loss: 1.0651939192030113e-05\n",
      "Iteration: 15220 Training Accuracy: 1.0 Loss: 3.1143848900683224e-05\n",
      "Iteration: 15230 Training Accuracy: 1.0 Loss: 0.0001092216552933678\n",
      "Iteration: 15240 Training Accuracy: 1.0 Loss: 8.077862730715424e-05\n",
      "Iteration: 15250 Training Accuracy: 1.0 Loss: 1.4441437997447792e-05\n",
      "Iteration: 15260 Training Accuracy: 1.0 Loss: 7.343891047639772e-05\n",
      "Iteration: 15270 Training Accuracy: 1.0 Loss: 4.912597796646878e-05\n",
      "Iteration: 15280 Training Accuracy: 1.0 Loss: 7.93671642895788e-05\n",
      "Iteration: 15290 Training Accuracy: 1.0 Loss: 0.00014849650324322283\n",
      "Iteration: 15300 Training Accuracy: 1.0 Loss: 4.108369103050791e-05\n",
      "Iteration: 15310 Training Accuracy: 1.0 Loss: 2.260883047711104e-05\n",
      "Iteration: 15320 Training Accuracy: 1.0 Loss: 8.971367788035423e-05\n",
      "Iteration: 15330 Training Accuracy: 1.0 Loss: 0.00014896929496899247\n",
      "Iteration: 15340 Training Accuracy: 1.0 Loss: 3.198274498572573e-05\n",
      "Iteration: 15350 Training Accuracy: 1.0 Loss: 4.5842421968700364e-05\n",
      "Iteration: 15360 Training Accuracy: 1.0 Loss: 2.105290514009539e-05\n",
      "Iteration: 15370 Training Accuracy: 1.0 Loss: 2.8994010790484026e-05\n",
      "Iteration: 15380 Training Accuracy: 1.0 Loss: 2.4026192477322184e-05\n",
      "Iteration: 15390 Training Accuracy: 1.0 Loss: 0.00022074609296396375\n",
      "Iteration: 15400 Training Accuracy: 1.0 Loss: 0.00014027599536348134\n",
      "Iteration: 15410 Training Accuracy: 1.0 Loss: 0.00015466148033738136\n",
      "Iteration: 15420 Training Accuracy: 1.0 Loss: 0.00016934500308707356\n",
      "Iteration: 15430 Training Accuracy: 1.0 Loss: 5.4386800911743194e-05\n",
      "Iteration: 15440 Training Accuracy: 1.0 Loss: 4.6856239350745454e-05\n",
      "Iteration: 15450 Training Accuracy: 1.0 Loss: 7.640638796146959e-05\n",
      "Iteration: 15460 Training Accuracy: 1.0 Loss: 1.4340605048346333e-05\n",
      "Iteration: 15470 Training Accuracy: 1.0 Loss: 3.51497255905997e-05\n",
      "Iteration: 15480 Training Accuracy: 1.0 Loss: 1.095306288334541e-05\n",
      "Iteration: 15490 Training Accuracy: 1.0 Loss: 3.725770875462331e-05\n",
      "Iteration: 15500 Training Accuracy: 1.0 Loss: 7.492459553759545e-05\n",
      "Iteration: 15510 Training Accuracy: 1.0 Loss: 0.0002607713686302304\n",
      "Iteration: 15520 Training Accuracy: 1.0 Loss: 5.569266068050638e-05\n",
      "Iteration: 15530 Training Accuracy: 1.0 Loss: 9.061652235686779e-05\n",
      "Iteration: 15540 Training Accuracy: 1.0 Loss: 9.865181345958263e-05\n",
      "Iteration: 15550 Training Accuracy: 1.0 Loss: 3.6331744922790676e-05\n",
      "Iteration: 15560 Training Accuracy: 1.0 Loss: 4.5061497075948864e-05\n",
      "Iteration: 15570 Training Accuracy: 1.0 Loss: 1.088537919713417e-05\n",
      "Iteration: 15580 Training Accuracy: 1.0 Loss: 8.565470488974825e-05\n",
      "Iteration: 15590 Training Accuracy: 1.0 Loss: 4.139763041166589e-05\n",
      "Iteration: 15600 Training Accuracy: 1.0 Loss: 3.4113523724954575e-05\n",
      "Iteration: 15610 Training Accuracy: 1.0 Loss: 9.365936421090737e-05\n",
      "Iteration: 15620 Training Accuracy: 1.0 Loss: 3.9097965782275423e-05\n",
      "Iteration: 15630 Training Accuracy: 1.0 Loss: 4.1820410842774436e-05\n",
      "Iteration: 15640 Training Accuracy: 1.0 Loss: 9.6270909125451e-05\n",
      "Iteration: 15650 Training Accuracy: 1.0 Loss: 4.930255454382859e-05\n",
      "Iteration: 15660 Training Accuracy: 1.0 Loss: 1.1828216884168796e-05\n",
      "Iteration: 15670 Training Accuracy: 1.0 Loss: 1.3320523066795431e-05\n",
      "Iteration: 15680 Training Accuracy: 1.0 Loss: 3.099143577856012e-05\n",
      "Iteration: 15690 Training Accuracy: 1.0 Loss: 5.242109909886494e-05\n",
      "Iteration: 15700 Training Accuracy: 1.0 Loss: 0.00012725834676530212\n",
      "Iteration: 15710 Training Accuracy: 1.0 Loss: 0.00018379736866336316\n",
      "Iteration: 15720 Training Accuracy: 1.0 Loss: 9.35719144763425e-05\n",
      "Iteration: 15730 Training Accuracy: 1.0 Loss: 3.0238345061661676e-05\n",
      "Iteration: 15740 Training Accuracy: 1.0 Loss: 6.894355465192348e-05\n",
      "Iteration: 15750 Training Accuracy: 1.0 Loss: 1.1426984201534651e-05\n",
      "Iteration: 15760 Training Accuracy: 1.0 Loss: 4.130454908590764e-05\n",
      "Iteration: 15770 Training Accuracy: 1.0 Loss: 4.227993849781342e-05\n",
      "Iteration: 15780 Training Accuracy: 1.0 Loss: 4.2020044929813594e-05\n",
      "Iteration: 15790 Training Accuracy: 1.0 Loss: 4.282789450371638e-05\n",
      "Iteration: 15800 Training Accuracy: 1.0 Loss: 0.00019639215315692127\n",
      "Iteration: 15810 Training Accuracy: 1.0 Loss: 2.066949673462659e-05\n",
      "Iteration: 15820 Training Accuracy: 1.0 Loss: 9.86586201179307e-06\n",
      "Iteration: 15830 Training Accuracy: 1.0 Loss: 6.424210732802749e-05\n",
      "Iteration: 15840 Training Accuracy: 1.0 Loss: 8.10112978797406e-05\n",
      "Iteration: 15850 Training Accuracy: 1.0 Loss: 0.0002431664033792913\n",
      "Iteration: 15860 Training Accuracy: 1.0 Loss: 5.170153599465266e-05\n",
      "Iteration: 15870 Training Accuracy: 1.0 Loss: 2.6461224479135126e-05\n",
      "Iteration: 15880 Training Accuracy: 1.0 Loss: 5.301911733113229e-05\n",
      "Iteration: 15890 Training Accuracy: 1.0 Loss: 7.082421507220715e-05\n",
      "Iteration: 15900 Training Accuracy: 1.0 Loss: 1.6708918337826617e-05\n",
      "Iteration: 15910 Training Accuracy: 1.0 Loss: 5.785790926893242e-05\n",
      "Iteration: 15920 Training Accuracy: 1.0 Loss: 0.00020377391774673015\n",
      "Iteration: 15930 Training Accuracy: 1.0 Loss: 2.4964450858533382e-05\n",
      "Iteration: 15940 Training Accuracy: 1.0 Loss: 5.146878174855374e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.929\n",
      "epoch: 17\n",
      "Iteration: 15950 Training Accuracy: 1.0 Loss: 9.078814764507115e-05\n",
      "Iteration: 15960 Training Accuracy: 1.0 Loss: 7.858032768126577e-05\n",
      "Iteration: 15970 Training Accuracy: 1.0 Loss: 3.2041869417298585e-05\n",
      "Iteration: 15980 Training Accuracy: 1.0 Loss: 8.622291352367029e-05\n",
      "Iteration: 15990 Training Accuracy: 1.0 Loss: 1.0788860890897922e-05\n",
      "Iteration: 16000 Training Accuracy: 1.0 Loss: 0.00015731656458228827\n",
      "Iteration: 16010 Training Accuracy: 1.0 Loss: 6.91855966579169e-05\n",
      "Iteration: 16020 Training Accuracy: 1.0 Loss: 3.5306344216223806e-05\n",
      "Iteration: 16030 Training Accuracy: 1.0 Loss: 3.457131970208138e-05\n",
      "Iteration: 16040 Training Accuracy: 1.0 Loss: 6.507354555651546e-05\n",
      "Iteration: 16050 Training Accuracy: 1.0 Loss: 6.695845513604581e-05\n",
      "Iteration: 16060 Training Accuracy: 1.0 Loss: 7.714061939623207e-05\n",
      "Iteration: 16070 Training Accuracy: 1.0 Loss: 7.453966827597469e-05\n",
      "Iteration: 16080 Training Accuracy: 1.0 Loss: 0.0002544009475968778\n",
      "Iteration: 16090 Training Accuracy: 1.0 Loss: 9.057992429006845e-05\n",
      "Iteration: 16100 Training Accuracy: 1.0 Loss: 3.05022404063493e-05\n",
      "Iteration: 16110 Training Accuracy: 1.0 Loss: 1.596884976606816e-05\n",
      "Iteration: 16120 Training Accuracy: 1.0 Loss: 5.726320159737952e-05\n",
      "Iteration: 16130 Training Accuracy: 1.0 Loss: 0.0001423551293555647\n",
      "Iteration: 16140 Training Accuracy: 1.0 Loss: 7.505649409722537e-05\n",
      "Iteration: 16150 Training Accuracy: 1.0 Loss: 3.23907875099394e-06\n",
      "Iteration: 16160 Training Accuracy: 1.0 Loss: 9.253872121917084e-05\n",
      "Iteration: 16170 Training Accuracy: 1.0 Loss: 9.206453250953928e-05\n",
      "Iteration: 16180 Training Accuracy: 1.0 Loss: 4.354137854534201e-05\n",
      "Iteration: 16190 Training Accuracy: 1.0 Loss: 0.0002704181242734194\n",
      "Iteration: 16200 Training Accuracy: 1.0 Loss: 1.7767324607120827e-05\n",
      "Iteration: 16210 Training Accuracy: 1.0 Loss: 3.215048127458431e-05\n",
      "Iteration: 16220 Training Accuracy: 1.0 Loss: 2.5719618861330673e-05\n",
      "Iteration: 16230 Training Accuracy: 1.0 Loss: 6.145984661998227e-05\n",
      "Iteration: 16240 Training Accuracy: 1.0 Loss: 7.312758680200204e-05\n",
      "Iteration: 16250 Training Accuracy: 1.0 Loss: 4.130913657718338e-05\n",
      "Iteration: 16260 Training Accuracy: 1.0 Loss: 0.00010615999053698033\n",
      "Iteration: 16270 Training Accuracy: 1.0 Loss: 5.2250372391426936e-05\n",
      "Iteration: 16280 Training Accuracy: 1.0 Loss: 4.649977927329019e-05\n",
      "Iteration: 16290 Training Accuracy: 1.0 Loss: 0.00022277366952039301\n",
      "Iteration: 16300 Training Accuracy: 1.0 Loss: 3.7719273677794263e-06\n",
      "Iteration: 16310 Training Accuracy: 1.0 Loss: 6.504780321847647e-05\n",
      "Iteration: 16320 Training Accuracy: 1.0 Loss: 5.421297828434035e-05\n",
      "Iteration: 16330 Training Accuracy: 1.0 Loss: 6.376227975124493e-05\n",
      "Iteration: 16340 Training Accuracy: 1.0 Loss: 2.2371777959051542e-05\n",
      "Iteration: 16350 Training Accuracy: 1.0 Loss: 9.539803613733966e-06\n",
      "Iteration: 16360 Training Accuracy: 1.0 Loss: 3.564484359230846e-05\n",
      "Iteration: 16370 Training Accuracy: 1.0 Loss: 3.5531331377569586e-05\n",
      "Iteration: 16380 Training Accuracy: 1.0 Loss: 1.676962710916996e-05\n",
      "Iteration: 16390 Training Accuracy: 0.984375 Loss: 0.002105091232806444\n",
      "Iteration: 16400 Training Accuracy: 1.0 Loss: 3.263139660703018e-05\n",
      "Iteration: 16410 Training Accuracy: 1.0 Loss: 7.718219421803951e-05\n",
      "Iteration: 16420 Training Accuracy: 1.0 Loss: 3.64771367458161e-05\n",
      "Iteration: 16430 Training Accuracy: 1.0 Loss: 0.00016953902377281338\n",
      "Iteration: 16440 Training Accuracy: 1.0 Loss: 5.731494820793159e-05\n",
      "Iteration: 16450 Training Accuracy: 1.0 Loss: 0.0004347020003478974\n",
      "Iteration: 16460 Training Accuracy: 1.0 Loss: 0.00011800944048445672\n",
      "Iteration: 16470 Training Accuracy: 1.0 Loss: 6.935679994057864e-05\n",
      "Iteration: 16480 Training Accuracy: 1.0 Loss: 5.6926928664324805e-05\n",
      "Iteration: 16490 Training Accuracy: 1.0 Loss: 8.918442290450912e-06\n",
      "Iteration: 16500 Training Accuracy: 1.0 Loss: 5.556577889365144e-05\n",
      "Iteration: 16510 Training Accuracy: 1.0 Loss: 5.2675779443234205e-05\n",
      "Iteration: 16520 Training Accuracy: 1.0 Loss: 2.876251528505236e-05\n",
      "Iteration: 16530 Training Accuracy: 1.0 Loss: 2.3397112158818345e-07\n",
      "Iteration: 16540 Training Accuracy: 1.0 Loss: 5.0309525249758735e-05\n",
      "Iteration: 16550 Training Accuracy: 1.0 Loss: 8.368719863938168e-05\n",
      "Iteration: 16560 Training Accuracy: 1.0 Loss: 0.00013239104009699076\n",
      "Iteration: 16570 Training Accuracy: 1.0 Loss: 0.00014588386693503708\n",
      "Iteration: 16580 Training Accuracy: 1.0 Loss: 4.916125544696115e-05\n",
      "Iteration: 16590 Training Accuracy: 1.0 Loss: 2.4264780222438276e-05\n",
      "Iteration: 16600 Training Accuracy: 1.0 Loss: 6.435311661334708e-05\n",
      "Iteration: 16610 Training Accuracy: 1.0 Loss: 4.7732919483678415e-05\n",
      "Iteration: 16620 Training Accuracy: 1.0 Loss: 7.1091899371822365e-06\n",
      "Iteration: 16630 Training Accuracy: 1.0 Loss: 3.473034666967578e-05\n",
      "Iteration: 16640 Training Accuracy: 1.0 Loss: 7.67389610700775e-06\n",
      "Iteration: 16650 Training Accuracy: 1.0 Loss: 0.00010757348354673013\n",
      "Iteration: 16660 Training Accuracy: 1.0 Loss: 4.274827733752318e-05\n",
      "Iteration: 16670 Training Accuracy: 1.0 Loss: 0.00018229300621896982\n",
      "Iteration: 16680 Training Accuracy: 1.0 Loss: 8.47641276777722e-05\n",
      "Iteration: 16690 Training Accuracy: 1.0 Loss: 9.31949180085212e-05\n",
      "Iteration: 16700 Training Accuracy: 1.0 Loss: 2.7662954380502924e-05\n",
      "Iteration: 16710 Training Accuracy: 1.0 Loss: 0.000182616277015768\n",
      "Iteration: 16720 Training Accuracy: 1.0 Loss: 6.189223495312035e-05\n",
      "Iteration: 16730 Training Accuracy: 1.0 Loss: 6.092156399972737e-05\n",
      "Iteration: 16740 Training Accuracy: 1.0 Loss: 4.228657053317875e-05\n",
      "Iteration: 16750 Training Accuracy: 1.0 Loss: 5.670179962180555e-05\n",
      "Iteration: 16760 Training Accuracy: 1.0 Loss: 6.978827514103614e-06\n",
      "Iteration: 16770 Training Accuracy: 1.0 Loss: 4.1777329897740856e-05\n",
      "Iteration: 16780 Training Accuracy: 1.0 Loss: 7.855764852138236e-05\n",
      "Iteration: 16790 Training Accuracy: 1.0 Loss: 0.00013213444617576897\n",
      "Iteration: 16800 Training Accuracy: 1.0 Loss: 6.309357559075579e-05\n",
      "Iteration: 16810 Training Accuracy: 1.0 Loss: 7.794478005962446e-05\n",
      "Iteration: 16820 Training Accuracy: 1.0 Loss: 5.599685391644016e-05\n",
      "Iteration: 16830 Training Accuracy: 1.0 Loss: 7.960169750731438e-05\n",
      "Iteration: 16840 Training Accuracy: 1.0 Loss: 0.00020031833264511079\n",
      "Iteration: 16850 Training Accuracy: 1.0 Loss: 3.822261351160705e-05\n",
      "Iteration: 16860 Training Accuracy: 1.0 Loss: 3.5792822018265724e-05\n",
      "Iteration: 16870 Training Accuracy: 1.0 Loss: 4.306781556806527e-05\n",
      "Iteration: 16880 Training Accuracy: 1.0 Loss: 2.059435246337671e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9291666666666667\n",
      "epoch: 18\n",
      "Iteration: 16890 Training Accuracy: 1.0 Loss: 4.420872573973611e-05\n",
      "Iteration: 16900 Training Accuracy: 1.0 Loss: 2.406430576229468e-05\n",
      "Iteration: 16910 Training Accuracy: 1.0 Loss: 8.847764547681436e-06\n",
      "Iteration: 16920 Training Accuracy: 1.0 Loss: 0.000170298881130293\n",
      "Iteration: 16930 Training Accuracy: 1.0 Loss: 8.535746746929362e-05\n",
      "Iteration: 16940 Training Accuracy: 1.0 Loss: 2.69152587861754e-05\n",
      "Iteration: 16950 Training Accuracy: 1.0 Loss: 0.0001339355658274144\n",
      "Iteration: 16960 Training Accuracy: 1.0 Loss: 0.00013925222447142005\n",
      "Iteration: 16970 Training Accuracy: 1.0 Loss: 0.00010379280138295144\n",
      "Iteration: 16980 Training Accuracy: 1.0 Loss: 0.00023102886916603893\n",
      "Iteration: 16990 Training Accuracy: 1.0 Loss: 0.00019437639275565743\n",
      "Iteration: 17000 Training Accuracy: 1.0 Loss: 1.982509820663836e-05\n",
      "Iteration: 17010 Training Accuracy: 0.984375 Loss: 0.0005484140710905194\n",
      "Iteration: 17020 Training Accuracy: 1.0 Loss: 1.6955587852862664e-05\n",
      "Iteration: 17030 Training Accuracy: 1.0 Loss: 7.352483953582123e-05\n",
      "Iteration: 17040 Training Accuracy: 1.0 Loss: 0.00018202833598479629\n",
      "Iteration: 17050 Training Accuracy: 1.0 Loss: 4.619992250809446e-05\n",
      "Iteration: 17060 Training Accuracy: 1.0 Loss: 0.00018741619715001434\n",
      "Iteration: 17070 Training Accuracy: 1.0 Loss: 6.243027746677399e-05\n",
      "Iteration: 17080 Training Accuracy: 1.0 Loss: 7.209113391581923e-05\n",
      "Iteration: 17090 Training Accuracy: 1.0 Loss: 0.00015792078920640051\n",
      "Iteration: 17100 Training Accuracy: 1.0 Loss: 0.00013131821469869465\n",
      "Iteration: 17110 Training Accuracy: 1.0 Loss: 0.00015330497990362346\n",
      "Iteration: 17120 Training Accuracy: 1.0 Loss: 0.00010752503294497728\n",
      "Iteration: 17130 Training Accuracy: 1.0 Loss: 5.545605381485075e-05\n",
      "Iteration: 17140 Training Accuracy: 1.0 Loss: 2.2861833713250235e-05\n",
      "Iteration: 17150 Training Accuracy: 1.0 Loss: 9.552641131449491e-05\n",
      "Iteration: 17160 Training Accuracy: 1.0 Loss: 6.06407702434808e-05\n",
      "Iteration: 17170 Training Accuracy: 1.0 Loss: 1.2679234714596532e-05\n",
      "Iteration: 17180 Training Accuracy: 1.0 Loss: 6.565515650436282e-05\n",
      "Iteration: 17190 Training Accuracy: 1.0 Loss: 5.273275382933207e-05\n",
      "Iteration: 17200 Training Accuracy: 1.0 Loss: 6.197113543748856e-05\n",
      "Iteration: 17210 Training Accuracy: 1.0 Loss: 5.489909199241083e-06\n",
      "Iteration: 17220 Training Accuracy: 1.0 Loss: 0.00023494561901316047\n",
      "Iteration: 17230 Training Accuracy: 1.0 Loss: 8.57208069646731e-05\n",
      "Iteration: 17240 Training Accuracy: 1.0 Loss: 3.935668064514175e-05\n",
      "Iteration: 17250 Training Accuracy: 1.0 Loss: 3.014321919181384e-05\n",
      "Iteration: 17260 Training Accuracy: 1.0 Loss: 0.00013154366752132773\n",
      "Iteration: 17270 Training Accuracy: 1.0 Loss: 0.00016287829203065485\n",
      "Iteration: 17280 Training Accuracy: 1.0 Loss: 2.584138019301463e-05\n",
      "Iteration: 17290 Training Accuracy: 1.0 Loss: 3.806559107033536e-05\n",
      "Iteration: 17300 Training Accuracy: 1.0 Loss: 3.175235178787261e-05\n",
      "Iteration: 17310 Training Accuracy: 1.0 Loss: 3.407457916182466e-05\n",
      "Iteration: 17320 Training Accuracy: 1.0 Loss: 4.224643998895772e-05\n",
      "Iteration: 17330 Training Accuracy: 1.0 Loss: 2.610623778309673e-05\n",
      "Iteration: 17340 Training Accuracy: 1.0 Loss: 6.8884323809470516e-06\n",
      "Iteration: 17350 Training Accuracy: 1.0 Loss: 1.7061709513654932e-05\n",
      "Iteration: 17360 Training Accuracy: 1.0 Loss: 6.32868759566918e-05\n",
      "Iteration: 17370 Training Accuracy: 1.0 Loss: 2.40302542806603e-05\n",
      "Iteration: 17380 Training Accuracy: 1.0 Loss: 4.668347173719667e-05\n",
      "Iteration: 17390 Training Accuracy: 1.0 Loss: 0.00011464302224339917\n",
      "Iteration: 17400 Training Accuracy: 1.0 Loss: 4.263594018993899e-05\n",
      "Iteration: 17410 Training Accuracy: 1.0 Loss: 6.47532579023391e-05\n",
      "Iteration: 17420 Training Accuracy: 1.0 Loss: 9.867356129689142e-05\n",
      "Iteration: 17430 Training Accuracy: 1.0 Loss: 0.00012740257079713047\n",
      "Iteration: 17440 Training Accuracy: 1.0 Loss: 1.552678781990835e-06\n",
      "Iteration: 17450 Training Accuracy: 1.0 Loss: 7.47358899388928e-06\n",
      "Iteration: 17460 Training Accuracy: 1.0 Loss: 4.197540692985058e-05\n",
      "Iteration: 17470 Training Accuracy: 1.0 Loss: 1.9261598936282098e-05\n",
      "Iteration: 17480 Training Accuracy: 1.0 Loss: 7.46024088584818e-05\n",
      "Iteration: 17490 Training Accuracy: 1.0 Loss: 3.868249405059032e-05\n",
      "Iteration: 17500 Training Accuracy: 1.0 Loss: 2.871051401598379e-05\n",
      "Iteration: 17510 Training Accuracy: 1.0 Loss: 7.129058758437168e-06\n",
      "Iteration: 17520 Training Accuracy: 1.0 Loss: 6.294001650530845e-05\n",
      "Iteration: 17530 Training Accuracy: 1.0 Loss: 7.821803592378274e-05\n",
      "Iteration: 17540 Training Accuracy: 1.0 Loss: 7.054479647194967e-05\n",
      "Iteration: 17550 Training Accuracy: 1.0 Loss: 1.918664202094078e-05\n",
      "Iteration: 17560 Training Accuracy: 1.0 Loss: 0.0001339884620392695\n",
      "Iteration: 17570 Training Accuracy: 1.0 Loss: 0.00011913578055100515\n",
      "Iteration: 17580 Training Accuracy: 1.0 Loss: 5.7092009228654206e-05\n",
      "Iteration: 17590 Training Accuracy: 1.0 Loss: 4.667684333981015e-05\n",
      "Iteration: 17600 Training Accuracy: 1.0 Loss: 9.440669964533299e-05\n",
      "Iteration: 17610 Training Accuracy: 1.0 Loss: 7.749389624223113e-05\n",
      "Iteration: 17620 Training Accuracy: 1.0 Loss: 0.00015351528418250382\n",
      "Iteration: 17630 Training Accuracy: 0.984375 Loss: 0.0006149994442239404\n",
      "Iteration: 17640 Training Accuracy: 1.0 Loss: 8.854953193804249e-05\n",
      "Iteration: 17650 Training Accuracy: 1.0 Loss: 0.00019337113189976662\n",
      "Iteration: 17660 Training Accuracy: 1.0 Loss: 4.2185336496913806e-05\n",
      "Iteration: 17670 Training Accuracy: 1.0 Loss: 0.0001154104174929671\n",
      "Iteration: 17680 Training Accuracy: 1.0 Loss: 0.00015922666352707893\n",
      "Iteration: 17690 Training Accuracy: 1.0 Loss: 1.1354906746419147e-05\n",
      "Iteration: 17700 Training Accuracy: 1.0 Loss: 6.135785952210426e-05\n",
      "Iteration: 17710 Training Accuracy: 1.0 Loss: 5.854117625858635e-05\n",
      "Iteration: 17720 Training Accuracy: 1.0 Loss: 6.740083335898817e-05\n",
      "Iteration: 17730 Training Accuracy: 1.0 Loss: 2.6112693376489915e-05\n",
      "Iteration: 17740 Training Accuracy: 1.0 Loss: 0.00020135623344685882\n",
      "Iteration: 17750 Training Accuracy: 1.0 Loss: 5.2263356337789446e-05\n",
      "Iteration: 17760 Training Accuracy: 1.0 Loss: 4.66601995867677e-05\n",
      "Iteration: 17770 Training Accuracy: 1.0 Loss: 7.611261389683932e-05\n",
      "Iteration: 17780 Training Accuracy: 1.0 Loss: 3.456763079157099e-05\n",
      "Iteration: 17790 Training Accuracy: 1.0 Loss: 6.51634472887963e-05\n",
      "Iteration: 17800 Training Accuracy: 1.0 Loss: 7.353546243393794e-05\n",
      "Iteration: 17810 Training Accuracy: 1.0 Loss: 3.613041917560622e-05\n",
      "Iteration: 17820 Training Accuracy: 1.0 Loss: 0.00010649660544004291\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9291666666666667\n",
      "epoch: 19\n",
      "Iteration: 17830 Training Accuracy: 1.0 Loss: 4.400682155392133e-05\n",
      "Iteration: 17840 Training Accuracy: 1.0 Loss: 2.4491700969520025e-05\n",
      "Iteration: 17850 Training Accuracy: 1.0 Loss: 0.00027012175996787846\n",
      "Iteration: 17860 Training Accuracy: 1.0 Loss: 3.169055707985535e-05\n",
      "Iteration: 17870 Training Accuracy: 1.0 Loss: 7.75747248553671e-05\n",
      "Iteration: 17880 Training Accuracy: 1.0 Loss: 1.4099890904617496e-05\n",
      "Iteration: 17890 Training Accuracy: 1.0 Loss: 0.0002927061286754906\n",
      "Iteration: 17900 Training Accuracy: 1.0 Loss: 0.00010355536505812779\n",
      "Iteration: 17910 Training Accuracy: 1.0 Loss: 0.00015013034862931818\n",
      "Iteration: 17920 Training Accuracy: 1.0 Loss: 6.317947554634884e-05\n",
      "Iteration: 17930 Training Accuracy: 1.0 Loss: 5.104050796944648e-05\n",
      "Iteration: 17940 Training Accuracy: 1.0 Loss: 0.00014246374485082924\n",
      "Iteration: 17950 Training Accuracy: 1.0 Loss: 5.464571586344391e-05\n",
      "Iteration: 17960 Training Accuracy: 1.0 Loss: 6.08381669735536e-05\n",
      "Iteration: 17970 Training Accuracy: 1.0 Loss: 9.513483382761478e-05\n",
      "Iteration: 17980 Training Accuracy: 1.0 Loss: 4.034150333609432e-05\n",
      "Iteration: 17990 Training Accuracy: 1.0 Loss: 9.789808973437175e-05\n",
      "Iteration: 18000 Training Accuracy: 1.0 Loss: 2.7501551812747493e-05\n",
      "Iteration: 18010 Training Accuracy: 1.0 Loss: 6.943116750335321e-05\n",
      "Iteration: 18020 Training Accuracy: 1.0 Loss: 3.4619737562024966e-05\n",
      "Iteration: 18030 Training Accuracy: 1.0 Loss: 1.0027564712800086e-05\n",
      "Iteration: 18040 Training Accuracy: 1.0 Loss: 3.551713234628551e-05\n",
      "Iteration: 18050 Training Accuracy: 1.0 Loss: 0.0001185405271826312\n",
      "Iteration: 18060 Training Accuracy: 1.0 Loss: 0.00011629257642198354\n",
      "Iteration: 18070 Training Accuracy: 1.0 Loss: 0.00013944761303719133\n",
      "Iteration: 18080 Training Accuracy: 1.0 Loss: 6.0515540099004284e-05\n",
      "Iteration: 18090 Training Accuracy: 1.0 Loss: 9.841970313573256e-05\n",
      "Iteration: 18100 Training Accuracy: 1.0 Loss: 0.00012526806676760316\n",
      "Iteration: 18110 Training Accuracy: 1.0 Loss: 3.618344635469839e-05\n",
      "Iteration: 18120 Training Accuracy: 1.0 Loss: 5.3706422477262095e-05\n",
      "Iteration: 18130 Training Accuracy: 1.0 Loss: 8.359269122593105e-05\n",
      "Iteration: 18140 Training Accuracy: 1.0 Loss: 0.00017300171020906419\n",
      "Iteration: 18150 Training Accuracy: 1.0 Loss: 2.3769354811520316e-05\n",
      "Iteration: 18160 Training Accuracy: 1.0 Loss: 6.399016456271056e-06\n",
      "Iteration: 18170 Training Accuracy: 1.0 Loss: 2.360272264922969e-05\n",
      "Iteration: 18180 Training Accuracy: 1.0 Loss: 2.6541330953477882e-05\n",
      "Iteration: 18190 Training Accuracy: 1.0 Loss: 0.00019508975674398243\n",
      "Iteration: 18200 Training Accuracy: 1.0 Loss: 6.464949547080323e-05\n",
      "Iteration: 18210 Training Accuracy: 1.0 Loss: 8.876522042555735e-05\n",
      "Iteration: 18220 Training Accuracy: 1.0 Loss: 0.00011383696983102709\n",
      "Iteration: 18230 Training Accuracy: 1.0 Loss: 0.0001485911780036986\n",
      "Iteration: 18240 Training Accuracy: 1.0 Loss: 1.0824453966051806e-05\n",
      "Iteration: 18250 Training Accuracy: 1.0 Loss: 7.004525105003268e-05\n",
      "Iteration: 18260 Training Accuracy: 1.0 Loss: 4.181088297627866e-05\n",
      "Iteration: 18270 Training Accuracy: 1.0 Loss: 0.0001257625117432326\n",
      "Iteration: 18280 Training Accuracy: 1.0 Loss: 7.716721484030131e-06\n",
      "Iteration: 18290 Training Accuracy: 1.0 Loss: 6.473661778727546e-05\n",
      "Iteration: 18300 Training Accuracy: 1.0 Loss: 0.00017851285520009696\n",
      "Iteration: 18310 Training Accuracy: 1.0 Loss: 6.884985486976802e-05\n",
      "Iteration: 18320 Training Accuracy: 1.0 Loss: 3.4128908737329766e-05\n",
      "Iteration: 18330 Training Accuracy: 1.0 Loss: 7.474224548786879e-05\n",
      "Iteration: 18340 Training Accuracy: 1.0 Loss: 5.977418186375871e-05\n",
      "Iteration: 18350 Training Accuracy: 1.0 Loss: 6.241111987037584e-05\n",
      "Iteration: 18360 Training Accuracy: 1.0 Loss: 0.00017048227891791612\n",
      "Iteration: 18370 Training Accuracy: 1.0 Loss: 0.0003846758627332747\n",
      "Iteration: 18380 Training Accuracy: 1.0 Loss: 5.3720006690127775e-05\n",
      "Iteration: 18390 Training Accuracy: 1.0 Loss: 0.00011552203795872629\n",
      "Iteration: 18400 Training Accuracy: 1.0 Loss: 2.286215931235347e-05\n",
      "Iteration: 18410 Training Accuracy: 1.0 Loss: 5.080586561234668e-05\n",
      "Iteration: 18420 Training Accuracy: 1.0 Loss: 6.805126031395048e-05\n",
      "Iteration: 18430 Training Accuracy: 1.0 Loss: 6.25644315732643e-05\n",
      "Iteration: 18440 Training Accuracy: 1.0 Loss: 4.007362440461293e-05\n",
      "Iteration: 18450 Training Accuracy: 1.0 Loss: 6.120198668213561e-05\n",
      "Iteration: 18460 Training Accuracy: 1.0 Loss: 0.00019713114306796342\n",
      "Iteration: 18470 Training Accuracy: 1.0 Loss: 9.030210094351787e-06\n",
      "Iteration: 18480 Training Accuracy: 1.0 Loss: 6.253164610825479e-05\n",
      "Iteration: 18490 Training Accuracy: 1.0 Loss: 0.00016197636432480067\n",
      "Iteration: 18500 Training Accuracy: 1.0 Loss: 5.253617018752266e-06\n",
      "Iteration: 18510 Training Accuracy: 1.0 Loss: 3.463325629127212e-05\n",
      "Iteration: 18520 Training Accuracy: 1.0 Loss: 1.250438072020188e-05\n",
      "Iteration: 18530 Training Accuracy: 1.0 Loss: 9.804449655348435e-05\n",
      "Iteration: 18540 Training Accuracy: 1.0 Loss: 0.00016318261623382568\n",
      "Iteration: 18550 Training Accuracy: 1.0 Loss: 3.4436245186952874e-05\n",
      "Iteration: 18560 Training Accuracy: 1.0 Loss: 9.305674757342786e-05\n",
      "Iteration: 18570 Training Accuracy: 1.0 Loss: 6.250150909181684e-05\n",
      "Iteration: 18580 Training Accuracy: 1.0 Loss: 9.507218783255666e-05\n",
      "Iteration: 18590 Training Accuracy: 1.0 Loss: 0.00010257715621264651\n",
      "Iteration: 18600 Training Accuracy: 1.0 Loss: 9.171085548587143e-05\n",
      "Iteration: 18610 Training Accuracy: 1.0 Loss: 7.229420589283109e-05\n",
      "Iteration: 18620 Training Accuracy: 1.0 Loss: 7.404688221868128e-05\n",
      "Iteration: 18630 Training Accuracy: 1.0 Loss: 6.594849401153624e-05\n",
      "Iteration: 18640 Training Accuracy: 1.0 Loss: 0.0002036207588389516\n",
      "Iteration: 18650 Training Accuracy: 1.0 Loss: 2.0121522084082244e-06\n",
      "Iteration: 18660 Training Accuracy: 1.0 Loss: 2.4180371838156134e-05\n",
      "Iteration: 18670 Training Accuracy: 1.0 Loss: 8.45261529320851e-05\n",
      "Iteration: 18680 Training Accuracy: 1.0 Loss: 0.00015090149827301502\n",
      "Iteration: 18690 Training Accuracy: 1.0 Loss: 0.00012082673492841423\n",
      "Iteration: 18700 Training Accuracy: 1.0 Loss: 0.00012900575529783964\n",
      "Iteration: 18710 Training Accuracy: 1.0 Loss: 5.9648587921401486e-05\n",
      "Iteration: 18720 Training Accuracy: 1.0 Loss: 2.2717595129506662e-05\n",
      "Iteration: 18730 Training Accuracy: 1.0 Loss: 1.9603592591010965e-05\n",
      "Iteration: 18740 Training Accuracy: 1.0 Loss: 6.247215787880123e-05\n",
      "Iteration: 18750 Training Accuracy: 1.0 Loss: 0.00010841313633136451\n",
      "Iteration: 18760 Training Accuracy: 1.0 Loss: 1.4606513104808982e-05\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9291666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy: 0.9997166666666667\n",
      "Validation Accuracy = 0.9291666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RESNET18_2(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use res net \n",
    "model = torch.load(\"full_resnet18_1_1.pth\")\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.005\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'full_resnet18_1_2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
