{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet34_Weights\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor = torch.load('./data_tensor.pth')\n",
    "train_labels_tensor = torch.load('./labels_tensor.pth')\n",
    "val_data_tensor = torch.load('./val_data_tensor.pth')\n",
    "val_labels_tensor = torch.load('./val_labels_tensor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = [], []\n",
    "for i in range(len(train_labels_tensor)):\n",
    "    train_dataset.append((train_data_tensor[i], train_labels_tensor[i]))\n",
    "\n",
    "for i in range(len(val_labels_tensor)):\n",
    "    val_dataset.append((val_data_tensor[i], val_labels_tensor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34 = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features1 = 128\n",
    "\n",
    "class RESNET34(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RESNET34, self).__init__()\n",
    "        # remove fully connected layer at the end\n",
    "        self.resnet = nn.Sequential(*list(resnet34.children())[:-1])\n",
    "        \n",
    "        self.fc1 = nn.Linear(resnet34.fc.in_features, out_features1)\n",
    "        self.fc2 = nn.Linear(out_features1, 30)\n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm1d(out_features1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        \n",
    "        # Forward pass through your fully connected layers\n",
    "        x = F.relu(self.batchnorm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy of the model prediction and the actual value\n",
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            model.eval()\n",
    "            if torch.backends.mps.is_built():\n",
    "                imgs = imgs.to(\"mps\")\n",
    "                labels = labels.to(\"mps\")\n",
    "            # get the output using resnet\n",
    "            output = model(imgs)\n",
    "\n",
    "            # select index with maximum prediction\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "            total += imgs.shape[0]\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using alex net\n",
    "def train(model, data, batch_size, num_epochs, learning_rate, momentum, verbose=False):\n",
    "    # use cross entropy loss function and SGD with momentum\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    iters, epochs, losses, train_acc, val_acc = [], [], [], [], []\n",
    "\n",
    "\n",
    "    n = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        mini_b = 0\n",
    "        mini_batch_correct = 0\n",
    "        mini_batch_total = 0\n",
    "        print(\"epoch: {}\".format(epoch))\n",
    "\n",
    "        for imgs, labels in iter(data):\n",
    "            if torch.backends.mps.is_built():\n",
    "                imgs = imgs.to(\"mps\")\n",
    "                labels = labels.to(\"mps\")\n",
    "            # calculate loss\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # find the loss and accuracy\n",
    "            pred = out.max(1, keepdim=True)[1]\n",
    "            mini_batch_correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "            mini_batch_total = imgs.shape[0]\n",
    "            train_acc.append(mini_batch_correct / mini_batch_total)\n",
    "            iters.append(n)\n",
    "            losses.append(float(loss) / batch_size)\n",
    "            n += 1\n",
    "            mini_b += 1\n",
    "            \n",
    "            if verbose and n % 10 == 0:\n",
    "                print(\"Iteration: {} Training Accuracy: {} Loss: {}\".format(n, train_acc[-1], losses[-1]))\n",
    "        scheduler.step()\n",
    "        # print the accuracy\n",
    "        val_acc.append(get_accuracy(model, val_loader))\n",
    "        epochs.append(epoch)\n",
    "        print(\"Training Accuracy = {}\".format(train_acc[-1]))\n",
    "        print(\"Validation Accuracy = {}\".format(val_acc[-1]))\n",
    "\n",
    "    # plot the loss curve\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot the training and validation curve\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(iters, train_acc, label=\"Training\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # plot the training and validation curve\n",
    "    plt.title(\"Validation Curve\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iteration: 10 Training Accuracy: 0.09375 Loss: 0.0535014346241951\n",
      "Iteration: 20 Training Accuracy: 0.21875 Loss: 0.047055259346961975\n",
      "Iteration: 30 Training Accuracy: 0.234375 Loss: 0.04458767548203468\n",
      "Iteration: 40 Training Accuracy: 0.34375 Loss: 0.04215949401259422\n",
      "Iteration: 50 Training Accuracy: 0.453125 Loss: 0.03816995024681091\n",
      "Iteration: 60 Training Accuracy: 0.375 Loss: 0.035341955721378326\n",
      "Iteration: 70 Training Accuracy: 0.40625 Loss: 0.034861285239458084\n",
      "Iteration: 80 Training Accuracy: 0.515625 Loss: 0.03184034302830696\n",
      "Iteration: 90 Training Accuracy: 0.515625 Loss: 0.029309816658496857\n",
      "Iteration: 100 Training Accuracy: 0.453125 Loss: 0.030030999332666397\n",
      "Iteration: 110 Training Accuracy: 0.53125 Loss: 0.027475297451019287\n",
      "Iteration: 120 Training Accuracy: 0.625 Loss: 0.022004393860697746\n",
      "Iteration: 130 Training Accuracy: 0.546875 Loss: 0.02673172578215599\n",
      "Iteration: 140 Training Accuracy: 0.546875 Loss: 0.025653496384620667\n",
      "Iteration: 150 Training Accuracy: 0.65625 Loss: 0.0221698135137558\n",
      "Iteration: 160 Training Accuracy: 0.625 Loss: 0.022896748036146164\n",
      "Iteration: 170 Training Accuracy: 0.625 Loss: 0.020871590822935104\n",
      "Iteration: 180 Training Accuracy: 0.53125 Loss: 0.024171650409698486\n",
      "Iteration: 190 Training Accuracy: 0.65625 Loss: 0.018879424780607224\n",
      "Iteration: 200 Training Accuracy: 0.671875 Loss: 0.018576135858893394\n",
      "Iteration: 210 Training Accuracy: 0.609375 Loss: 0.023922942578792572\n",
      "Iteration: 220 Training Accuracy: 0.578125 Loss: 0.020644962787628174\n",
      "Iteration: 230 Training Accuracy: 0.59375 Loss: 0.019603440538048744\n",
      "Iteration: 240 Training Accuracy: 0.609375 Loss: 0.020636914297938347\n",
      "Iteration: 250 Training Accuracy: 0.65625 Loss: 0.02106573060154915\n",
      "Iteration: 260 Training Accuracy: 0.65625 Loss: 0.017105478793382645\n",
      "Iteration: 270 Training Accuracy: 0.71875 Loss: 0.019083363935351372\n",
      "Iteration: 280 Training Accuracy: 0.609375 Loss: 0.01933509111404419\n",
      "Iteration: 290 Training Accuracy: 0.65625 Loss: 0.0170951709151268\n",
      "Iteration: 300 Training Accuracy: 0.734375 Loss: 0.014872078783810139\n",
      "Iteration: 310 Training Accuracy: 0.640625 Loss: 0.019500592723488808\n",
      "Iteration: 320 Training Accuracy: 0.75 Loss: 0.017297986894845963\n",
      "Iteration: 330 Training Accuracy: 0.65625 Loss: 0.0174265094101429\n",
      "Iteration: 340 Training Accuracy: 0.734375 Loss: 0.015566903166472912\n",
      "Iteration: 350 Training Accuracy: 0.65625 Loss: 0.01667904295027256\n",
      "Iteration: 360 Training Accuracy: 0.625 Loss: 0.019411597400903702\n",
      "Iteration: 370 Training Accuracy: 0.609375 Loss: 0.018859073519706726\n",
      "Iteration: 380 Training Accuracy: 0.609375 Loss: 0.021004777401685715\n",
      "Iteration: 390 Training Accuracy: 0.6875 Loss: 0.017873378470540047\n",
      "Iteration: 400 Training Accuracy: 0.625 Loss: 0.017732974141836166\n",
      "Iteration: 410 Training Accuracy: 0.6875 Loss: 0.01577673852443695\n",
      "Iteration: 420 Training Accuracy: 0.734375 Loss: 0.015221851877868176\n",
      "Iteration: 430 Training Accuracy: 0.671875 Loss: 0.016464535146951675\n",
      "Iteration: 440 Training Accuracy: 0.734375 Loss: 0.015398294664919376\n",
      "Iteration: 450 Training Accuracy: 0.703125 Loss: 0.01638314686715603\n",
      "Iteration: 460 Training Accuracy: 0.671875 Loss: 0.014842765405774117\n",
      "Iteration: 470 Training Accuracy: 0.6875 Loss: 0.017510443925857544\n",
      "Iteration: 480 Training Accuracy: 0.65625 Loss: 0.017640622332692146\n",
      "Iteration: 490 Training Accuracy: 0.8125 Loss: 0.011572962626814842\n",
      "Iteration: 500 Training Accuracy: 0.8125 Loss: 0.012907788157463074\n",
      "Iteration: 510 Training Accuracy: 0.75 Loss: 0.012754167430102825\n",
      "Iteration: 520 Training Accuracy: 0.609375 Loss: 0.018685221672058105\n",
      "Iteration: 530 Training Accuracy: 0.75 Loss: 0.010278161615133286\n",
      "Iteration: 540 Training Accuracy: 0.765625 Loss: 0.013144709169864655\n",
      "Iteration: 550 Training Accuracy: 0.734375 Loss: 0.01563423126935959\n",
      "Iteration: 560 Training Accuracy: 0.6875 Loss: 0.01566247269511223\n",
      "Iteration: 570 Training Accuracy: 0.78125 Loss: 0.011557787656784058\n",
      "Iteration: 580 Training Accuracy: 0.671875 Loss: 0.016699079424142838\n",
      "Iteration: 590 Training Accuracy: 0.734375 Loss: 0.013151369988918304\n",
      "Iteration: 600 Training Accuracy: 0.671875 Loss: 0.018391581252217293\n",
      "Iteration: 610 Training Accuracy: 0.75 Loss: 0.01078592799603939\n",
      "Iteration: 620 Training Accuracy: 0.734375 Loss: 0.014726963825523853\n",
      "Iteration: 630 Training Accuracy: 0.75 Loss: 0.013581754639744759\n",
      "Iteration: 640 Training Accuracy: 0.703125 Loss: 0.012508179992437363\n",
      "Iteration: 650 Training Accuracy: 0.828125 Loss: 0.01064363494515419\n",
      "Iteration: 660 Training Accuracy: 0.78125 Loss: 0.012883472256362438\n",
      "Iteration: 670 Training Accuracy: 0.796875 Loss: 0.013625698164105415\n",
      "Iteration: 680 Training Accuracy: 0.671875 Loss: 0.018638145178556442\n",
      "Iteration: 690 Training Accuracy: 0.71875 Loss: 0.014781543985009193\n",
      "Iteration: 700 Training Accuracy: 0.71875 Loss: 0.012843397445976734\n",
      "Iteration: 710 Training Accuracy: 0.78125 Loss: 0.010262208059430122\n",
      "Iteration: 720 Training Accuracy: 0.734375 Loss: 0.014623388648033142\n",
      "Iteration: 730 Training Accuracy: 0.78125 Loss: 0.01136326976120472\n",
      "Iteration: 740 Training Accuracy: 0.78125 Loss: 0.011818986386060715\n",
      "Iteration: 750 Training Accuracy: 0.71875 Loss: 0.013954659923911095\n",
      "Iteration: 760 Training Accuracy: 0.71875 Loss: 0.016713587567210197\n",
      "Iteration: 770 Training Accuracy: 0.796875 Loss: 0.012211489491164684\n",
      "Iteration: 780 Training Accuracy: 0.765625 Loss: 0.01235310360789299\n",
      "Iteration: 790 Training Accuracy: 0.78125 Loss: 0.011368805542588234\n",
      "Iteration: 800 Training Accuracy: 0.6875 Loss: 0.017072618007659912\n",
      "Iteration: 810 Training Accuracy: 0.75 Loss: 0.011742201633751392\n",
      "Iteration: 820 Training Accuracy: 0.78125 Loss: 0.010966742411255836\n",
      "Iteration: 830 Training Accuracy: 0.71875 Loss: 0.015108324587345123\n",
      "Iteration: 840 Training Accuracy: 0.75 Loss: 0.012190906330943108\n",
      "Iteration: 850 Training Accuracy: 0.796875 Loss: 0.011821024119853973\n",
      "Iteration: 860 Training Accuracy: 0.75 Loss: 0.010315392166376114\n",
      "Iteration: 870 Training Accuracy: 0.90625 Loss: 0.009644648060202599\n",
      "Iteration: 880 Training Accuracy: 0.75 Loss: 0.012913024984300137\n",
      "Iteration: 890 Training Accuracy: 0.78125 Loss: 0.01192413829267025\n",
      "Iteration: 900 Training Accuracy: 0.75 Loss: 0.011935003101825714\n",
      "Iteration: 910 Training Accuracy: 0.734375 Loss: 0.013784962706267834\n",
      "Iteration: 920 Training Accuracy: 0.8125 Loss: 0.00980202853679657\n",
      "Iteration: 930 Training Accuracy: 0.8125 Loss: 0.011548999696969986\n",
      "Training Accuracy = 0.78125\n",
      "Validation Accuracy = 0.8186666666666667\n",
      "epoch: 1\n",
      "Iteration: 940 Training Accuracy: 0.734375 Loss: 0.013031410053372383\n",
      "Iteration: 950 Training Accuracy: 0.78125 Loss: 0.010258503258228302\n",
      "Iteration: 960 Training Accuracy: 0.734375 Loss: 0.010582298040390015\n",
      "Iteration: 970 Training Accuracy: 0.84375 Loss: 0.009674882516264915\n",
      "Iteration: 980 Training Accuracy: 0.8125 Loss: 0.0113034937530756\n",
      "Iteration: 990 Training Accuracy: 0.828125 Loss: 0.008739098906517029\n",
      "Iteration: 1000 Training Accuracy: 0.84375 Loss: 0.007636500522494316\n",
      "Iteration: 1010 Training Accuracy: 0.8125 Loss: 0.008257332257926464\n",
      "Iteration: 1020 Training Accuracy: 0.828125 Loss: 0.008450482040643692\n",
      "Iteration: 1030 Training Accuracy: 0.90625 Loss: 0.007619996555149555\n",
      "Iteration: 1040 Training Accuracy: 0.765625 Loss: 0.012611579149961472\n",
      "Iteration: 1050 Training Accuracy: 0.859375 Loss: 0.008691100403666496\n",
      "Iteration: 1060 Training Accuracy: 0.828125 Loss: 0.007558080833405256\n",
      "Iteration: 1070 Training Accuracy: 0.78125 Loss: 0.010981012135744095\n",
      "Iteration: 1080 Training Accuracy: 0.8125 Loss: 0.008101693354547024\n",
      "Iteration: 1090 Training Accuracy: 0.875 Loss: 0.007821806706488132\n",
      "Iteration: 1100 Training Accuracy: 0.875 Loss: 0.008111543953418732\n",
      "Iteration: 1110 Training Accuracy: 0.875 Loss: 0.006793559528887272\n",
      "Iteration: 1120 Training Accuracy: 0.8125 Loss: 0.009042877703905106\n",
      "Iteration: 1130 Training Accuracy: 0.8125 Loss: 0.008831122890114784\n",
      "Iteration: 1140 Training Accuracy: 0.9375 Loss: 0.0035539448726922274\n",
      "Iteration: 1150 Training Accuracy: 0.84375 Loss: 0.008064685389399529\n",
      "Iteration: 1160 Training Accuracy: 0.75 Loss: 0.010625511407852173\n",
      "Iteration: 1170 Training Accuracy: 0.8125 Loss: 0.01032235473394394\n",
      "Iteration: 1180 Training Accuracy: 0.921875 Loss: 0.0048614442348480225\n",
      "Iteration: 1190 Training Accuracy: 0.84375 Loss: 0.007340813055634499\n",
      "Iteration: 1200 Training Accuracy: 0.828125 Loss: 0.009252478368580341\n",
      "Iteration: 1210 Training Accuracy: 0.828125 Loss: 0.00900642666965723\n",
      "Iteration: 1220 Training Accuracy: 0.8125 Loss: 0.009124995209276676\n",
      "Iteration: 1230 Training Accuracy: 0.828125 Loss: 0.009262194857001305\n",
      "Iteration: 1240 Training Accuracy: 0.875 Loss: 0.007134119980037212\n",
      "Iteration: 1250 Training Accuracy: 0.765625 Loss: 0.014479720033705235\n",
      "Iteration: 1260 Training Accuracy: 0.8125 Loss: 0.00890131201595068\n",
      "Iteration: 1270 Training Accuracy: 0.859375 Loss: 0.007470244076102972\n",
      "Iteration: 1280 Training Accuracy: 0.875 Loss: 0.006279545836150646\n",
      "Iteration: 1290 Training Accuracy: 0.8125 Loss: 0.009077271446585655\n",
      "Iteration: 1300 Training Accuracy: 0.828125 Loss: 0.007800082676112652\n",
      "Iteration: 1310 Training Accuracy: 0.8125 Loss: 0.007387111894786358\n",
      "Iteration: 1320 Training Accuracy: 0.75 Loss: 0.01092994213104248\n",
      "Iteration: 1330 Training Accuracy: 0.84375 Loss: 0.007485884241759777\n",
      "Iteration: 1340 Training Accuracy: 0.78125 Loss: 0.010289428755640984\n",
      "Iteration: 1350 Training Accuracy: 0.765625 Loss: 0.012480639852583408\n",
      "Iteration: 1360 Training Accuracy: 0.8125 Loss: 0.008632838726043701\n",
      "Iteration: 1370 Training Accuracy: 0.8125 Loss: 0.009484396316111088\n",
      "Iteration: 1380 Training Accuracy: 0.875 Loss: 0.006522282026708126\n",
      "Iteration: 1390 Training Accuracy: 0.8125 Loss: 0.008485791273415089\n",
      "Iteration: 1400 Training Accuracy: 0.859375 Loss: 0.007738927379250526\n",
      "Iteration: 1410 Training Accuracy: 0.875 Loss: 0.006971972994506359\n",
      "Iteration: 1420 Training Accuracy: 0.890625 Loss: 0.005653173662722111\n",
      "Iteration: 1430 Training Accuracy: 0.859375 Loss: 0.006407433655112982\n",
      "Iteration: 1440 Training Accuracy: 0.78125 Loss: 0.00912261102348566\n",
      "Iteration: 1450 Training Accuracy: 0.859375 Loss: 0.0076059987768530846\n",
      "Iteration: 1460 Training Accuracy: 0.890625 Loss: 0.006125240586698055\n",
      "Iteration: 1470 Training Accuracy: 0.828125 Loss: 0.007614362984895706\n",
      "Iteration: 1480 Training Accuracy: 0.859375 Loss: 0.005806819535791874\n",
      "Iteration: 1490 Training Accuracy: 0.890625 Loss: 0.006298254709690809\n",
      "Iteration: 1500 Training Accuracy: 0.890625 Loss: 0.004607748240232468\n",
      "Iteration: 1510 Training Accuracy: 0.875 Loss: 0.006491073872894049\n",
      "Iteration: 1520 Training Accuracy: 0.953125 Loss: 0.0041171652264893055\n",
      "Iteration: 1530 Training Accuracy: 0.875 Loss: 0.004462411627173424\n",
      "Iteration: 1540 Training Accuracy: 0.8125 Loss: 0.008939391933381557\n",
      "Iteration: 1550 Training Accuracy: 0.796875 Loss: 0.00998666137456894\n",
      "Iteration: 1560 Training Accuracy: 0.84375 Loss: 0.008232719264924526\n",
      "Iteration: 1570 Training Accuracy: 0.9375 Loss: 0.004721042700111866\n",
      "Iteration: 1580 Training Accuracy: 0.9375 Loss: 0.003553139977157116\n",
      "Iteration: 1590 Training Accuracy: 0.859375 Loss: 0.008198237046599388\n",
      "Iteration: 1600 Training Accuracy: 0.84375 Loss: 0.00763465091586113\n",
      "Iteration: 1610 Training Accuracy: 0.84375 Loss: 0.01035265065729618\n",
      "Iteration: 1620 Training Accuracy: 0.828125 Loss: 0.009386183694005013\n",
      "Iteration: 1630 Training Accuracy: 0.921875 Loss: 0.004359785467386246\n",
      "Iteration: 1640 Training Accuracy: 0.796875 Loss: 0.009473980404436588\n",
      "Iteration: 1650 Training Accuracy: 0.8125 Loss: 0.00769457221031189\n",
      "Iteration: 1660 Training Accuracy: 0.859375 Loss: 0.005875606555491686\n",
      "Iteration: 1670 Training Accuracy: 0.90625 Loss: 0.006376049946993589\n",
      "Iteration: 1680 Training Accuracy: 0.90625 Loss: 0.004023220855742693\n",
      "Iteration: 1690 Training Accuracy: 0.875 Loss: 0.004723287653177977\n",
      "Iteration: 1700 Training Accuracy: 0.890625 Loss: 0.005907226353883743\n",
      "Iteration: 1710 Training Accuracy: 0.84375 Loss: 0.007131319493055344\n",
      "Iteration: 1720 Training Accuracy: 0.8125 Loss: 0.010046599432826042\n",
      "Iteration: 1730 Training Accuracy: 0.875 Loss: 0.0069544631987810135\n",
      "Iteration: 1740 Training Accuracy: 0.859375 Loss: 0.00757733965292573\n",
      "Iteration: 1750 Training Accuracy: 0.890625 Loss: 0.006884826812893152\n",
      "Iteration: 1760 Training Accuracy: 0.921875 Loss: 0.004966764245182276\n",
      "Iteration: 1770 Training Accuracy: 0.859375 Loss: 0.007333680987358093\n",
      "Iteration: 1780 Training Accuracy: 0.859375 Loss: 0.006569299381226301\n",
      "Iteration: 1790 Training Accuracy: 0.78125 Loss: 0.009416995570063591\n",
      "Iteration: 1800 Training Accuracy: 0.8125 Loss: 0.007713004015386105\n",
      "Iteration: 1810 Training Accuracy: 0.921875 Loss: 0.004352276213467121\n",
      "Iteration: 1820 Training Accuracy: 0.859375 Loss: 0.007115345448255539\n",
      "Iteration: 1830 Training Accuracy: 0.828125 Loss: 0.007810750976204872\n",
      "Iteration: 1840 Training Accuracy: 0.875 Loss: 0.005566499195992947\n",
      "Iteration: 1850 Training Accuracy: 0.875 Loss: 0.006445880979299545\n",
      "Iteration: 1860 Training Accuracy: 0.90625 Loss: 0.0037842895835638046\n",
      "Iteration: 1870 Training Accuracy: 0.890625 Loss: 0.004960843361914158\n",
      "Training Accuracy = 0.84375\n",
      "Validation Accuracy = 0.8571666666666666\n",
      "epoch: 2\n",
      "Iteration: 1880 Training Accuracy: 0.890625 Loss: 0.006664967630058527\n",
      "Iteration: 1890 Training Accuracy: 0.828125 Loss: 0.007084978744387627\n",
      "Iteration: 1900 Training Accuracy: 0.90625 Loss: 0.003643775824457407\n",
      "Iteration: 1910 Training Accuracy: 0.90625 Loss: 0.0062571242451667786\n",
      "Iteration: 1920 Training Accuracy: 0.890625 Loss: 0.005963037721812725\n",
      "Iteration: 1930 Training Accuracy: 0.78125 Loss: 0.00989249162375927\n",
      "Iteration: 1940 Training Accuracy: 0.84375 Loss: 0.0066307964734733105\n",
      "Iteration: 1950 Training Accuracy: 0.828125 Loss: 0.008718835189938545\n",
      "Iteration: 1960 Training Accuracy: 0.828125 Loss: 0.0052276193164289\n",
      "Iteration: 1970 Training Accuracy: 0.90625 Loss: 0.004087948240339756\n",
      "Iteration: 1980 Training Accuracy: 0.8125 Loss: 0.00891072303056717\n",
      "Iteration: 1990 Training Accuracy: 0.890625 Loss: 0.005356769543141127\n",
      "Iteration: 2000 Training Accuracy: 0.828125 Loss: 0.007929012179374695\n",
      "Iteration: 2010 Training Accuracy: 0.78125 Loss: 0.0105836046859622\n",
      "Iteration: 2020 Training Accuracy: 0.90625 Loss: 0.004848070442676544\n",
      "Iteration: 2030 Training Accuracy: 0.859375 Loss: 0.005177696701139212\n",
      "Iteration: 2040 Training Accuracy: 0.84375 Loss: 0.006783493794500828\n",
      "Iteration: 2050 Training Accuracy: 0.890625 Loss: 0.005543224513530731\n",
      "Iteration: 2060 Training Accuracy: 0.890625 Loss: 0.006132322363555431\n",
      "Iteration: 2070 Training Accuracy: 0.828125 Loss: 0.007020134944468737\n",
      "Iteration: 2080 Training Accuracy: 0.90625 Loss: 0.00487982714548707\n",
      "Iteration: 2090 Training Accuracy: 0.890625 Loss: 0.0058130016550421715\n",
      "Iteration: 2100 Training Accuracy: 0.890625 Loss: 0.005068567581474781\n",
      "Iteration: 2110 Training Accuracy: 0.921875 Loss: 0.005430629942566156\n",
      "Iteration: 2120 Training Accuracy: 0.859375 Loss: 0.006311909295618534\n",
      "Iteration: 2130 Training Accuracy: 0.921875 Loss: 0.0052044386975467205\n",
      "Iteration: 2140 Training Accuracy: 0.9375 Loss: 0.0028483292553573847\n",
      "Iteration: 2150 Training Accuracy: 0.921875 Loss: 0.0038021327927708626\n",
      "Iteration: 2160 Training Accuracy: 0.859375 Loss: 0.008478144183754921\n",
      "Iteration: 2170 Training Accuracy: 0.84375 Loss: 0.007120432332158089\n",
      "Iteration: 2180 Training Accuracy: 0.953125 Loss: 0.0033968801144510508\n",
      "Iteration: 2190 Training Accuracy: 0.90625 Loss: 0.004476611502468586\n",
      "Iteration: 2200 Training Accuracy: 0.828125 Loss: 0.008241426199674606\n",
      "Iteration: 2210 Training Accuracy: 0.859375 Loss: 0.006314415484666824\n",
      "Iteration: 2220 Training Accuracy: 0.875 Loss: 0.008105834014713764\n",
      "Iteration: 2230 Training Accuracy: 0.96875 Loss: 0.0019359111320227385\n",
      "Iteration: 2240 Training Accuracy: 0.859375 Loss: 0.00738073093816638\n",
      "Iteration: 2250 Training Accuracy: 0.875 Loss: 0.005442105233669281\n",
      "Iteration: 2260 Training Accuracy: 0.859375 Loss: 0.007191087119281292\n",
      "Iteration: 2270 Training Accuracy: 0.875 Loss: 0.005620741285383701\n",
      "Iteration: 2280 Training Accuracy: 0.9375 Loss: 0.002711661858484149\n",
      "Iteration: 2290 Training Accuracy: 0.890625 Loss: 0.005268413573503494\n",
      "Iteration: 2300 Training Accuracy: 0.90625 Loss: 0.006063027307391167\n",
      "Iteration: 2310 Training Accuracy: 0.9375 Loss: 0.005232428200542927\n",
      "Iteration: 2320 Training Accuracy: 0.921875 Loss: 0.005226471461355686\n",
      "Iteration: 2330 Training Accuracy: 0.90625 Loss: 0.004883327521383762\n",
      "Iteration: 2340 Training Accuracy: 0.9375 Loss: 0.004042675253003836\n",
      "Iteration: 2350 Training Accuracy: 0.90625 Loss: 0.005031069274991751\n",
      "Iteration: 2360 Training Accuracy: 0.859375 Loss: 0.006498295813798904\n",
      "Iteration: 2370 Training Accuracy: 0.875 Loss: 0.006543123163282871\n",
      "Iteration: 2380 Training Accuracy: 0.875 Loss: 0.007173982914537191\n",
      "Iteration: 2390 Training Accuracy: 0.84375 Loss: 0.005840081721544266\n",
      "Iteration: 2400 Training Accuracy: 0.859375 Loss: 0.0058265868574380875\n",
      "Iteration: 2410 Training Accuracy: 0.890625 Loss: 0.007866008207201958\n",
      "Iteration: 2420 Training Accuracy: 0.859375 Loss: 0.006163054145872593\n",
      "Iteration: 2430 Training Accuracy: 0.828125 Loss: 0.007037193980067968\n",
      "Iteration: 2440 Training Accuracy: 0.90625 Loss: 0.0046287109144032\n",
      "Iteration: 2450 Training Accuracy: 0.875 Loss: 0.005994837731122971\n",
      "Iteration: 2460 Training Accuracy: 0.875 Loss: 0.005883092060685158\n",
      "Iteration: 2470 Training Accuracy: 0.875 Loss: 0.004977813456207514\n",
      "Iteration: 2480 Training Accuracy: 0.875 Loss: 0.006692313589155674\n",
      "Iteration: 2490 Training Accuracy: 0.875 Loss: 0.005121189169585705\n",
      "Iteration: 2500 Training Accuracy: 0.84375 Loss: 0.008388709276914597\n",
      "Iteration: 2510 Training Accuracy: 0.875 Loss: 0.007268252316862345\n",
      "Iteration: 2520 Training Accuracy: 0.921875 Loss: 0.006186339072883129\n",
      "Iteration: 2530 Training Accuracy: 0.9375 Loss: 0.0046966783702373505\n",
      "Iteration: 2540 Training Accuracy: 0.90625 Loss: 0.005641622934490442\n",
      "Iteration: 2550 Training Accuracy: 0.953125 Loss: 0.002485850593075156\n",
      "Iteration: 2560 Training Accuracy: 0.890625 Loss: 0.005225779488682747\n",
      "Iteration: 2570 Training Accuracy: 0.921875 Loss: 0.004959264770150185\n",
      "Iteration: 2580 Training Accuracy: 0.890625 Loss: 0.004542528185993433\n",
      "Iteration: 2590 Training Accuracy: 0.953125 Loss: 0.003148050745949149\n",
      "Iteration: 2600 Training Accuracy: 0.828125 Loss: 0.008210897445678711\n",
      "Iteration: 2610 Training Accuracy: 0.921875 Loss: 0.0039595952257514\n",
      "Iteration: 2620 Training Accuracy: 0.9375 Loss: 0.004359956830739975\n",
      "Iteration: 2630 Training Accuracy: 0.90625 Loss: 0.0058395895175635815\n",
      "Iteration: 2640 Training Accuracy: 0.84375 Loss: 0.008904125541448593\n",
      "Iteration: 2650 Training Accuracy: 0.84375 Loss: 0.007204524241387844\n",
      "Iteration: 2660 Training Accuracy: 0.78125 Loss: 0.010417568497359753\n",
      "Iteration: 2670 Training Accuracy: 0.890625 Loss: 0.0047445944510400295\n",
      "Iteration: 2680 Training Accuracy: 0.921875 Loss: 0.004505298566073179\n",
      "Iteration: 2690 Training Accuracy: 0.796875 Loss: 0.009146353229880333\n",
      "Iteration: 2700 Training Accuracy: 0.859375 Loss: 0.006294068414717913\n",
      "Iteration: 2710 Training Accuracy: 0.921875 Loss: 0.004622296430170536\n",
      "Iteration: 2720 Training Accuracy: 0.921875 Loss: 0.004117230419069529\n",
      "Iteration: 2730 Training Accuracy: 0.875 Loss: 0.005642264150083065\n",
      "Iteration: 2740 Training Accuracy: 0.90625 Loss: 0.00467698834836483\n",
      "Iteration: 2750 Training Accuracy: 0.9375 Loss: 0.003535183612257242\n",
      "Iteration: 2760 Training Accuracy: 0.859375 Loss: 0.006411188747733831\n",
      "Iteration: 2770 Training Accuracy: 0.859375 Loss: 0.0061246538534760475\n",
      "Iteration: 2780 Training Accuracy: 0.875 Loss: 0.004878840409219265\n",
      "Iteration: 2790 Training Accuracy: 0.890625 Loss: 0.006088654510676861\n",
      "Iteration: 2800 Training Accuracy: 0.859375 Loss: 0.005621951073408127\n",
      "Iteration: 2810 Training Accuracy: 0.96875 Loss: 0.002238730899989605\n",
      "Training Accuracy = 0.9375\n",
      "Validation Accuracy = 0.8753333333333333\n",
      "epoch: 3\n",
      "Iteration: 2820 Training Accuracy: 0.90625 Loss: 0.004968684632331133\n",
      "Iteration: 2830 Training Accuracy: 0.890625 Loss: 0.004694004077464342\n",
      "Iteration: 2840 Training Accuracy: 0.921875 Loss: 0.002997639123350382\n",
      "Iteration: 2850 Training Accuracy: 0.953125 Loss: 0.003128599375486374\n",
      "Iteration: 2860 Training Accuracy: 0.953125 Loss: 0.00425175204873085\n",
      "Iteration: 2870 Training Accuracy: 0.921875 Loss: 0.005702294409275055\n",
      "Iteration: 2880 Training Accuracy: 0.875 Loss: 0.0064130621030926704\n",
      "Iteration: 2890 Training Accuracy: 0.890625 Loss: 0.005314799956977367\n",
      "Iteration: 2900 Training Accuracy: 0.859375 Loss: 0.005059706978499889\n",
      "Iteration: 2910 Training Accuracy: 0.875 Loss: 0.00471216905862093\n",
      "Iteration: 2920 Training Accuracy: 0.890625 Loss: 0.005169427487999201\n",
      "Iteration: 2930 Training Accuracy: 0.875 Loss: 0.004493090324103832\n",
      "Iteration: 2940 Training Accuracy: 0.90625 Loss: 0.005789495073258877\n",
      "Iteration: 2950 Training Accuracy: 0.890625 Loss: 0.0044718277640640736\n",
      "Iteration: 2960 Training Accuracy: 0.90625 Loss: 0.005177910439670086\n",
      "Iteration: 2970 Training Accuracy: 0.859375 Loss: 0.007543858140707016\n",
      "Iteration: 2980 Training Accuracy: 0.890625 Loss: 0.004823080729693174\n",
      "Iteration: 2990 Training Accuracy: 0.875 Loss: 0.004431222099810839\n",
      "Iteration: 3000 Training Accuracy: 0.9375 Loss: 0.0028130016289651394\n",
      "Iteration: 3010 Training Accuracy: 0.9375 Loss: 0.0036827409639954567\n",
      "Iteration: 3020 Training Accuracy: 0.9375 Loss: 0.003306204918771982\n",
      "Iteration: 3030 Training Accuracy: 0.875 Loss: 0.008496656082570553\n",
      "Iteration: 3040 Training Accuracy: 0.9375 Loss: 0.0029538688249886036\n",
      "Iteration: 3050 Training Accuracy: 0.9375 Loss: 0.003457611659541726\n",
      "Iteration: 3060 Training Accuracy: 0.90625 Loss: 0.0045002358965575695\n",
      "Iteration: 3070 Training Accuracy: 0.953125 Loss: 0.0030167170334607363\n",
      "Iteration: 3080 Training Accuracy: 0.9375 Loss: 0.00406938511878252\n",
      "Iteration: 3090 Training Accuracy: 0.90625 Loss: 0.004511924460530281\n",
      "Iteration: 3100 Training Accuracy: 0.96875 Loss: 0.0022273161448538303\n",
      "Iteration: 3110 Training Accuracy: 0.9375 Loss: 0.0034238810185343027\n",
      "Iteration: 3120 Training Accuracy: 0.921875 Loss: 0.0028027251828461885\n",
      "Iteration: 3130 Training Accuracy: 0.90625 Loss: 0.004859027452766895\n",
      "Iteration: 3140 Training Accuracy: 0.953125 Loss: 0.0034432983957231045\n",
      "Iteration: 3150 Training Accuracy: 0.859375 Loss: 0.006459414027631283\n",
      "Iteration: 3160 Training Accuracy: 0.953125 Loss: 0.0031488128006458282\n",
      "Iteration: 3170 Training Accuracy: 0.9375 Loss: 0.0034160485956817865\n",
      "Iteration: 3180 Training Accuracy: 0.9375 Loss: 0.0030567916110157967\n",
      "Iteration: 3190 Training Accuracy: 0.84375 Loss: 0.004937148652970791\n",
      "Iteration: 3200 Training Accuracy: 0.859375 Loss: 0.00650921743363142\n",
      "Iteration: 3210 Training Accuracy: 0.96875 Loss: 0.0027590948157012463\n",
      "Iteration: 3220 Training Accuracy: 0.9375 Loss: 0.0026733786799013615\n",
      "Iteration: 3230 Training Accuracy: 0.9375 Loss: 0.0047271461226046085\n",
      "Iteration: 3240 Training Accuracy: 0.921875 Loss: 0.004077455494552851\n",
      "Iteration: 3250 Training Accuracy: 0.890625 Loss: 0.004287030082195997\n",
      "Iteration: 3260 Training Accuracy: 0.90625 Loss: 0.0038565509021282196\n",
      "Iteration: 3270 Training Accuracy: 0.890625 Loss: 0.005244122818112373\n",
      "Iteration: 3280 Training Accuracy: 0.890625 Loss: 0.004295988939702511\n",
      "Iteration: 3290 Training Accuracy: 0.875 Loss: 0.007254066877067089\n",
      "Iteration: 3300 Training Accuracy: 0.921875 Loss: 0.0061075822450220585\n",
      "Iteration: 3310 Training Accuracy: 0.890625 Loss: 0.004245511256158352\n",
      "Iteration: 3320 Training Accuracy: 0.90625 Loss: 0.00535275973379612\n",
      "Iteration: 3330 Training Accuracy: 0.953125 Loss: 0.0033851792104542255\n",
      "Iteration: 3340 Training Accuracy: 0.890625 Loss: 0.006629997864365578\n",
      "Iteration: 3350 Training Accuracy: 0.921875 Loss: 0.003439967753365636\n",
      "Iteration: 3360 Training Accuracy: 0.890625 Loss: 0.005506525281816721\n",
      "Iteration: 3370 Training Accuracy: 0.90625 Loss: 0.0056541478261351585\n",
      "Iteration: 3380 Training Accuracy: 0.9375 Loss: 0.004111052490770817\n",
      "Iteration: 3390 Training Accuracy: 0.921875 Loss: 0.00431843800470233\n",
      "Iteration: 3400 Training Accuracy: 0.90625 Loss: 0.00616130605340004\n",
      "Iteration: 3410 Training Accuracy: 0.921875 Loss: 0.0040253582410514355\n",
      "Iteration: 3420 Training Accuracy: 0.921875 Loss: 0.004200142342597246\n",
      "Iteration: 3430 Training Accuracy: 0.96875 Loss: 0.0011997735127806664\n",
      "Iteration: 3440 Training Accuracy: 0.875 Loss: 0.004262398928403854\n",
      "Iteration: 3450 Training Accuracy: 0.953125 Loss: 0.002968009328469634\n",
      "Iteration: 3460 Training Accuracy: 0.90625 Loss: 0.004603689070791006\n",
      "Iteration: 3470 Training Accuracy: 0.953125 Loss: 0.003069009631872177\n",
      "Iteration: 3480 Training Accuracy: 0.90625 Loss: 0.004430406726896763\n",
      "Iteration: 3490 Training Accuracy: 0.9375 Loss: 0.0033735311590135098\n",
      "Iteration: 3500 Training Accuracy: 0.84375 Loss: 0.006146942265331745\n",
      "Iteration: 3510 Training Accuracy: 0.875 Loss: 0.005690484773367643\n",
      "Iteration: 3520 Training Accuracy: 0.921875 Loss: 0.004554735496640205\n",
      "Iteration: 3530 Training Accuracy: 0.828125 Loss: 0.005320422351360321\n",
      "Iteration: 3540 Training Accuracy: 0.9375 Loss: 0.0031592771410942078\n",
      "Iteration: 3550 Training Accuracy: 0.984375 Loss: 0.003832947462797165\n",
      "Iteration: 3560 Training Accuracy: 0.890625 Loss: 0.005529480054974556\n",
      "Iteration: 3570 Training Accuracy: 0.859375 Loss: 0.004492408595979214\n",
      "Iteration: 3580 Training Accuracy: 0.84375 Loss: 0.0060239373706281185\n",
      "Iteration: 3590 Training Accuracy: 0.90625 Loss: 0.0040996018797159195\n",
      "Iteration: 3600 Training Accuracy: 0.953125 Loss: 0.003674356732517481\n",
      "Iteration: 3610 Training Accuracy: 0.9375 Loss: 0.0030132003594189882\n",
      "Iteration: 3620 Training Accuracy: 0.90625 Loss: 0.0037798318080604076\n",
      "Iteration: 3630 Training Accuracy: 0.859375 Loss: 0.007041038945317268\n",
      "Iteration: 3640 Training Accuracy: 0.9375 Loss: 0.004582899622619152\n",
      "Iteration: 3650 Training Accuracy: 0.90625 Loss: 0.004209103528410196\n",
      "Iteration: 3660 Training Accuracy: 0.875 Loss: 0.004770447965711355\n",
      "Iteration: 3670 Training Accuracy: 0.890625 Loss: 0.006829767022281885\n",
      "Iteration: 3680 Training Accuracy: 0.890625 Loss: 0.004906798712909222\n",
      "Iteration: 3690 Training Accuracy: 0.90625 Loss: 0.004287504591047764\n",
      "Iteration: 3700 Training Accuracy: 0.8125 Loss: 0.007522909436374903\n",
      "Iteration: 3710 Training Accuracy: 0.921875 Loss: 0.0024001789279282093\n",
      "Iteration: 3720 Training Accuracy: 0.90625 Loss: 0.004221358336508274\n",
      "Iteration: 3730 Training Accuracy: 0.9375 Loss: 0.004171078559011221\n",
      "Iteration: 3740 Training Accuracy: 0.90625 Loss: 0.006578679662197828\n",
      "Iteration: 3750 Training Accuracy: 0.953125 Loss: 0.0021995750721544027\n",
      "Training Accuracy = 0.96875\n",
      "Validation Accuracy = 0.8843333333333333\n",
      "epoch: 4\n",
      "Iteration: 3760 Training Accuracy: 0.90625 Loss: 0.0033777975477278233\n",
      "Iteration: 3770 Training Accuracy: 0.96875 Loss: 0.0017716668080538511\n",
      "Iteration: 3780 Training Accuracy: 0.890625 Loss: 0.005064750090241432\n",
      "Iteration: 3790 Training Accuracy: 0.96875 Loss: 0.001969151431694627\n",
      "Iteration: 3800 Training Accuracy: 0.9375 Loss: 0.0032865318935364485\n",
      "Iteration: 3810 Training Accuracy: 0.921875 Loss: 0.004145022016018629\n",
      "Iteration: 3820 Training Accuracy: 0.875 Loss: 0.005086966324597597\n",
      "Iteration: 3830 Training Accuracy: 0.875 Loss: 0.00419855210930109\n",
      "Iteration: 3840 Training Accuracy: 0.953125 Loss: 0.004233620595186949\n",
      "Iteration: 3850 Training Accuracy: 0.9375 Loss: 0.0025382409803569317\n",
      "Iteration: 3860 Training Accuracy: 0.921875 Loss: 0.003111541736871004\n",
      "Iteration: 3870 Training Accuracy: 0.828125 Loss: 0.006993710529059172\n",
      "Iteration: 3880 Training Accuracy: 0.890625 Loss: 0.004215755499899387\n",
      "Iteration: 3890 Training Accuracy: 0.84375 Loss: 0.006969904527068138\n",
      "Iteration: 3900 Training Accuracy: 0.84375 Loss: 0.008865034207701683\n",
      "Iteration: 3910 Training Accuracy: 0.90625 Loss: 0.0035078420769423246\n",
      "Iteration: 3920 Training Accuracy: 0.9375 Loss: 0.002921667881309986\n",
      "Iteration: 3930 Training Accuracy: 0.9375 Loss: 0.0035283444449305534\n",
      "Iteration: 3940 Training Accuracy: 0.921875 Loss: 0.003135340753942728\n",
      "Iteration: 3950 Training Accuracy: 0.984375 Loss: 0.0014299012254923582\n",
      "Iteration: 3960 Training Accuracy: 1.0 Loss: 0.001553455600515008\n",
      "Iteration: 3970 Training Accuracy: 0.953125 Loss: 0.0030027751345187426\n",
      "Iteration: 3980 Training Accuracy: 0.96875 Loss: 0.0025594725739210844\n",
      "Iteration: 3990 Training Accuracy: 0.890625 Loss: 0.004419670440256596\n",
      "Iteration: 4000 Training Accuracy: 0.875 Loss: 0.003907851874828339\n",
      "Iteration: 4010 Training Accuracy: 0.890625 Loss: 0.004002182744443417\n",
      "Iteration: 4020 Training Accuracy: 0.875 Loss: 0.0055641597136855125\n",
      "Iteration: 4030 Training Accuracy: 0.875 Loss: 0.006699064746499062\n",
      "Iteration: 4040 Training Accuracy: 0.90625 Loss: 0.004447205923497677\n",
      "Iteration: 4050 Training Accuracy: 0.859375 Loss: 0.004692032001912594\n",
      "Iteration: 4060 Training Accuracy: 0.875 Loss: 0.005619102157652378\n",
      "Iteration: 4070 Training Accuracy: 0.875 Loss: 0.004866760224103928\n",
      "Iteration: 4080 Training Accuracy: 0.953125 Loss: 0.002728667575865984\n",
      "Iteration: 4090 Training Accuracy: 1.0 Loss: 0.0006799036636948586\n",
      "Iteration: 4100 Training Accuracy: 0.921875 Loss: 0.002518558641895652\n",
      "Iteration: 4110 Training Accuracy: 0.953125 Loss: 0.002173410961404443\n",
      "Iteration: 4120 Training Accuracy: 0.953125 Loss: 0.0028343996964395046\n",
      "Iteration: 4130 Training Accuracy: 0.921875 Loss: 0.0027904952876269817\n",
      "Iteration: 4140 Training Accuracy: 0.921875 Loss: 0.0043814415112137794\n",
      "Iteration: 4150 Training Accuracy: 0.921875 Loss: 0.003490003990009427\n",
      "Iteration: 4160 Training Accuracy: 0.921875 Loss: 0.003873805282637477\n",
      "Iteration: 4170 Training Accuracy: 0.890625 Loss: 0.003433382138609886\n",
      "Iteration: 4180 Training Accuracy: 0.890625 Loss: 0.0052806721068918705\n",
      "Iteration: 4190 Training Accuracy: 0.921875 Loss: 0.0035220091231167316\n",
      "Iteration: 4200 Training Accuracy: 0.921875 Loss: 0.00553225539624691\n",
      "Iteration: 4210 Training Accuracy: 0.90625 Loss: 0.006732175126671791\n",
      "Iteration: 4220 Training Accuracy: 0.953125 Loss: 0.0015681509394198656\n",
      "Iteration: 4230 Training Accuracy: 0.9375 Loss: 0.0032958611845970154\n",
      "Iteration: 4240 Training Accuracy: 0.90625 Loss: 0.003642059862613678\n",
      "Iteration: 4250 Training Accuracy: 0.953125 Loss: 0.003254703478887677\n",
      "Iteration: 4260 Training Accuracy: 0.921875 Loss: 0.002401706762611866\n",
      "Iteration: 4270 Training Accuracy: 0.953125 Loss: 0.0036878599785268307\n",
      "Iteration: 4280 Training Accuracy: 0.96875 Loss: 0.003046753816306591\n",
      "Iteration: 4290 Training Accuracy: 0.96875 Loss: 0.0018810813780874014\n",
      "Iteration: 4300 Training Accuracy: 0.9375 Loss: 0.0030393872875720263\n",
      "Iteration: 4310 Training Accuracy: 0.875 Loss: 0.006531462073326111\n",
      "Iteration: 4320 Training Accuracy: 0.921875 Loss: 0.004246181342750788\n",
      "Iteration: 4330 Training Accuracy: 0.921875 Loss: 0.0035743978805840015\n",
      "Iteration: 4340 Training Accuracy: 0.953125 Loss: 0.003112310077995062\n",
      "Iteration: 4350 Training Accuracy: 0.9375 Loss: 0.003479165956377983\n",
      "Iteration: 4360 Training Accuracy: 0.9375 Loss: 0.0033901971764862537\n",
      "Iteration: 4370 Training Accuracy: 0.9375 Loss: 0.004103301092982292\n",
      "Iteration: 4380 Training Accuracy: 0.921875 Loss: 0.0030049802735447884\n",
      "Iteration: 4390 Training Accuracy: 0.890625 Loss: 0.0049703363329172134\n",
      "Iteration: 4400 Training Accuracy: 0.984375 Loss: 0.001889309729449451\n",
      "Iteration: 4410 Training Accuracy: 0.921875 Loss: 0.005465466529130936\n",
      "Iteration: 4420 Training Accuracy: 0.921875 Loss: 0.0038422085344791412\n",
      "Iteration: 4430 Training Accuracy: 0.921875 Loss: 0.0032998870592564344\n",
      "Iteration: 4440 Training Accuracy: 0.9375 Loss: 0.0028771967627108097\n",
      "Iteration: 4450 Training Accuracy: 0.953125 Loss: 0.002538367873057723\n",
      "Iteration: 4460 Training Accuracy: 0.953125 Loss: 0.0018191738054156303\n",
      "Iteration: 4470 Training Accuracy: 0.953125 Loss: 0.0022990447469055653\n",
      "Iteration: 4480 Training Accuracy: 0.921875 Loss: 0.003731462173163891\n",
      "Iteration: 4490 Training Accuracy: 0.9375 Loss: 0.0025345683097839355\n",
      "Iteration: 4500 Training Accuracy: 0.90625 Loss: 0.004479645751416683\n",
      "Iteration: 4510 Training Accuracy: 0.921875 Loss: 0.002995595568791032\n",
      "Iteration: 4520 Training Accuracy: 0.921875 Loss: 0.0037988023832440376\n",
      "Iteration: 4530 Training Accuracy: 0.953125 Loss: 0.0024778256192803383\n",
      "Iteration: 4540 Training Accuracy: 0.96875 Loss: 0.0027053290978074074\n",
      "Iteration: 4550 Training Accuracy: 0.921875 Loss: 0.0033826970029622316\n",
      "Iteration: 4560 Training Accuracy: 0.890625 Loss: 0.004365686792880297\n",
      "Iteration: 4570 Training Accuracy: 0.96875 Loss: 0.0018669829005375504\n",
      "Iteration: 4580 Training Accuracy: 0.96875 Loss: 0.00203520804643631\n",
      "Iteration: 4590 Training Accuracy: 0.953125 Loss: 0.0021284320391714573\n",
      "Iteration: 4600 Training Accuracy: 0.921875 Loss: 0.0034412641543895006\n",
      "Iteration: 4610 Training Accuracy: 0.953125 Loss: 0.0033619869500398636\n",
      "Iteration: 4620 Training Accuracy: 0.9375 Loss: 0.002218123758211732\n",
      "Iteration: 4630 Training Accuracy: 0.953125 Loss: 0.0030238283798098564\n",
      "Iteration: 4640 Training Accuracy: 0.9375 Loss: 0.0038311530370265245\n",
      "Iteration: 4650 Training Accuracy: 0.953125 Loss: 0.0030875527299940586\n",
      "Iteration: 4660 Training Accuracy: 0.96875 Loss: 0.002013585064560175\n",
      "Iteration: 4670 Training Accuracy: 0.9375 Loss: 0.003411277197301388\n",
      "Iteration: 4680 Training Accuracy: 0.9375 Loss: 0.0030996263958513737\n",
      "Iteration: 4690 Training Accuracy: 1.0 Loss: 0.0018108884105458856\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.8815\n",
      "epoch: 5\n",
      "Iteration: 4700 Training Accuracy: 0.890625 Loss: 0.004436144605278969\n",
      "Iteration: 4710 Training Accuracy: 0.953125 Loss: 0.0027166414074599743\n",
      "Iteration: 4720 Training Accuracy: 0.96875 Loss: 0.0016223620623350143\n",
      "Iteration: 4730 Training Accuracy: 0.96875 Loss: 0.0024250720161944628\n",
      "Iteration: 4740 Training Accuracy: 0.890625 Loss: 0.00384677411057055\n",
      "Iteration: 4750 Training Accuracy: 0.875 Loss: 0.007504800334572792\n",
      "Iteration: 4760 Training Accuracy: 0.890625 Loss: 0.004389022011309862\n",
      "Iteration: 4770 Training Accuracy: 0.890625 Loss: 0.004274608101695776\n",
      "Iteration: 4780 Training Accuracy: 0.9375 Loss: 0.003855200484395027\n",
      "Iteration: 4790 Training Accuracy: 0.90625 Loss: 0.0032757679000496864\n",
      "Iteration: 4800 Training Accuracy: 0.90625 Loss: 0.005050293169915676\n",
      "Iteration: 4810 Training Accuracy: 0.96875 Loss: 0.0012835079105570912\n",
      "Iteration: 4820 Training Accuracy: 0.90625 Loss: 0.004633886739611626\n",
      "Iteration: 4830 Training Accuracy: 0.890625 Loss: 0.006238740868866444\n",
      "Iteration: 4840 Training Accuracy: 0.984375 Loss: 0.0013459119945764542\n",
      "Iteration: 4850 Training Accuracy: 0.90625 Loss: 0.004257374443113804\n",
      "Iteration: 4860 Training Accuracy: 0.90625 Loss: 0.0043937088921666145\n",
      "Iteration: 4870 Training Accuracy: 0.890625 Loss: 0.003922002390027046\n",
      "Iteration: 4880 Training Accuracy: 0.953125 Loss: 0.002143728081136942\n",
      "Iteration: 4890 Training Accuracy: 0.921875 Loss: 0.0036894110962748528\n",
      "Iteration: 4900 Training Accuracy: 0.953125 Loss: 0.0035589844919741154\n",
      "Iteration: 4910 Training Accuracy: 0.921875 Loss: 0.0037719211541116238\n",
      "Iteration: 4920 Training Accuracy: 0.921875 Loss: 0.003447512863203883\n",
      "Iteration: 4930 Training Accuracy: 0.921875 Loss: 0.0039599561132490635\n",
      "Iteration: 4940 Training Accuracy: 0.953125 Loss: 0.002428473439067602\n",
      "Iteration: 4950 Training Accuracy: 0.9375 Loss: 0.002288019983097911\n",
      "Iteration: 4960 Training Accuracy: 0.9375 Loss: 0.003736613318324089\n",
      "Iteration: 4970 Training Accuracy: 0.90625 Loss: 0.0036030092742294073\n",
      "Iteration: 4980 Training Accuracy: 0.90625 Loss: 0.003370208665728569\n",
      "Iteration: 4990 Training Accuracy: 0.875 Loss: 0.004680016543716192\n",
      "Iteration: 5000 Training Accuracy: 0.9375 Loss: 0.0045356228947639465\n",
      "Iteration: 5010 Training Accuracy: 0.9375 Loss: 0.0032147071324288845\n",
      "Iteration: 5020 Training Accuracy: 0.9375 Loss: 0.0030387425795197487\n",
      "Iteration: 5030 Training Accuracy: 0.9375 Loss: 0.001960652880370617\n",
      "Iteration: 5040 Training Accuracy: 0.96875 Loss: 0.0024391724728047848\n",
      "Iteration: 5050 Training Accuracy: 0.953125 Loss: 0.003354465588927269\n",
      "Iteration: 5060 Training Accuracy: 0.890625 Loss: 0.004852692596614361\n",
      "Iteration: 5070 Training Accuracy: 0.90625 Loss: 0.0038689863868057728\n",
      "Iteration: 5080 Training Accuracy: 0.9375 Loss: 0.002967229811474681\n",
      "Iteration: 5090 Training Accuracy: 0.953125 Loss: 0.002083537867292762\n",
      "Iteration: 5100 Training Accuracy: 0.90625 Loss: 0.0037938596215099096\n",
      "Iteration: 5110 Training Accuracy: 0.96875 Loss: 0.002190842991694808\n",
      "Iteration: 5120 Training Accuracy: 0.921875 Loss: 0.002562924288213253\n",
      "Iteration: 5130 Training Accuracy: 0.921875 Loss: 0.003946519456803799\n",
      "Iteration: 5140 Training Accuracy: 0.875 Loss: 0.005023673176765442\n",
      "Iteration: 5150 Training Accuracy: 0.9375 Loss: 0.002461059018969536\n",
      "Iteration: 5160 Training Accuracy: 0.890625 Loss: 0.0038379854522645473\n",
      "Iteration: 5170 Training Accuracy: 0.9375 Loss: 0.0027628648094832897\n",
      "Iteration: 5180 Training Accuracy: 0.921875 Loss: 0.003412066027522087\n",
      "Iteration: 5190 Training Accuracy: 0.9375 Loss: 0.0036510168574750423\n",
      "Iteration: 5200 Training Accuracy: 0.953125 Loss: 0.0023528090678155422\n",
      "Iteration: 5210 Training Accuracy: 0.953125 Loss: 0.002662253100425005\n",
      "Iteration: 5220 Training Accuracy: 0.984375 Loss: 0.0007016611052677035\n",
      "Iteration: 5230 Training Accuracy: 0.9375 Loss: 0.0033044815063476562\n",
      "Iteration: 5240 Training Accuracy: 0.96875 Loss: 0.002601343672722578\n",
      "Iteration: 5250 Training Accuracy: 0.921875 Loss: 0.0036693508736789227\n",
      "Iteration: 5260 Training Accuracy: 0.9375 Loss: 0.004686273634433746\n",
      "Iteration: 5270 Training Accuracy: 0.90625 Loss: 0.0039729462005198\n",
      "Iteration: 5280 Training Accuracy: 0.9375 Loss: 0.002393166534602642\n",
      "Iteration: 5290 Training Accuracy: 0.921875 Loss: 0.003341721836477518\n",
      "Iteration: 5300 Training Accuracy: 0.953125 Loss: 0.0023258267901837826\n",
      "Iteration: 5310 Training Accuracy: 0.90625 Loss: 0.003553553484380245\n",
      "Iteration: 5320 Training Accuracy: 1.0 Loss: 0.001874434994533658\n",
      "Iteration: 5330 Training Accuracy: 0.9375 Loss: 0.00300972955301404\n",
      "Iteration: 5340 Training Accuracy: 0.984375 Loss: 0.0011396027402952313\n",
      "Iteration: 5350 Training Accuracy: 0.90625 Loss: 0.0031685982830822468\n",
      "Iteration: 5360 Training Accuracy: 0.953125 Loss: 0.0023121198173612356\n",
      "Iteration: 5370 Training Accuracy: 0.921875 Loss: 0.0034230942837893963\n",
      "Iteration: 5380 Training Accuracy: 0.921875 Loss: 0.0024306662380695343\n",
      "Iteration: 5390 Training Accuracy: 0.96875 Loss: 0.0019584971014410257\n",
      "Iteration: 5400 Training Accuracy: 0.96875 Loss: 0.000958816846832633\n",
      "Iteration: 5410 Training Accuracy: 0.96875 Loss: 0.0016263998113572598\n",
      "Iteration: 5420 Training Accuracy: 0.90625 Loss: 0.003235030919313431\n",
      "Iteration: 5430 Training Accuracy: 0.984375 Loss: 0.0013191781472414732\n",
      "Iteration: 5440 Training Accuracy: 0.90625 Loss: 0.004030198790132999\n",
      "Iteration: 5450 Training Accuracy: 0.9375 Loss: 0.0035522598773241043\n",
      "Iteration: 5460 Training Accuracy: 0.921875 Loss: 0.003380686277523637\n",
      "Iteration: 5470 Training Accuracy: 0.890625 Loss: 0.005514613818377256\n",
      "Iteration: 5480 Training Accuracy: 0.96875 Loss: 0.0018381592817604542\n",
      "Iteration: 5490 Training Accuracy: 0.859375 Loss: 0.0059417723678052425\n",
      "Iteration: 5500 Training Accuracy: 0.875 Loss: 0.004537587519735098\n",
      "Iteration: 5510 Training Accuracy: 0.890625 Loss: 0.003628282807767391\n",
      "Iteration: 5520 Training Accuracy: 0.859375 Loss: 0.008509482257068157\n",
      "Iteration: 5530 Training Accuracy: 0.9375 Loss: 0.003230183618143201\n",
      "Iteration: 5540 Training Accuracy: 0.984375 Loss: 0.0017141876742243767\n",
      "Iteration: 5550 Training Accuracy: 1.0 Loss: 0.0010107639245688915\n",
      "Iteration: 5560 Training Accuracy: 1.0 Loss: 0.0011146250180900097\n",
      "Iteration: 5570 Training Accuracy: 0.9375 Loss: 0.003250107169151306\n",
      "Iteration: 5580 Training Accuracy: 0.953125 Loss: 0.0023247133940458298\n",
      "Iteration: 5590 Training Accuracy: 0.953125 Loss: 0.0025029804091900587\n",
      "Iteration: 5600 Training Accuracy: 0.953125 Loss: 0.0029089408926665783\n",
      "Iteration: 5610 Training Accuracy: 0.953125 Loss: 0.002271858509629965\n",
      "Iteration: 5620 Training Accuracy: 0.984375 Loss: 0.0015871550422161818\n",
      "Training Accuracy = 0.9375\n",
      "Validation Accuracy = 0.8896666666666667\n",
      "epoch: 6\n",
      "Iteration: 5630 Training Accuracy: 0.890625 Loss: 0.004378116689622402\n",
      "Iteration: 5640 Training Accuracy: 0.921875 Loss: 0.0030147219076752663\n",
      "Iteration: 5650 Training Accuracy: 0.90625 Loss: 0.004191770683974028\n",
      "Iteration: 5660 Training Accuracy: 0.90625 Loss: 0.003384172450751066\n",
      "Iteration: 5670 Training Accuracy: 0.890625 Loss: 0.004512104671448469\n",
      "Iteration: 5680 Training Accuracy: 0.921875 Loss: 0.003589790314435959\n",
      "Iteration: 5690 Training Accuracy: 0.96875 Loss: 0.0010525082470849156\n",
      "Iteration: 5700 Training Accuracy: 0.953125 Loss: 0.0025122908409684896\n",
      "Iteration: 5710 Training Accuracy: 0.96875 Loss: 0.003050522180274129\n",
      "Iteration: 5720 Training Accuracy: 0.953125 Loss: 0.0024290350265800953\n",
      "Iteration: 5730 Training Accuracy: 0.90625 Loss: 0.00604533776640892\n",
      "Iteration: 5740 Training Accuracy: 0.90625 Loss: 0.004473648965358734\n",
      "Iteration: 5750 Training Accuracy: 0.9375 Loss: 0.0035853716544806957\n",
      "Iteration: 5760 Training Accuracy: 0.890625 Loss: 0.005251470021903515\n",
      "Iteration: 5770 Training Accuracy: 0.953125 Loss: 0.0018946246709674597\n",
      "Iteration: 5780 Training Accuracy: 0.9375 Loss: 0.00360311521217227\n",
      "Iteration: 5790 Training Accuracy: 0.9375 Loss: 0.004487052094191313\n",
      "Iteration: 5800 Training Accuracy: 0.953125 Loss: 0.001968706026673317\n",
      "Iteration: 5810 Training Accuracy: 0.96875 Loss: 0.0031541278585791588\n",
      "Iteration: 5820 Training Accuracy: 0.90625 Loss: 0.00335752428509295\n",
      "Iteration: 5830 Training Accuracy: 1.0 Loss: 0.0006836270913481712\n",
      "Iteration: 5840 Training Accuracy: 0.96875 Loss: 0.0018267406849190593\n",
      "Iteration: 5850 Training Accuracy: 0.9375 Loss: 0.002948152832686901\n",
      "Iteration: 5860 Training Accuracy: 0.953125 Loss: 0.0020413731690496206\n",
      "Iteration: 5870 Training Accuracy: 1.0 Loss: 0.0006850914796814322\n",
      "Iteration: 5880 Training Accuracy: 0.9375 Loss: 0.002234018873423338\n",
      "Iteration: 5890 Training Accuracy: 0.921875 Loss: 0.003794949734583497\n",
      "Iteration: 5900 Training Accuracy: 0.921875 Loss: 0.0036092596128582954\n",
      "Iteration: 5910 Training Accuracy: 0.890625 Loss: 0.00550477672368288\n",
      "Iteration: 5920 Training Accuracy: 0.90625 Loss: 0.0035501723177731037\n",
      "Iteration: 5930 Training Accuracy: 0.96875 Loss: 0.0018228909466415644\n",
      "Iteration: 5940 Training Accuracy: 0.875 Loss: 0.00521176727488637\n",
      "Iteration: 5950 Training Accuracy: 0.90625 Loss: 0.004491948522627354\n",
      "Iteration: 5960 Training Accuracy: 0.90625 Loss: 0.0034806113690137863\n",
      "Iteration: 5970 Training Accuracy: 0.9375 Loss: 0.0025009538512676954\n",
      "Iteration: 5980 Training Accuracy: 0.96875 Loss: 0.002734947483986616\n",
      "Iteration: 5990 Training Accuracy: 0.984375 Loss: 0.0015370988985523582\n",
      "Iteration: 6000 Training Accuracy: 0.953125 Loss: 0.0019843755289912224\n",
      "Iteration: 6010 Training Accuracy: 0.9375 Loss: 0.0031461776234209538\n",
      "Iteration: 6020 Training Accuracy: 0.984375 Loss: 0.00155809021089226\n",
      "Iteration: 6030 Training Accuracy: 0.890625 Loss: 0.00491830799728632\n",
      "Iteration: 6040 Training Accuracy: 0.875 Loss: 0.00554219726473093\n",
      "Iteration: 6050 Training Accuracy: 0.90625 Loss: 0.003117492189630866\n",
      "Iteration: 6060 Training Accuracy: 0.953125 Loss: 0.0030199517495930195\n",
      "Iteration: 6070 Training Accuracy: 0.9375 Loss: 0.0029836627654731274\n",
      "Iteration: 6080 Training Accuracy: 0.890625 Loss: 0.0042725736275315285\n",
      "Iteration: 6090 Training Accuracy: 0.921875 Loss: 0.0034035881981253624\n",
      "Iteration: 6100 Training Accuracy: 0.953125 Loss: 0.0020590019412338734\n",
      "Iteration: 6110 Training Accuracy: 0.96875 Loss: 0.0017955329967662692\n",
      "Iteration: 6120 Training Accuracy: 0.953125 Loss: 0.0017221603775396943\n",
      "Iteration: 6130 Training Accuracy: 0.953125 Loss: 0.0027233550790697336\n",
      "Iteration: 6140 Training Accuracy: 0.90625 Loss: 0.003658740548416972\n",
      "Iteration: 6150 Training Accuracy: 0.953125 Loss: 0.0020374772138893604\n",
      "Iteration: 6160 Training Accuracy: 0.890625 Loss: 0.0036468091420829296\n",
      "Iteration: 6170 Training Accuracy: 0.984375 Loss: 0.000885111978277564\n",
      "Iteration: 6180 Training Accuracy: 0.9375 Loss: 0.0025434414856135845\n",
      "Iteration: 6190 Training Accuracy: 0.921875 Loss: 0.0028160521760582924\n",
      "Iteration: 6200 Training Accuracy: 0.9375 Loss: 0.002465052530169487\n",
      "Iteration: 6210 Training Accuracy: 0.921875 Loss: 0.0031449547968804836\n",
      "Iteration: 6220 Training Accuracy: 0.953125 Loss: 0.0022061970084905624\n",
      "Iteration: 6230 Training Accuracy: 0.96875 Loss: 0.002769453451037407\n",
      "Iteration: 6240 Training Accuracy: 0.90625 Loss: 0.003934513311833143\n",
      "Iteration: 6250 Training Accuracy: 0.96875 Loss: 0.00232367473654449\n",
      "Iteration: 6260 Training Accuracy: 0.96875 Loss: 0.001975895371288061\n",
      "Iteration: 6270 Training Accuracy: 0.96875 Loss: 0.0012221725191920996\n",
      "Iteration: 6280 Training Accuracy: 0.953125 Loss: 0.0018635615706443787\n",
      "Iteration: 6290 Training Accuracy: 0.984375 Loss: 0.001044567790813744\n",
      "Iteration: 6300 Training Accuracy: 0.90625 Loss: 0.006391823757439852\n",
      "Iteration: 6310 Training Accuracy: 0.921875 Loss: 0.0028655431233346462\n",
      "Iteration: 6320 Training Accuracy: 0.953125 Loss: 0.00137224025093019\n",
      "Iteration: 6330 Training Accuracy: 0.96875 Loss: 0.0019556316547095776\n",
      "Iteration: 6340 Training Accuracy: 0.984375 Loss: 0.001611360814422369\n",
      "Iteration: 6350 Training Accuracy: 0.9375 Loss: 0.0032604606822133064\n",
      "Iteration: 6360 Training Accuracy: 0.96875 Loss: 0.0034365204628556967\n",
      "Iteration: 6370 Training Accuracy: 0.96875 Loss: 0.001366682699881494\n",
      "Iteration: 6380 Training Accuracy: 0.96875 Loss: 0.0021200552582740784\n",
      "Iteration: 6390 Training Accuracy: 0.953125 Loss: 0.002323960652574897\n",
      "Iteration: 6400 Training Accuracy: 0.9375 Loss: 0.002830240409821272\n",
      "Iteration: 6410 Training Accuracy: 0.96875 Loss: 0.003475173842161894\n",
      "Iteration: 6420 Training Accuracy: 0.921875 Loss: 0.002490689978003502\n",
      "Iteration: 6430 Training Accuracy: 0.921875 Loss: 0.002908693626523018\n",
      "Iteration: 6440 Training Accuracy: 0.921875 Loss: 0.003379836678504944\n",
      "Iteration: 6450 Training Accuracy: 0.953125 Loss: 0.0017756327288225293\n",
      "Iteration: 6460 Training Accuracy: 0.953125 Loss: 0.00415644608438015\n",
      "Iteration: 6470 Training Accuracy: 0.90625 Loss: 0.003645403077825904\n",
      "Iteration: 6480 Training Accuracy: 0.90625 Loss: 0.0048070140182971954\n",
      "Iteration: 6490 Training Accuracy: 0.921875 Loss: 0.002450298983603716\n",
      "Iteration: 6500 Training Accuracy: 0.96875 Loss: 0.0013097546761855483\n",
      "Iteration: 6510 Training Accuracy: 0.9375 Loss: 0.002456170506775379\n",
      "Iteration: 6520 Training Accuracy: 0.96875 Loss: 0.0022202502004802227\n",
      "Iteration: 6530 Training Accuracy: 0.96875 Loss: 0.002310987561941147\n",
      "Iteration: 6540 Training Accuracy: 0.953125 Loss: 0.0027666206005960703\n",
      "Iteration: 6550 Training Accuracy: 0.953125 Loss: 0.0018494738033041358\n",
      "Iteration: 6560 Training Accuracy: 0.953125 Loss: 0.0018281687516719103\n",
      "Training Accuracy = 0.96875\n",
      "Validation Accuracy = 0.8861666666666667\n",
      "epoch: 7\n",
      "Iteration: 6570 Training Accuracy: 0.921875 Loss: 0.0031125633977353573\n",
      "Iteration: 6580 Training Accuracy: 0.984375 Loss: 0.001185820670798421\n",
      "Iteration: 6590 Training Accuracy: 0.9375 Loss: 0.001447631511837244\n",
      "Iteration: 6600 Training Accuracy: 0.953125 Loss: 0.0017636471893638372\n",
      "Iteration: 6610 Training Accuracy: 0.953125 Loss: 0.0015556130092591047\n",
      "Iteration: 6620 Training Accuracy: 0.921875 Loss: 0.00446992227807641\n",
      "Iteration: 6630 Training Accuracy: 0.984375 Loss: 0.0012335206847637892\n",
      "Iteration: 6640 Training Accuracy: 0.890625 Loss: 0.003577963449060917\n",
      "Iteration: 6650 Training Accuracy: 0.96875 Loss: 0.0015366424340754747\n",
      "Iteration: 6660 Training Accuracy: 0.984375 Loss: 0.0005620454321615398\n",
      "Iteration: 6670 Training Accuracy: 0.90625 Loss: 0.002656171564012766\n",
      "Iteration: 6680 Training Accuracy: 0.921875 Loss: 0.0027736909687519073\n",
      "Iteration: 6690 Training Accuracy: 0.953125 Loss: 0.00234282948076725\n",
      "Iteration: 6700 Training Accuracy: 0.9375 Loss: 0.003745963331311941\n",
      "Iteration: 6710 Training Accuracy: 0.96875 Loss: 0.0016400471795350313\n",
      "Iteration: 6720 Training Accuracy: 0.9375 Loss: 0.0025920518673956394\n",
      "Iteration: 6730 Training Accuracy: 0.953125 Loss: 0.002392595401033759\n",
      "Iteration: 6740 Training Accuracy: 1.0 Loss: 0.0008106371387839317\n",
      "Iteration: 6750 Training Accuracy: 0.921875 Loss: 0.0023058925289660692\n",
      "Iteration: 6760 Training Accuracy: 0.96875 Loss: 0.0016744775930419564\n",
      "Iteration: 6770 Training Accuracy: 0.984375 Loss: 0.0005434582708403468\n",
      "Iteration: 6780 Training Accuracy: 0.9375 Loss: 0.0019706827588379383\n",
      "Iteration: 6790 Training Accuracy: 0.984375 Loss: 0.0011884935665875673\n",
      "Iteration: 6800 Training Accuracy: 0.953125 Loss: 0.0029529831372201443\n",
      "Iteration: 6810 Training Accuracy: 0.96875 Loss: 0.001134927966631949\n",
      "Iteration: 6820 Training Accuracy: 0.953125 Loss: 0.002295524114742875\n",
      "Iteration: 6830 Training Accuracy: 0.96875 Loss: 0.0014633992686867714\n",
      "Iteration: 6840 Training Accuracy: 0.953125 Loss: 0.0015506148338317871\n",
      "Iteration: 6850 Training Accuracy: 0.984375 Loss: 0.0029925683047622442\n",
      "Iteration: 6860 Training Accuracy: 0.953125 Loss: 0.002102793427184224\n",
      "Iteration: 6870 Training Accuracy: 0.984375 Loss: 0.0008070941548794508\n",
      "Iteration: 6880 Training Accuracy: 0.96875 Loss: 0.0017293733544647694\n",
      "Iteration: 6890 Training Accuracy: 0.96875 Loss: 0.002157393144443631\n",
      "Iteration: 6900 Training Accuracy: 0.96875 Loss: 0.0023274444974958897\n",
      "Iteration: 6910 Training Accuracy: 0.9375 Loss: 0.002141769975423813\n",
      "Iteration: 6920 Training Accuracy: 1.0 Loss: 0.0005706195952370763\n",
      "Iteration: 6930 Training Accuracy: 0.984375 Loss: 0.001652382081374526\n",
      "Iteration: 6940 Training Accuracy: 0.921875 Loss: 0.0023649833165109158\n",
      "Iteration: 6950 Training Accuracy: 0.921875 Loss: 0.0034151894506067038\n",
      "Iteration: 6960 Training Accuracy: 0.984375 Loss: 0.0011054741917178035\n",
      "Iteration: 6970 Training Accuracy: 0.984375 Loss: 0.0009527005604468286\n",
      "Iteration: 6980 Training Accuracy: 0.953125 Loss: 0.00182407780084759\n",
      "Iteration: 6990 Training Accuracy: 0.96875 Loss: 0.0015106186037883162\n",
      "Iteration: 7000 Training Accuracy: 0.953125 Loss: 0.0016450413968414068\n",
      "Iteration: 7010 Training Accuracy: 0.9375 Loss: 0.004044000990688801\n",
      "Iteration: 7020 Training Accuracy: 0.984375 Loss: 0.0011683334596455097\n",
      "Iteration: 7030 Training Accuracy: 0.953125 Loss: 0.0019310575444251299\n",
      "Iteration: 7040 Training Accuracy: 0.96875 Loss: 0.0008288126555271447\n",
      "Iteration: 7050 Training Accuracy: 0.953125 Loss: 0.0016946530668064952\n",
      "Iteration: 7060 Training Accuracy: 0.953125 Loss: 0.002359088510274887\n",
      "Iteration: 7070 Training Accuracy: 0.953125 Loss: 0.0034796935506165028\n",
      "Iteration: 7080 Training Accuracy: 0.984375 Loss: 0.001352150458842516\n",
      "Iteration: 7090 Training Accuracy: 0.9375 Loss: 0.002424980979412794\n",
      "Iteration: 7100 Training Accuracy: 0.953125 Loss: 0.002471968764439225\n",
      "Iteration: 7110 Training Accuracy: 0.921875 Loss: 0.004456517286598682\n",
      "Iteration: 7120 Training Accuracy: 0.921875 Loss: 0.00338744861073792\n",
      "Iteration: 7130 Training Accuracy: 0.984375 Loss: 0.001656022621318698\n",
      "Iteration: 7140 Training Accuracy: 0.9375 Loss: 0.002068242058157921\n",
      "Iteration: 7150 Training Accuracy: 0.984375 Loss: 0.0006922478205524385\n",
      "Iteration: 7160 Training Accuracy: 0.90625 Loss: 0.002820298308506608\n",
      "Iteration: 7170 Training Accuracy: 0.9375 Loss: 0.0035477993078529835\n",
      "Iteration: 7180 Training Accuracy: 0.96875 Loss: 0.001490927068516612\n",
      "Iteration: 7190 Training Accuracy: 0.984375 Loss: 0.0024517402052879333\n",
      "Iteration: 7200 Training Accuracy: 0.984375 Loss: 0.0011908842716366053\n",
      "Iteration: 7210 Training Accuracy: 1.0 Loss: 0.0006279911613091826\n",
      "Iteration: 7220 Training Accuracy: 0.984375 Loss: 0.0009108585654757917\n",
      "Iteration: 7230 Training Accuracy: 0.96875 Loss: 0.00131085398606956\n",
      "Iteration: 7240 Training Accuracy: 0.984375 Loss: 0.0007567920256406069\n",
      "Iteration: 7250 Training Accuracy: 1.0 Loss: 0.000758479000069201\n",
      "Iteration: 7260 Training Accuracy: 0.984375 Loss: 0.0015807088930159807\n",
      "Iteration: 7270 Training Accuracy: 0.953125 Loss: 0.0017690059030428529\n",
      "Iteration: 7280 Training Accuracy: 0.984375 Loss: 0.0006905748159624636\n",
      "Iteration: 7290 Training Accuracy: 0.9375 Loss: 0.0022681746631860733\n",
      "Iteration: 7300 Training Accuracy: 1.0 Loss: 0.00051355198957026\n",
      "Iteration: 7310 Training Accuracy: 0.9375 Loss: 0.002505631884559989\n",
      "Iteration: 7320 Training Accuracy: 0.984375 Loss: 0.0015825977316126227\n",
      "Iteration: 7330 Training Accuracy: 0.96875 Loss: 0.002042451174929738\n",
      "Iteration: 7340 Training Accuracy: 0.984375 Loss: 0.0014032748294994235\n",
      "Iteration: 7350 Training Accuracy: 0.953125 Loss: 0.0017646304331719875\n",
      "Iteration: 7360 Training Accuracy: 0.96875 Loss: 0.0012093349359929562\n",
      "Iteration: 7370 Training Accuracy: 0.9375 Loss: 0.0017600737046450377\n",
      "Iteration: 7380 Training Accuracy: 0.96875 Loss: 0.0015449027996510267\n",
      "Iteration: 7390 Training Accuracy: 0.96875 Loss: 0.0014722547493875027\n",
      "Iteration: 7400 Training Accuracy: 0.96875 Loss: 0.0013916355092078447\n",
      "Iteration: 7410 Training Accuracy: 0.953125 Loss: 0.0027177976444363594\n",
      "Iteration: 7420 Training Accuracy: 0.96875 Loss: 0.0016767564229667187\n",
      "Iteration: 7430 Training Accuracy: 0.984375 Loss: 0.0008642206666991115\n",
      "Iteration: 7440 Training Accuracy: 0.984375 Loss: 0.001285388134419918\n",
      "Iteration: 7450 Training Accuracy: 0.953125 Loss: 0.0014915086794644594\n",
      "Iteration: 7460 Training Accuracy: 0.96875 Loss: 0.0019189546583220363\n",
      "Iteration: 7470 Training Accuracy: 0.96875 Loss: 0.0012382382992655039\n",
      "Iteration: 7480 Training Accuracy: 0.984375 Loss: 0.0011880086967721581\n",
      "Iteration: 7490 Training Accuracy: 0.953125 Loss: 0.0015221151988953352\n",
      "Iteration: 7500 Training Accuracy: 0.984375 Loss: 0.0006130968104116619\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9151666666666667\n",
      "epoch: 8\n",
      "Iteration: 7510 Training Accuracy: 0.953125 Loss: 0.0015468832571059465\n",
      "Iteration: 7520 Training Accuracy: 0.9375 Loss: 0.001974293729290366\n",
      "Iteration: 7530 Training Accuracy: 1.0 Loss: 0.000851362943649292\n",
      "Iteration: 7540 Training Accuracy: 0.96875 Loss: 0.0016123943496495485\n",
      "Iteration: 7550 Training Accuracy: 0.953125 Loss: 0.0013556353515014052\n",
      "Iteration: 7560 Training Accuracy: 0.953125 Loss: 0.001495815347880125\n",
      "Iteration: 7570 Training Accuracy: 0.953125 Loss: 0.0030635171569883823\n",
      "Iteration: 7580 Training Accuracy: 0.9375 Loss: 0.003087742254137993\n",
      "Iteration: 7590 Training Accuracy: 0.984375 Loss: 0.0012403525179252028\n",
      "Iteration: 7600 Training Accuracy: 0.96875 Loss: 0.0020015104673802853\n",
      "Iteration: 7610 Training Accuracy: 0.96875 Loss: 0.0015943408943712711\n",
      "Iteration: 7620 Training Accuracy: 0.984375 Loss: 0.001268528401851654\n",
      "Iteration: 7630 Training Accuracy: 0.984375 Loss: 0.001433854689821601\n",
      "Iteration: 7640 Training Accuracy: 0.984375 Loss: 0.0008950852206908166\n",
      "Iteration: 7650 Training Accuracy: 0.984375 Loss: 0.0009262771345674992\n",
      "Iteration: 7660 Training Accuracy: 0.953125 Loss: 0.00362063548527658\n",
      "Iteration: 7670 Training Accuracy: 0.96875 Loss: 0.0009013038943521678\n",
      "Iteration: 7680 Training Accuracy: 0.984375 Loss: 0.001168460352346301\n",
      "Iteration: 7690 Training Accuracy: 1.0 Loss: 0.0005647925427183509\n",
      "Iteration: 7700 Training Accuracy: 0.984375 Loss: 0.0005990155623294413\n",
      "Iteration: 7710 Training Accuracy: 0.96875 Loss: 0.0012463934253901243\n",
      "Iteration: 7720 Training Accuracy: 0.9375 Loss: 0.004270507488399744\n",
      "Iteration: 7730 Training Accuracy: 0.96875 Loss: 0.0008686833316460252\n",
      "Iteration: 7740 Training Accuracy: 0.96875 Loss: 0.001047542435117066\n",
      "Iteration: 7750 Training Accuracy: 1.0 Loss: 0.0011763566872105002\n",
      "Iteration: 7760 Training Accuracy: 0.96875 Loss: 0.0011997115798294544\n",
      "Iteration: 7770 Training Accuracy: 0.96875 Loss: 0.002085246844217181\n",
      "Iteration: 7780 Training Accuracy: 0.96875 Loss: 0.0014579123817384243\n",
      "Iteration: 7790 Training Accuracy: 1.0 Loss: 0.0005705725634470582\n",
      "Iteration: 7800 Training Accuracy: 0.96875 Loss: 0.001497892546467483\n",
      "Iteration: 7810 Training Accuracy: 0.984375 Loss: 0.00044756638817489147\n",
      "Iteration: 7820 Training Accuracy: 0.96875 Loss: 0.0014509708853438497\n",
      "Iteration: 7830 Training Accuracy: 0.984375 Loss: 0.0006310744793154299\n",
      "Iteration: 7840 Training Accuracy: 0.96875 Loss: 0.0021095029078423977\n",
      "Iteration: 7850 Training Accuracy: 0.984375 Loss: 0.0010383175686001778\n",
      "Iteration: 7860 Training Accuracy: 0.984375 Loss: 0.0010869251564145088\n",
      "Iteration: 7870 Training Accuracy: 0.96875 Loss: 0.0010757041163742542\n",
      "Iteration: 7880 Training Accuracy: 0.984375 Loss: 0.0010477559408172965\n",
      "Iteration: 7890 Training Accuracy: 0.9375 Loss: 0.002913797041401267\n",
      "Iteration: 7900 Training Accuracy: 0.96875 Loss: 0.0014884722186252475\n",
      "Iteration: 7910 Training Accuracy: 0.984375 Loss: 0.0007090161088854074\n",
      "Iteration: 7920 Training Accuracy: 1.0 Loss: 0.0011574483942240477\n",
      "Iteration: 7930 Training Accuracy: 0.984375 Loss: 0.0009163008071482182\n",
      "Iteration: 7940 Training Accuracy: 0.953125 Loss: 0.0018653827719390392\n",
      "Iteration: 7950 Training Accuracy: 0.96875 Loss: 0.0018133539706468582\n",
      "Iteration: 7960 Training Accuracy: 0.984375 Loss: 0.0010920681525021791\n",
      "Iteration: 7970 Training Accuracy: 0.9375 Loss: 0.0021843742579221725\n",
      "Iteration: 7980 Training Accuracy: 0.953125 Loss: 0.0022917543537914753\n",
      "Iteration: 7990 Training Accuracy: 0.96875 Loss: 0.001250331406481564\n",
      "Iteration: 8000 Training Accuracy: 0.984375 Loss: 0.0008230702951550484\n",
      "Iteration: 8010 Training Accuracy: 0.984375 Loss: 0.0011363370576873422\n",
      "Iteration: 8020 Training Accuracy: 0.984375 Loss: 0.0015901138540357351\n",
      "Iteration: 8030 Training Accuracy: 0.953125 Loss: 0.0013920932542532682\n",
      "Iteration: 8040 Training Accuracy: 1.0 Loss: 0.0007593806367367506\n",
      "Iteration: 8050 Training Accuracy: 0.984375 Loss: 0.0014849224826321006\n",
      "Iteration: 8060 Training Accuracy: 1.0 Loss: 0.0005174053367227316\n",
      "Iteration: 8070 Training Accuracy: 0.984375 Loss: 0.0005978897679597139\n",
      "Iteration: 8080 Training Accuracy: 0.96875 Loss: 0.0016372293466702104\n",
      "Iteration: 8090 Training Accuracy: 0.96875 Loss: 0.001669512246735394\n",
      "Iteration: 8100 Training Accuracy: 0.984375 Loss: 0.0014551208587363362\n",
      "Iteration: 8110 Training Accuracy: 0.96875 Loss: 0.0019379036966711283\n",
      "Iteration: 8120 Training Accuracy: 0.984375 Loss: 0.0006620544590987265\n",
      "Iteration: 8130 Training Accuracy: 0.953125 Loss: 0.002016779500991106\n",
      "Iteration: 8140 Training Accuracy: 0.984375 Loss: 0.0008775020251050591\n",
      "Iteration: 8150 Training Accuracy: 0.953125 Loss: 0.0018337025539949536\n",
      "Iteration: 8160 Training Accuracy: 1.0 Loss: 0.0006897667772136629\n",
      "Iteration: 8170 Training Accuracy: 0.984375 Loss: 0.0007317273993976414\n",
      "Iteration: 8180 Training Accuracy: 1.0 Loss: 0.001100613852031529\n",
      "Iteration: 8190 Training Accuracy: 0.96875 Loss: 0.0017488988814875484\n",
      "Iteration: 8200 Training Accuracy: 1.0 Loss: 0.0008976035751402378\n",
      "Iteration: 8210 Training Accuracy: 0.984375 Loss: 0.0008022807887755334\n",
      "Iteration: 8220 Training Accuracy: 0.96875 Loss: 0.002266530878841877\n",
      "Iteration: 8230 Training Accuracy: 1.0 Loss: 0.0005956207751296461\n",
      "Iteration: 8240 Training Accuracy: 0.953125 Loss: 0.00192523212172091\n",
      "Iteration: 8250 Training Accuracy: 0.984375 Loss: 0.001673658611252904\n",
      "Iteration: 8260 Training Accuracy: 1.0 Loss: 0.0009726932039484382\n",
      "Iteration: 8270 Training Accuracy: 1.0 Loss: 0.0012710418086498976\n",
      "Iteration: 8280 Training Accuracy: 0.9375 Loss: 0.0019687802996486425\n",
      "Iteration: 8290 Training Accuracy: 1.0 Loss: 0.0009724635747261345\n",
      "Iteration: 8300 Training Accuracy: 0.984375 Loss: 0.0008135294774547219\n",
      "Iteration: 8310 Training Accuracy: 0.96875 Loss: 0.0014816904440522194\n",
      "Iteration: 8320 Training Accuracy: 0.96875 Loss: 0.0015333557967096567\n",
      "Iteration: 8330 Training Accuracy: 0.9375 Loss: 0.002385735744610429\n",
      "Iteration: 8340 Training Accuracy: 1.0 Loss: 0.0007752098608762026\n",
      "Iteration: 8350 Training Accuracy: 1.0 Loss: 0.0005700555630028248\n",
      "Iteration: 8360 Training Accuracy: 0.953125 Loss: 0.0020337295718491077\n",
      "Iteration: 8370 Training Accuracy: 1.0 Loss: 0.000898637343198061\n",
      "Iteration: 8380 Training Accuracy: 0.984375 Loss: 0.0010092833545058966\n",
      "Iteration: 8390 Training Accuracy: 0.9375 Loss: 0.0028420081362128258\n",
      "Iteration: 8400 Training Accuracy: 1.0 Loss: 0.0007708938210271299\n",
      "Iteration: 8410 Training Accuracy: 1.0 Loss: 0.0008500953554175794\n",
      "Iteration: 8420 Training Accuracy: 0.96875 Loss: 0.0015710368752479553\n",
      "Iteration: 8430 Training Accuracy: 0.9375 Loss: 0.0028886478394269943\n",
      "Iteration: 8440 Training Accuracy: 0.984375 Loss: 0.0007847687811590731\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9163333333333333\n",
      "epoch: 9\n",
      "Iteration: 8450 Training Accuracy: 0.984375 Loss: 0.0006377142271958292\n",
      "Iteration: 8460 Training Accuracy: 0.984375 Loss: 0.0006622992223128676\n",
      "Iteration: 8470 Training Accuracy: 0.953125 Loss: 0.0020204544998705387\n",
      "Iteration: 8480 Training Accuracy: 0.984375 Loss: 0.0008105498855002224\n",
      "Iteration: 8490 Training Accuracy: 0.96875 Loss: 0.0019465566147118807\n",
      "Iteration: 8500 Training Accuracy: 0.984375 Loss: 0.0012998513411730528\n",
      "Iteration: 8510 Training Accuracy: 0.953125 Loss: 0.001771664246916771\n",
      "Iteration: 8520 Training Accuracy: 0.96875 Loss: 0.0018345138523727655\n",
      "Iteration: 8530 Training Accuracy: 0.984375 Loss: 0.0014370951103046536\n",
      "Iteration: 8540 Training Accuracy: 0.96875 Loss: 0.0018824515864253044\n",
      "Iteration: 8550 Training Accuracy: 1.0 Loss: 0.00045806163689121604\n",
      "Iteration: 8560 Training Accuracy: 0.9375 Loss: 0.0031074052676558495\n",
      "Iteration: 8570 Training Accuracy: 0.953125 Loss: 0.0022401949390769005\n",
      "Iteration: 8580 Training Accuracy: 0.984375 Loss: 0.0013187656877562404\n",
      "Iteration: 8590 Training Accuracy: 0.9375 Loss: 0.0032389555126428604\n",
      "Iteration: 8600 Training Accuracy: 0.96875 Loss: 0.0017524121794849634\n",
      "Iteration: 8610 Training Accuracy: 0.984375 Loss: 0.001545565202832222\n",
      "Iteration: 8620 Training Accuracy: 1.0 Loss: 0.0006319316453300416\n",
      "Iteration: 8630 Training Accuracy: 0.984375 Loss: 0.0013177187647670507\n",
      "Iteration: 8640 Training Accuracy: 0.984375 Loss: 0.00039081007707864046\n",
      "Iteration: 8650 Training Accuracy: 0.984375 Loss: 0.0006897533312439919\n",
      "Iteration: 8660 Training Accuracy: 0.984375 Loss: 0.0011075763031840324\n",
      "Iteration: 8670 Training Accuracy: 0.953125 Loss: 0.0018081943271681666\n",
      "Iteration: 8680 Training Accuracy: 0.953125 Loss: 0.0015934868715703487\n",
      "Iteration: 8690 Training Accuracy: 0.953125 Loss: 0.0019046701490879059\n",
      "Iteration: 8700 Training Accuracy: 0.96875 Loss: 0.0014811146538704634\n",
      "Iteration: 8710 Training Accuracy: 0.953125 Loss: 0.002341619925573468\n",
      "Iteration: 8720 Training Accuracy: 0.921875 Loss: 0.002631703158840537\n",
      "Iteration: 8730 Training Accuracy: 0.9375 Loss: 0.002291128970682621\n",
      "Iteration: 8740 Training Accuracy: 0.984375 Loss: 0.0011648423969745636\n",
      "Iteration: 8750 Training Accuracy: 0.953125 Loss: 0.002512845443561673\n",
      "Iteration: 8760 Training Accuracy: 0.9375 Loss: 0.0028310916386544704\n",
      "Iteration: 8770 Training Accuracy: 0.96875 Loss: 0.0013263407163321972\n",
      "Iteration: 8780 Training Accuracy: 1.0 Loss: 0.00017339990881737322\n",
      "Iteration: 8790 Training Accuracy: 1.0 Loss: 0.0007035103626549244\n",
      "Iteration: 8800 Training Accuracy: 1.0 Loss: 0.0007169779273681343\n",
      "Iteration: 8810 Training Accuracy: 0.96875 Loss: 0.0012135618599131703\n",
      "Iteration: 8820 Training Accuracy: 0.96875 Loss: 0.0012455207761377096\n",
      "Iteration: 8830 Training Accuracy: 0.9375 Loss: 0.0027765752747654915\n",
      "Iteration: 8840 Training Accuracy: 1.0 Loss: 0.000812410784419626\n",
      "Iteration: 8850 Training Accuracy: 0.96875 Loss: 0.0019202415132895112\n",
      "Iteration: 8860 Training Accuracy: 0.96875 Loss: 0.0014094514772295952\n",
      "Iteration: 8870 Training Accuracy: 0.9375 Loss: 0.002182355150580406\n",
      "Iteration: 8880 Training Accuracy: 0.953125 Loss: 0.0015790673205628991\n",
      "Iteration: 8890 Training Accuracy: 0.9375 Loss: 0.0025080800987780094\n",
      "Iteration: 8900 Training Accuracy: 0.953125 Loss: 0.0017620843136683106\n",
      "Iteration: 8910 Training Accuracy: 1.0 Loss: 0.0003025768673978746\n",
      "Iteration: 8920 Training Accuracy: 0.984375 Loss: 0.0013161448296159506\n",
      "Iteration: 8930 Training Accuracy: 0.984375 Loss: 0.0011795602040365338\n",
      "Iteration: 8940 Training Accuracy: 0.984375 Loss: 0.0007010636618360877\n",
      "Iteration: 8950 Training Accuracy: 1.0 Loss: 0.0006041638553142548\n",
      "Iteration: 8960 Training Accuracy: 0.96875 Loss: 0.0008497607195749879\n",
      "Iteration: 8970 Training Accuracy: 0.984375 Loss: 0.000726266298443079\n",
      "Iteration: 8980 Training Accuracy: 0.984375 Loss: 0.0010194978676736355\n",
      "Iteration: 8990 Training Accuracy: 0.96875 Loss: 0.0017222501337528229\n",
      "Iteration: 9000 Training Accuracy: 0.921875 Loss: 0.0020910047460347414\n",
      "Iteration: 9010 Training Accuracy: 0.96875 Loss: 0.0010308146011084318\n",
      "Iteration: 9020 Training Accuracy: 0.984375 Loss: 0.0018612700514495373\n",
      "Iteration: 9030 Training Accuracy: 0.96875 Loss: 0.0016412774566560984\n",
      "Iteration: 9040 Training Accuracy: 0.984375 Loss: 0.001010265899822116\n",
      "Iteration: 9050 Training Accuracy: 0.984375 Loss: 0.0010690689086914062\n",
      "Iteration: 9060 Training Accuracy: 0.9375 Loss: 0.0021539023146033287\n",
      "Iteration: 9070 Training Accuracy: 0.984375 Loss: 0.0007825433858670294\n",
      "Iteration: 9080 Training Accuracy: 0.96875 Loss: 0.002333954907953739\n",
      "Iteration: 9090 Training Accuracy: 0.984375 Loss: 0.0008815198671072721\n",
      "Iteration: 9100 Training Accuracy: 0.96875 Loss: 0.0021461297292262316\n",
      "Iteration: 9110 Training Accuracy: 0.96875 Loss: 0.0014339287299662828\n",
      "Iteration: 9120 Training Accuracy: 1.0 Loss: 0.0011105949524790049\n",
      "Iteration: 9130 Training Accuracy: 0.96875 Loss: 0.0009926222264766693\n",
      "Iteration: 9140 Training Accuracy: 0.953125 Loss: 0.0020826540421694517\n",
      "Iteration: 9150 Training Accuracy: 1.0 Loss: 0.00035963431582786143\n",
      "Iteration: 9160 Training Accuracy: 0.984375 Loss: 0.0008488085586577654\n",
      "Iteration: 9170 Training Accuracy: 0.953125 Loss: 0.0015846139285713434\n",
      "Iteration: 9180 Training Accuracy: 0.9375 Loss: 0.0018992089899256825\n",
      "Iteration: 9190 Training Accuracy: 0.953125 Loss: 0.002131934044882655\n",
      "Iteration: 9200 Training Accuracy: 1.0 Loss: 0.001168710645288229\n",
      "Iteration: 9210 Training Accuracy: 0.984375 Loss: 0.0009582286584191024\n",
      "Iteration: 9220 Training Accuracy: 1.0 Loss: 0.0006217991467565298\n",
      "Iteration: 9230 Training Accuracy: 0.984375 Loss: 0.0009346746373921633\n",
      "Iteration: 9240 Training Accuracy: 0.984375 Loss: 0.0012812406057491899\n",
      "Iteration: 9250 Training Accuracy: 0.9375 Loss: 0.002475881017744541\n",
      "Iteration: 9260 Training Accuracy: 1.0 Loss: 0.0004917628830298781\n",
      "Iteration: 9270 Training Accuracy: 1.0 Loss: 0.0005805967375636101\n",
      "Iteration: 9280 Training Accuracy: 1.0 Loss: 0.0004319128056522459\n",
      "Iteration: 9290 Training Accuracy: 1.0 Loss: 0.0008515209192410111\n",
      "Iteration: 9300 Training Accuracy: 0.96875 Loss: 0.0011676119174808264\n",
      "Iteration: 9310 Training Accuracy: 1.0 Loss: 0.0008099126862362027\n",
      "Iteration: 9320 Training Accuracy: 0.96875 Loss: 0.0011881402460858226\n",
      "Iteration: 9330 Training Accuracy: 0.953125 Loss: 0.0020016985945403576\n",
      "Iteration: 9340 Training Accuracy: 1.0 Loss: 0.00025040426407940686\n",
      "Iteration: 9350 Training Accuracy: 1.0 Loss: 0.000391725538065657\n",
      "Iteration: 9360 Training Accuracy: 1.0 Loss: 0.0007196479709818959\n",
      "Iteration: 9370 Training Accuracy: 0.96875 Loss: 0.0012331567704677582\n",
      "Iteration: 9380 Training Accuracy: 1.0 Loss: 0.0006398666300810874\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9181666666666667\n",
      "epoch: 10\n",
      "Iteration: 9390 Training Accuracy: 0.96875 Loss: 0.0015869751805439591\n",
      "Iteration: 9400 Training Accuracy: 0.984375 Loss: 0.0009214907186105847\n",
      "Iteration: 9410 Training Accuracy: 0.96875 Loss: 0.0016232746420428157\n",
      "Iteration: 9420 Training Accuracy: 0.96875 Loss: 0.0010803148616105318\n",
      "Iteration: 9430 Training Accuracy: 0.984375 Loss: 0.0011431125458329916\n",
      "Iteration: 9440 Training Accuracy: 0.890625 Loss: 0.003730421420186758\n",
      "Iteration: 9450 Training Accuracy: 0.96875 Loss: 0.0022801109589636326\n",
      "Iteration: 9460 Training Accuracy: 0.953125 Loss: 0.0017635758267715573\n",
      "Iteration: 9470 Training Accuracy: 0.953125 Loss: 0.0028434719424694777\n",
      "Iteration: 9480 Training Accuracy: 0.96875 Loss: 0.0014404274988919497\n",
      "Iteration: 9490 Training Accuracy: 0.96875 Loss: 0.0014742460334673524\n",
      "Iteration: 9500 Training Accuracy: 0.984375 Loss: 0.0009855306707322598\n",
      "Iteration: 9510 Training Accuracy: 0.9375 Loss: 0.002558700740337372\n",
      "Iteration: 9520 Training Accuracy: 0.921875 Loss: 0.0037361057475209236\n",
      "Iteration: 9530 Training Accuracy: 1.0 Loss: 0.0005345538957044482\n",
      "Iteration: 9540 Training Accuracy: 0.984375 Loss: 0.001260116696357727\n",
      "Iteration: 9550 Training Accuracy: 0.96875 Loss: 0.0016764563042670488\n",
      "Iteration: 9560 Training Accuracy: 0.921875 Loss: 0.002939241472631693\n",
      "Iteration: 9570 Training Accuracy: 1.0 Loss: 0.0006234924076125026\n",
      "Iteration: 9580 Training Accuracy: 0.984375 Loss: 0.0013373459223657846\n",
      "Iteration: 9590 Training Accuracy: 0.984375 Loss: 0.0015278779901564121\n",
      "Iteration: 9600 Training Accuracy: 0.96875 Loss: 0.0020736297592520714\n",
      "Iteration: 9610 Training Accuracy: 0.96875 Loss: 0.001664973795413971\n",
      "Iteration: 9620 Training Accuracy: 0.984375 Loss: 0.0013348849024623632\n",
      "Iteration: 9630 Training Accuracy: 0.984375 Loss: 0.0011895822826772928\n",
      "Iteration: 9640 Training Accuracy: 0.984375 Loss: 0.0016490520210936666\n",
      "Iteration: 9650 Training Accuracy: 0.9375 Loss: 0.0026577322278171778\n",
      "Iteration: 9660 Training Accuracy: 0.984375 Loss: 0.0016903416253626347\n",
      "Iteration: 9670 Training Accuracy: 0.96875 Loss: 0.001581529388204217\n",
      "Iteration: 9680 Training Accuracy: 0.9375 Loss: 0.0019769722130149603\n",
      "Iteration: 9690 Training Accuracy: 0.9375 Loss: 0.0034341253340244293\n",
      "Iteration: 9700 Training Accuracy: 0.96875 Loss: 0.0017797406762838364\n",
      "Iteration: 9710 Training Accuracy: 0.96875 Loss: 0.0014780203346163034\n",
      "Iteration: 9720 Training Accuracy: 1.0 Loss: 0.0006831447826698422\n",
      "Iteration: 9730 Training Accuracy: 0.96875 Loss: 0.0016594917979091406\n",
      "Iteration: 9740 Training Accuracy: 0.984375 Loss: 0.0013913497095927596\n",
      "Iteration: 9750 Training Accuracy: 0.96875 Loss: 0.0023475198540836573\n",
      "Iteration: 9760 Training Accuracy: 0.96875 Loss: 0.0012526856735348701\n",
      "Iteration: 9770 Training Accuracy: 0.984375 Loss: 0.0009196566534228623\n",
      "Iteration: 9780 Training Accuracy: 0.96875 Loss: 0.001196896773763001\n",
      "Iteration: 9790 Training Accuracy: 1.0 Loss: 0.0007230486953631043\n",
      "Iteration: 9800 Training Accuracy: 0.984375 Loss: 0.0007018165779300034\n",
      "Iteration: 9810 Training Accuracy: 0.96875 Loss: 0.0015808066818863153\n",
      "Iteration: 9820 Training Accuracy: 0.953125 Loss: 0.0016016206936910748\n",
      "Iteration: 9830 Training Accuracy: 0.9375 Loss: 0.001673052553087473\n",
      "Iteration: 9840 Training Accuracy: 1.0 Loss: 0.0005534720839932561\n",
      "Iteration: 9850 Training Accuracy: 0.9375 Loss: 0.00214760797098279\n",
      "Iteration: 9860 Training Accuracy: 0.984375 Loss: 0.0010413540294393897\n",
      "Iteration: 9870 Training Accuracy: 1.0 Loss: 0.0007823393098078668\n",
      "Iteration: 9880 Training Accuracy: 0.984375 Loss: 0.0021685792598873377\n",
      "Iteration: 9890 Training Accuracy: 0.96875 Loss: 0.0015339819947257638\n",
      "Iteration: 9900 Training Accuracy: 0.953125 Loss: 0.0012415720848366618\n",
      "Iteration: 9910 Training Accuracy: 1.0 Loss: 7.171964534791186e-05\n",
      "Iteration: 9920 Training Accuracy: 0.953125 Loss: 0.0019308760529384017\n",
      "Iteration: 9930 Training Accuracy: 0.96875 Loss: 0.0013061659410595894\n",
      "Iteration: 9940 Training Accuracy: 1.0 Loss: 0.00044372910633683205\n",
      "Iteration: 9950 Training Accuracy: 0.953125 Loss: 0.0019707796163856983\n",
      "Iteration: 9960 Training Accuracy: 0.96875 Loss: 0.0015453113010153174\n",
      "Iteration: 9970 Training Accuracy: 0.984375 Loss: 0.000587981310673058\n",
      "Iteration: 9980 Training Accuracy: 0.96875 Loss: 0.0015662071527913213\n",
      "Iteration: 9990 Training Accuracy: 1.0 Loss: 0.00042460812255740166\n",
      "Iteration: 10000 Training Accuracy: 0.953125 Loss: 0.001923010335303843\n",
      "Iteration: 10010 Training Accuracy: 1.0 Loss: 0.0006259151850827038\n",
      "Iteration: 10020 Training Accuracy: 0.96875 Loss: 0.0012184074148535728\n",
      "Iteration: 10030 Training Accuracy: 0.984375 Loss: 0.0008023605914786458\n",
      "Iteration: 10040 Training Accuracy: 0.96875 Loss: 0.0013226373121142387\n",
      "Iteration: 10050 Training Accuracy: 0.96875 Loss: 0.0008641274180263281\n",
      "Iteration: 10060 Training Accuracy: 0.96875 Loss: 0.0017379894852638245\n",
      "Iteration: 10070 Training Accuracy: 1.0 Loss: 0.0005876311915926635\n",
      "Iteration: 10080 Training Accuracy: 1.0 Loss: 0.0008083622669801116\n",
      "Iteration: 10090 Training Accuracy: 1.0 Loss: 0.0006044186884537339\n",
      "Iteration: 10100 Training Accuracy: 1.0 Loss: 0.0007069529965519905\n",
      "Iteration: 10110 Training Accuracy: 0.953125 Loss: 0.0016830121167004108\n",
      "Iteration: 10120 Training Accuracy: 1.0 Loss: 0.00042162579484283924\n",
      "Iteration: 10130 Training Accuracy: 0.953125 Loss: 0.0015833290526643395\n",
      "Iteration: 10140 Training Accuracy: 0.984375 Loss: 0.001041093491949141\n",
      "Iteration: 10150 Training Accuracy: 0.96875 Loss: 0.0017746775411069393\n",
      "Iteration: 10160 Training Accuracy: 0.96875 Loss: 0.0019092672737315297\n",
      "Iteration: 10170 Training Accuracy: 1.0 Loss: 0.0005071325576864183\n",
      "Iteration: 10180 Training Accuracy: 0.984375 Loss: 0.0013411676045507193\n",
      "Iteration: 10190 Training Accuracy: 0.953125 Loss: 0.0017956088995561004\n",
      "Iteration: 10200 Training Accuracy: 0.96875 Loss: 0.0011763228103518486\n",
      "Iteration: 10210 Training Accuracy: 0.953125 Loss: 0.001981787383556366\n",
      "Iteration: 10220 Training Accuracy: 0.96875 Loss: 0.001457114820368588\n",
      "Iteration: 10230 Training Accuracy: 1.0 Loss: 0.00018184421060141176\n",
      "Iteration: 10240 Training Accuracy: 1.0 Loss: 0.0004471638530958444\n",
      "Iteration: 10250 Training Accuracy: 1.0 Loss: 0.0003251250018365681\n",
      "Iteration: 10260 Training Accuracy: 0.984375 Loss: 0.001015553600154817\n",
      "Iteration: 10270 Training Accuracy: 0.984375 Loss: 0.0008617035346105695\n",
      "Iteration: 10280 Training Accuracy: 1.0 Loss: 0.0006212831940501928\n",
      "Iteration: 10290 Training Accuracy: 0.96875 Loss: 0.00113407033495605\n",
      "Iteration: 10300 Training Accuracy: 0.984375 Loss: 0.0007336920825764537\n",
      "Iteration: 10310 Training Accuracy: 1.0 Loss: 0.0008562239818274975\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9198333333333333\n",
      "epoch: 11\n",
      "Iteration: 10320 Training Accuracy: 0.953125 Loss: 0.0029734577983617783\n",
      "Iteration: 10330 Training Accuracy: 0.9375 Loss: 0.0023770215921103954\n",
      "Iteration: 10340 Training Accuracy: 0.921875 Loss: 0.0030579224694520235\n",
      "Iteration: 10350 Training Accuracy: 0.984375 Loss: 0.0014093811623752117\n",
      "Iteration: 10360 Training Accuracy: 0.90625 Loss: 0.002872907556593418\n",
      "Iteration: 10370 Training Accuracy: 1.0 Loss: 0.0007321943994611502\n",
      "Iteration: 10380 Training Accuracy: 1.0 Loss: 0.0004566757706925273\n",
      "Iteration: 10390 Training Accuracy: 0.984375 Loss: 0.0010147499851882458\n",
      "Iteration: 10400 Training Accuracy: 0.96875 Loss: 0.0013283449225127697\n",
      "Iteration: 10410 Training Accuracy: 1.0 Loss: 0.0007105647237040102\n",
      "Iteration: 10420 Training Accuracy: 0.953125 Loss: 0.0022587564308196306\n",
      "Iteration: 10430 Training Accuracy: 0.9375 Loss: 0.003068844322115183\n",
      "Iteration: 10440 Training Accuracy: 1.0 Loss: 0.0006435108371078968\n",
      "Iteration: 10450 Training Accuracy: 0.9375 Loss: 0.003188424278050661\n",
      "Iteration: 10460 Training Accuracy: 0.984375 Loss: 0.0012874770909547806\n",
      "Iteration: 10470 Training Accuracy: 0.96875 Loss: 0.0013055960880592465\n",
      "Iteration: 10480 Training Accuracy: 0.984375 Loss: 0.001862228149548173\n",
      "Iteration: 10490 Training Accuracy: 0.953125 Loss: 0.0017327682580798864\n",
      "Iteration: 10500 Training Accuracy: 0.96875 Loss: 0.0014965429436415434\n",
      "Iteration: 10510 Training Accuracy: 0.984375 Loss: 0.000750282546505332\n",
      "Iteration: 10520 Training Accuracy: 1.0 Loss: 0.0002495597000233829\n",
      "Iteration: 10530 Training Accuracy: 0.984375 Loss: 0.0006789230392314494\n",
      "Iteration: 10540 Training Accuracy: 0.984375 Loss: 0.001196014927700162\n",
      "Iteration: 10550 Training Accuracy: 0.96875 Loss: 0.001511154230684042\n",
      "Iteration: 10560 Training Accuracy: 1.0 Loss: 0.000247361371293664\n",
      "Iteration: 10570 Training Accuracy: 0.953125 Loss: 0.00146824074909091\n",
      "Iteration: 10580 Training Accuracy: 0.9375 Loss: 0.002667271764948964\n",
      "Iteration: 10590 Training Accuracy: 0.953125 Loss: 0.0017215377883985639\n",
      "Iteration: 10600 Training Accuracy: 0.9375 Loss: 0.0035729287192225456\n",
      "Iteration: 10610 Training Accuracy: 1.0 Loss: 0.0010032820282503963\n",
      "Iteration: 10620 Training Accuracy: 1.0 Loss: 0.0003102292539551854\n",
      "Iteration: 10630 Training Accuracy: 0.9375 Loss: 0.0024838391691446304\n",
      "Iteration: 10640 Training Accuracy: 0.953125 Loss: 0.001770437229424715\n",
      "Iteration: 10650 Training Accuracy: 0.984375 Loss: 0.0008070588810369372\n",
      "Iteration: 10660 Training Accuracy: 1.0 Loss: 0.0006055528065189719\n",
      "Iteration: 10670 Training Accuracy: 0.9375 Loss: 0.003031737869605422\n",
      "Iteration: 10680 Training Accuracy: 1.0 Loss: 0.0007391448016278446\n",
      "Iteration: 10690 Training Accuracy: 0.953125 Loss: 0.0017169970087707043\n",
      "Iteration: 10700 Training Accuracy: 0.96875 Loss: 0.0019095648312941194\n",
      "Iteration: 10710 Training Accuracy: 1.0 Loss: 0.0011196057312190533\n",
      "Iteration: 10720 Training Accuracy: 0.953125 Loss: 0.0023452755995094776\n",
      "Iteration: 10730 Training Accuracy: 0.984375 Loss: 0.0014686300419270992\n",
      "Iteration: 10740 Training Accuracy: 0.96875 Loss: 0.001509614521637559\n",
      "Iteration: 10750 Training Accuracy: 0.984375 Loss: 0.0011501720873638988\n",
      "Iteration: 10760 Training Accuracy: 1.0 Loss: 0.0005755728925578296\n",
      "Iteration: 10770 Training Accuracy: 0.953125 Loss: 0.002077026292681694\n",
      "Iteration: 10780 Training Accuracy: 0.96875 Loss: 0.001960422843694687\n",
      "Iteration: 10790 Training Accuracy: 0.984375 Loss: 0.0013077720068395138\n",
      "Iteration: 10800 Training Accuracy: 1.0 Loss: 0.000461313669802621\n",
      "Iteration: 10810 Training Accuracy: 0.984375 Loss: 0.0008609815849922597\n",
      "Iteration: 10820 Training Accuracy: 0.984375 Loss: 0.0011170182842761278\n",
      "Iteration: 10830 Training Accuracy: 0.984375 Loss: 0.0012695948826149106\n",
      "Iteration: 10840 Training Accuracy: 0.984375 Loss: 0.0008566158940084279\n",
      "Iteration: 10850 Training Accuracy: 0.9375 Loss: 0.0022112324368208647\n",
      "Iteration: 10860 Training Accuracy: 1.0 Loss: 0.0004625208384823054\n",
      "Iteration: 10870 Training Accuracy: 1.0 Loss: 0.0006788659957237542\n",
      "Iteration: 10880 Training Accuracy: 0.984375 Loss: 0.0009184195660054684\n",
      "Iteration: 10890 Training Accuracy: 1.0 Loss: 0.00035128244780935347\n",
      "Iteration: 10900 Training Accuracy: 0.984375 Loss: 0.001146340393461287\n",
      "Iteration: 10910 Training Accuracy: 1.0 Loss: 0.0003681526577565819\n",
      "Iteration: 10920 Training Accuracy: 0.984375 Loss: 0.0015629923436790705\n",
      "Iteration: 10930 Training Accuracy: 0.953125 Loss: 0.001959552289918065\n",
      "Iteration: 10940 Training Accuracy: 0.96875 Loss: 0.0014444662956520915\n",
      "Iteration: 10950 Training Accuracy: 0.984375 Loss: 0.0008980708662420511\n",
      "Iteration: 10960 Training Accuracy: 0.984375 Loss: 0.0004971874295733869\n",
      "Iteration: 10970 Training Accuracy: 0.96875 Loss: 0.0014710843097418547\n",
      "Iteration: 10980 Training Accuracy: 1.0 Loss: 0.00020128581672906876\n",
      "Iteration: 10990 Training Accuracy: 0.984375 Loss: 0.0015129178063943982\n",
      "Iteration: 11000 Training Accuracy: 0.953125 Loss: 0.001823606202378869\n",
      "Iteration: 11010 Training Accuracy: 1.0 Loss: 0.0006604649825021625\n",
      "Iteration: 11020 Training Accuracy: 0.984375 Loss: 0.0010255862725898623\n",
      "Iteration: 11030 Training Accuracy: 0.984375 Loss: 0.0012510203523561358\n",
      "Iteration: 11040 Training Accuracy: 0.96875 Loss: 0.0010819005547091365\n",
      "Iteration: 11050 Training Accuracy: 0.953125 Loss: 0.0020609619095921516\n",
      "Iteration: 11060 Training Accuracy: 0.984375 Loss: 0.0006389129557646811\n",
      "Iteration: 11070 Training Accuracy: 1.0 Loss: 0.0009379899129271507\n",
      "Iteration: 11080 Training Accuracy: 1.0 Loss: 0.0006078201113268733\n",
      "Iteration: 11090 Training Accuracy: 0.96875 Loss: 0.0013291186187416315\n",
      "Iteration: 11100 Training Accuracy: 0.953125 Loss: 0.0034122939687222242\n",
      "Iteration: 11110 Training Accuracy: 0.984375 Loss: 0.0007417278829962015\n",
      "Iteration: 11120 Training Accuracy: 1.0 Loss: 0.0005830992013216019\n",
      "Iteration: 11130 Training Accuracy: 0.984375 Loss: 0.001104228664189577\n",
      "Iteration: 11140 Training Accuracy: 0.984375 Loss: 0.001228385604918003\n",
      "Iteration: 11150 Training Accuracy: 0.984375 Loss: 0.002043396234512329\n",
      "Iteration: 11160 Training Accuracy: 0.984375 Loss: 0.0013169777812436223\n",
      "Iteration: 11170 Training Accuracy: 0.953125 Loss: 0.0022217747755348682\n",
      "Iteration: 11180 Training Accuracy: 1.0 Loss: 0.0006322046974673867\n",
      "Iteration: 11190 Training Accuracy: 1.0 Loss: 0.00019938216428272426\n",
      "Iteration: 11200 Training Accuracy: 1.0 Loss: 0.0007832433329895139\n",
      "Iteration: 11210 Training Accuracy: 0.984375 Loss: 0.0012819832190871239\n",
      "Iteration: 11220 Training Accuracy: 0.984375 Loss: 0.0012882149312645197\n",
      "Iteration: 11230 Training Accuracy: 0.984375 Loss: 0.0012507187202572823\n",
      "Iteration: 11240 Training Accuracy: 1.0 Loss: 0.00044292310485616326\n",
      "Iteration: 11250 Training Accuracy: 1.0 Loss: 0.0004727859632112086\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9203333333333333\n",
      "epoch: 12\n",
      "Iteration: 11260 Training Accuracy: 0.953125 Loss: 0.0026332957204431295\n",
      "Iteration: 11270 Training Accuracy: 0.96875 Loss: 0.0008496860973536968\n",
      "Iteration: 11280 Training Accuracy: 0.984375 Loss: 0.0005239698803052306\n",
      "Iteration: 11290 Training Accuracy: 0.984375 Loss: 0.0011023420374840498\n",
      "Iteration: 11300 Training Accuracy: 1.0 Loss: 0.0006139259785413742\n",
      "Iteration: 11310 Training Accuracy: 0.953125 Loss: 0.0029402689542621374\n",
      "Iteration: 11320 Training Accuracy: 0.984375 Loss: 0.0007581573445349932\n",
      "Iteration: 11330 Training Accuracy: 0.953125 Loss: 0.002659175544977188\n",
      "Iteration: 11340 Training Accuracy: 0.96875 Loss: 0.0009531372925266623\n",
      "Iteration: 11350 Training Accuracy: 1.0 Loss: 0.00037847046041861176\n",
      "Iteration: 11360 Training Accuracy: 0.96875 Loss: 0.001665318151935935\n",
      "Iteration: 11370 Training Accuracy: 0.9375 Loss: 0.0021770603489130735\n",
      "Iteration: 11380 Training Accuracy: 0.984375 Loss: 0.001444850699044764\n",
      "Iteration: 11390 Training Accuracy: 0.96875 Loss: 0.0023041982203722\n",
      "Iteration: 11400 Training Accuracy: 1.0 Loss: 0.0007135662017390132\n",
      "Iteration: 11410 Training Accuracy: 0.953125 Loss: 0.001312297536060214\n",
      "Iteration: 11420 Training Accuracy: 0.984375 Loss: 0.0019366898341104388\n",
      "Iteration: 11430 Training Accuracy: 1.0 Loss: 0.0005773763987235725\n",
      "Iteration: 11440 Training Accuracy: 0.96875 Loss: 0.001601134310476482\n",
      "Iteration: 11450 Training Accuracy: 0.96875 Loss: 0.0013725515455007553\n",
      "Iteration: 11460 Training Accuracy: 0.984375 Loss: 0.0004841161717195064\n",
      "Iteration: 11470 Training Accuracy: 0.953125 Loss: 0.0016301230061799288\n",
      "Iteration: 11480 Training Accuracy: 0.984375 Loss: 0.0006372191128320992\n",
      "Iteration: 11490 Training Accuracy: 0.953125 Loss: 0.0022246581502258778\n",
      "Iteration: 11500 Training Accuracy: 0.984375 Loss: 0.0009206995600834489\n",
      "Iteration: 11510 Training Accuracy: 0.953125 Loss: 0.0016745376633480191\n",
      "Iteration: 11520 Training Accuracy: 0.96875 Loss: 0.0009391340427100658\n",
      "Iteration: 11530 Training Accuracy: 0.96875 Loss: 0.0010450471891090274\n",
      "Iteration: 11540 Training Accuracy: 0.984375 Loss: 0.0025254786014556885\n",
      "Iteration: 11550 Training Accuracy: 0.96875 Loss: 0.0015490050427615643\n",
      "Iteration: 11560 Training Accuracy: 1.0 Loss: 0.0005267826491035521\n",
      "Iteration: 11570 Training Accuracy: 0.96875 Loss: 0.0012677605263888836\n",
      "Iteration: 11580 Training Accuracy: 0.96875 Loss: 0.0014829309657216072\n",
      "Iteration: 11590 Training Accuracy: 0.96875 Loss: 0.0019766513723880053\n",
      "Iteration: 11600 Training Accuracy: 0.96875 Loss: 0.0013148720609024167\n",
      "Iteration: 11610 Training Accuracy: 1.0 Loss: 0.0004384944913908839\n",
      "Iteration: 11620 Training Accuracy: 0.984375 Loss: 0.0011796699836850166\n",
      "Iteration: 11630 Training Accuracy: 0.953125 Loss: 0.0017805781681090593\n",
      "Iteration: 11640 Training Accuracy: 0.921875 Loss: 0.002653977135196328\n",
      "Iteration: 11650 Training Accuracy: 0.96875 Loss: 0.000895573350135237\n",
      "Iteration: 11660 Training Accuracy: 0.984375 Loss: 0.0006340179825201631\n",
      "Iteration: 11670 Training Accuracy: 0.96875 Loss: 0.0013708043843507767\n",
      "Iteration: 11680 Training Accuracy: 0.96875 Loss: 0.001039816183038056\n",
      "Iteration: 11690 Training Accuracy: 0.96875 Loss: 0.0012580030597746372\n",
      "Iteration: 11700 Training Accuracy: 0.953125 Loss: 0.002727809827774763\n",
      "Iteration: 11710 Training Accuracy: 1.0 Loss: 0.0008627393981441855\n",
      "Iteration: 11720 Training Accuracy: 0.953125 Loss: 0.0012678969651460648\n",
      "Iteration: 11730 Training Accuracy: 0.984375 Loss: 0.0006597444298677146\n",
      "Iteration: 11740 Training Accuracy: 0.984375 Loss: 0.0012644634116441011\n",
      "Iteration: 11750 Training Accuracy: 0.953125 Loss: 0.0021003377623856068\n",
      "Iteration: 11760 Training Accuracy: 0.96875 Loss: 0.0027096548583358526\n",
      "Iteration: 11770 Training Accuracy: 0.96875 Loss: 0.001094261766411364\n",
      "Iteration: 11780 Training Accuracy: 0.96875 Loss: 0.00185818737372756\n",
      "Iteration: 11790 Training Accuracy: 0.96875 Loss: 0.002039979910477996\n",
      "Iteration: 11800 Training Accuracy: 0.953125 Loss: 0.0034959043841809034\n",
      "Iteration: 11810 Training Accuracy: 0.96875 Loss: 0.002629994647577405\n",
      "Iteration: 11820 Training Accuracy: 0.984375 Loss: 0.0009839204140007496\n",
      "Iteration: 11830 Training Accuracy: 0.984375 Loss: 0.0015854252269491553\n",
      "Iteration: 11840 Training Accuracy: 1.0 Loss: 0.0004797939327545464\n",
      "Iteration: 11850 Training Accuracy: 0.953125 Loss: 0.0019052778370678425\n",
      "Iteration: 11860 Training Accuracy: 0.953125 Loss: 0.0027476255781948566\n",
      "Iteration: 11870 Training Accuracy: 0.984375 Loss: 0.0011256963480263948\n",
      "Iteration: 11880 Training Accuracy: 0.984375 Loss: 0.0021842874120920897\n",
      "Iteration: 11890 Training Accuracy: 0.984375 Loss: 0.00099577393848449\n",
      "Iteration: 11900 Training Accuracy: 1.0 Loss: 0.000507921795360744\n",
      "Iteration: 11910 Training Accuracy: 1.0 Loss: 0.0007789068622514606\n",
      "Iteration: 11920 Training Accuracy: 0.984375 Loss: 0.0008682183688506484\n",
      "Iteration: 11930 Training Accuracy: 0.984375 Loss: 0.0007786288624629378\n",
      "Iteration: 11940 Training Accuracy: 1.0 Loss: 0.0005814203177578747\n",
      "Iteration: 11950 Training Accuracy: 0.984375 Loss: 0.001292844652198255\n",
      "Iteration: 11960 Training Accuracy: 0.953125 Loss: 0.0015024712774902582\n",
      "Iteration: 11970 Training Accuracy: 0.984375 Loss: 0.0004974771291017532\n",
      "Iteration: 11980 Training Accuracy: 0.953125 Loss: 0.0016258518444374204\n",
      "Iteration: 11990 Training Accuracy: 1.0 Loss: 0.0005111662321723998\n",
      "Iteration: 12000 Training Accuracy: 0.96875 Loss: 0.0018189341062679887\n",
      "Iteration: 12010 Training Accuracy: 0.984375 Loss: 0.0011154951062053442\n",
      "Iteration: 12020 Training Accuracy: 0.96875 Loss: 0.001784453634172678\n",
      "Iteration: 12030 Training Accuracy: 0.984375 Loss: 0.0011212255340069532\n",
      "Iteration: 12040 Training Accuracy: 0.984375 Loss: 0.0014785048551857471\n",
      "Iteration: 12050 Training Accuracy: 0.96875 Loss: 0.0008938973187468946\n",
      "Iteration: 12060 Training Accuracy: 0.984375 Loss: 0.0013524151872843504\n",
      "Iteration: 12070 Training Accuracy: 0.984375 Loss: 0.0012054711114615202\n",
      "Iteration: 12080 Training Accuracy: 0.96875 Loss: 0.00114311627112329\n",
      "Iteration: 12090 Training Accuracy: 0.96875 Loss: 0.001190335606224835\n",
      "Iteration: 12100 Training Accuracy: 0.953125 Loss: 0.0023611513897776604\n",
      "Iteration: 12110 Training Accuracy: 0.96875 Loss: 0.0014539060648530722\n",
      "Iteration: 12120 Training Accuracy: 0.984375 Loss: 0.0007628388702869415\n",
      "Iteration: 12130 Training Accuracy: 1.0 Loss: 0.000920713588129729\n",
      "Iteration: 12140 Training Accuracy: 0.953125 Loss: 0.0013234566431492567\n",
      "Iteration: 12150 Training Accuracy: 0.984375 Loss: 0.0015451819635927677\n",
      "Iteration: 12160 Training Accuracy: 0.96875 Loss: 0.0010564677650108933\n",
      "Iteration: 12170 Training Accuracy: 1.0 Loss: 0.0009087655344046652\n",
      "Iteration: 12180 Training Accuracy: 0.96875 Loss: 0.0015373314963653684\n",
      "Iteration: 12190 Training Accuracy: 0.984375 Loss: 0.0005570984212681651\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9206666666666666\n",
      "epoch: 13\n",
      "Iteration: 12200 Training Accuracy: 0.984375 Loss: 0.0010628579184412956\n",
      "Iteration: 12210 Training Accuracy: 0.96875 Loss: 0.0018770887982100248\n",
      "Iteration: 12220 Training Accuracy: 1.0 Loss: 0.0007394455606117845\n",
      "Iteration: 12230 Training Accuracy: 0.96875 Loss: 0.0011476116487756371\n",
      "Iteration: 12240 Training Accuracy: 1.0 Loss: 0.0008496041409671307\n",
      "Iteration: 12250 Training Accuracy: 0.984375 Loss: 0.0011628776555880904\n",
      "Iteration: 12260 Training Accuracy: 0.953125 Loss: 0.0024028969928622246\n",
      "Iteration: 12270 Training Accuracy: 0.96875 Loss: 0.0024567805230617523\n",
      "Iteration: 12280 Training Accuracy: 0.984375 Loss: 0.0008259902242571115\n",
      "Iteration: 12290 Training Accuracy: 0.953125 Loss: 0.0015579069731757045\n",
      "Iteration: 12300 Training Accuracy: 0.96875 Loss: 0.001064300537109375\n",
      "Iteration: 12310 Training Accuracy: 0.984375 Loss: 0.0009991523111239076\n",
      "Iteration: 12320 Training Accuracy: 0.984375 Loss: 0.0009649485582485795\n",
      "Iteration: 12330 Training Accuracy: 1.0 Loss: 0.0006400044658221304\n",
      "Iteration: 12340 Training Accuracy: 1.0 Loss: 0.0006998818716965616\n",
      "Iteration: 12350 Training Accuracy: 0.96875 Loss: 0.002657087752595544\n",
      "Iteration: 12360 Training Accuracy: 0.984375 Loss: 0.00077864492777735\n",
      "Iteration: 12370 Training Accuracy: 1.0 Loss: 0.0007949323626235127\n",
      "Iteration: 12380 Training Accuracy: 1.0 Loss: 0.00043180244392715394\n",
      "Iteration: 12390 Training Accuracy: 0.984375 Loss: 0.0005264359642751515\n",
      "Iteration: 12400 Training Accuracy: 0.984375 Loss: 0.0009231442818418145\n",
      "Iteration: 12410 Training Accuracy: 0.9375 Loss: 0.0037578425835818052\n",
      "Iteration: 12420 Training Accuracy: 0.984375 Loss: 0.0007054451270960271\n",
      "Iteration: 12430 Training Accuracy: 0.96875 Loss: 0.0008108083275146782\n",
      "Iteration: 12440 Training Accuracy: 1.0 Loss: 0.0009014499955810606\n",
      "Iteration: 12450 Training Accuracy: 0.984375 Loss: 0.0008662562468089163\n",
      "Iteration: 12460 Training Accuracy: 0.984375 Loss: 0.0016287241596728563\n",
      "Iteration: 12470 Training Accuracy: 0.96875 Loss: 0.0010181546676903963\n",
      "Iteration: 12480 Training Accuracy: 1.0 Loss: 0.00045033605420030653\n",
      "Iteration: 12490 Training Accuracy: 0.96875 Loss: 0.0011735965963453054\n",
      "Iteration: 12500 Training Accuracy: 0.984375 Loss: 0.0003746938018593937\n",
      "Iteration: 12510 Training Accuracy: 0.984375 Loss: 0.0009299791418015957\n",
      "Iteration: 12520 Training Accuracy: 1.0 Loss: 0.0005048396997153759\n",
      "Iteration: 12530 Training Accuracy: 0.96875 Loss: 0.0015218340558931231\n",
      "Iteration: 12540 Training Accuracy: 1.0 Loss: 0.0007323180907405913\n",
      "Iteration: 12550 Training Accuracy: 0.984375 Loss: 0.0008336149039678276\n",
      "Iteration: 12560 Training Accuracy: 0.984375 Loss: 0.0007846251828595996\n",
      "Iteration: 12570 Training Accuracy: 0.984375 Loss: 0.0009501283639110625\n",
      "Iteration: 12580 Training Accuracy: 0.9375 Loss: 0.002366437343880534\n",
      "Iteration: 12590 Training Accuracy: 0.984375 Loss: 0.0013343493919819593\n",
      "Iteration: 12600 Training Accuracy: 0.984375 Loss: 0.0006687755812890828\n",
      "Iteration: 12610 Training Accuracy: 1.0 Loss: 0.000998270115815103\n",
      "Iteration: 12620 Training Accuracy: 1.0 Loss: 0.0007102796225808561\n",
      "Iteration: 12630 Training Accuracy: 0.96875 Loss: 0.0013050842098891735\n",
      "Iteration: 12640 Training Accuracy: 0.984375 Loss: 0.0012144402135163546\n",
      "Iteration: 12650 Training Accuracy: 0.984375 Loss: 0.000748762337025255\n",
      "Iteration: 12660 Training Accuracy: 0.953125 Loss: 0.001512318616732955\n",
      "Iteration: 12670 Training Accuracy: 0.953125 Loss: 0.001937313936650753\n",
      "Iteration: 12680 Training Accuracy: 0.984375 Loss: 0.0009796745143830776\n",
      "Iteration: 12690 Training Accuracy: 0.984375 Loss: 0.0006668712012469769\n",
      "Iteration: 12700 Training Accuracy: 1.0 Loss: 0.0008540217531844974\n",
      "Iteration: 12710 Training Accuracy: 0.984375 Loss: 0.0014036004431545734\n",
      "Iteration: 12720 Training Accuracy: 0.984375 Loss: 0.0010037175379693508\n",
      "Iteration: 12730 Training Accuracy: 1.0 Loss: 0.0005128863267600536\n",
      "Iteration: 12740 Training Accuracy: 1.0 Loss: 0.0011551263742148876\n",
      "Iteration: 12750 Training Accuracy: 1.0 Loss: 0.0004860253247898072\n",
      "Iteration: 12760 Training Accuracy: 1.0 Loss: 0.0004680283891502768\n",
      "Iteration: 12770 Training Accuracy: 0.96875 Loss: 0.0011077584931626916\n",
      "Iteration: 12780 Training Accuracy: 0.984375 Loss: 0.0012742712860926986\n",
      "Iteration: 12790 Training Accuracy: 0.984375 Loss: 0.0013223749119788408\n",
      "Iteration: 12800 Training Accuracy: 0.96875 Loss: 0.0017527217278257012\n",
      "Iteration: 12810 Training Accuracy: 0.984375 Loss: 0.000578344683162868\n",
      "Iteration: 12820 Training Accuracy: 0.984375 Loss: 0.0016575391637161374\n",
      "Iteration: 12830 Training Accuracy: 1.0 Loss: 0.0007830631802789867\n",
      "Iteration: 12840 Training Accuracy: 0.984375 Loss: 0.0013852427946403623\n",
      "Iteration: 12850 Training Accuracy: 1.0 Loss: 0.0005708823446184397\n",
      "Iteration: 12860 Training Accuracy: 0.984375 Loss: 0.0007450137054547668\n",
      "Iteration: 12870 Training Accuracy: 1.0 Loss: 0.0008817457128316164\n",
      "Iteration: 12880 Training Accuracy: 0.96875 Loss: 0.0014996766112744808\n",
      "Iteration: 12890 Training Accuracy: 1.0 Loss: 0.0007383868214674294\n",
      "Iteration: 12900 Training Accuracy: 1.0 Loss: 0.000529558164998889\n",
      "Iteration: 12910 Training Accuracy: 0.984375 Loss: 0.0019414937123656273\n",
      "Iteration: 12920 Training Accuracy: 1.0 Loss: 0.000494328502099961\n",
      "Iteration: 12930 Training Accuracy: 0.953125 Loss: 0.0017235665582120419\n",
      "Iteration: 12940 Training Accuracy: 0.984375 Loss: 0.0014976689126342535\n",
      "Iteration: 12950 Training Accuracy: 1.0 Loss: 0.0008651982643641531\n",
      "Iteration: 12960 Training Accuracy: 1.0 Loss: 0.0010438989847898483\n",
      "Iteration: 12970 Training Accuracy: 0.953125 Loss: 0.0014991388889029622\n",
      "Iteration: 12980 Training Accuracy: 1.0 Loss: 0.0007773999241180718\n",
      "Iteration: 12990 Training Accuracy: 1.0 Loss: 0.0007439250475727022\n",
      "Iteration: 13000 Training Accuracy: 0.96875 Loss: 0.001235384028404951\n",
      "Iteration: 13010 Training Accuracy: 0.96875 Loss: 0.0013604434207081795\n",
      "Iteration: 13020 Training Accuracy: 0.96875 Loss: 0.0021064598113298416\n",
      "Iteration: 13030 Training Accuracy: 1.0 Loss: 0.0007915057940408587\n",
      "Iteration: 13040 Training Accuracy: 1.0 Loss: 0.0005137756816111505\n",
      "Iteration: 13050 Training Accuracy: 0.953125 Loss: 0.0017589153721928596\n",
      "Iteration: 13060 Training Accuracy: 1.0 Loss: 0.0008324628579430282\n",
      "Iteration: 13070 Training Accuracy: 0.984375 Loss: 0.0009852568618953228\n",
      "Iteration: 13080 Training Accuracy: 0.953125 Loss: 0.0026673434767872095\n",
      "Iteration: 13090 Training Accuracy: 1.0 Loss: 0.0006589467520825565\n",
      "Iteration: 13100 Training Accuracy: 1.0 Loss: 0.0007731015793979168\n",
      "Iteration: 13110 Training Accuracy: 0.984375 Loss: 0.0014164999593049288\n",
      "Iteration: 13120 Training Accuracy: 0.953125 Loss: 0.002207712735980749\n",
      "Iteration: 13130 Training Accuracy: 0.984375 Loss: 0.0006377535173669457\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9206666666666666\n",
      "epoch: 14\n",
      "Iteration: 13140 Training Accuracy: 1.0 Loss: 0.00045958635746501386\n",
      "Iteration: 13150 Training Accuracy: 1.0 Loss: 0.0006061314488761127\n",
      "Iteration: 13160 Training Accuracy: 0.953125 Loss: 0.0015640018973499537\n",
      "Iteration: 13170 Training Accuracy: 0.96875 Loss: 0.000856525672134012\n",
      "Iteration: 13180 Training Accuracy: 0.96875 Loss: 0.001704959198832512\n",
      "Iteration: 13190 Training Accuracy: 0.984375 Loss: 0.001015263143926859\n",
      "Iteration: 13200 Training Accuracy: 0.953125 Loss: 0.0014990248018875718\n",
      "Iteration: 13210 Training Accuracy: 0.984375 Loss: 0.0017658985452726483\n",
      "Iteration: 13220 Training Accuracy: 0.984375 Loss: 0.0013967965496703982\n",
      "Iteration: 13230 Training Accuracy: 0.984375 Loss: 0.001425612368620932\n",
      "Iteration: 13240 Training Accuracy: 1.0 Loss: 0.00041865979437716305\n",
      "Iteration: 13250 Training Accuracy: 0.9375 Loss: 0.002544818678870797\n",
      "Iteration: 13260 Training Accuracy: 0.953125 Loss: 0.001740525709465146\n",
      "Iteration: 13270 Training Accuracy: 0.953125 Loss: 0.0012797643430531025\n",
      "Iteration: 13280 Training Accuracy: 0.953125 Loss: 0.002326715737581253\n",
      "Iteration: 13290 Training Accuracy: 0.96875 Loss: 0.001393654732964933\n",
      "Iteration: 13300 Training Accuracy: 0.984375 Loss: 0.000993284396827221\n",
      "Iteration: 13310 Training Accuracy: 0.984375 Loss: 0.0007457577157765627\n",
      "Iteration: 13320 Training Accuracy: 0.96875 Loss: 0.001081956084817648\n",
      "Iteration: 13330 Training Accuracy: 1.0 Loss: 0.0003637225308921188\n",
      "Iteration: 13340 Training Accuracy: 0.96875 Loss: 0.0006964595522731543\n",
      "Iteration: 13350 Training Accuracy: 1.0 Loss: 0.0010372604010626674\n",
      "Iteration: 13360 Training Accuracy: 0.984375 Loss: 0.0014339323388412595\n",
      "Iteration: 13370 Training Accuracy: 0.96875 Loss: 0.0011930568143725395\n",
      "Iteration: 13380 Training Accuracy: 0.984375 Loss: 0.0014466162538155913\n",
      "Iteration: 13390 Training Accuracy: 0.953125 Loss: 0.0011968406615778804\n",
      "Iteration: 13400 Training Accuracy: 0.953125 Loss: 0.0019439243478700519\n",
      "Iteration: 13410 Training Accuracy: 0.921875 Loss: 0.0019822625909000635\n",
      "Iteration: 13420 Training Accuracy: 0.96875 Loss: 0.0014073748607188463\n",
      "Iteration: 13430 Training Accuracy: 1.0 Loss: 0.0009183321380987763\n",
      "Iteration: 13440 Training Accuracy: 0.953125 Loss: 0.0024215341545641422\n",
      "Iteration: 13450 Training Accuracy: 0.984375 Loss: 0.0021760091185569763\n",
      "Iteration: 13460 Training Accuracy: 0.96875 Loss: 0.0009557290468364954\n",
      "Iteration: 13470 Training Accuracy: 1.0 Loss: 0.00013956708426121622\n",
      "Iteration: 13480 Training Accuracy: 0.984375 Loss: 0.0005810817237943411\n",
      "Iteration: 13490 Training Accuracy: 0.984375 Loss: 0.0007867655949667096\n",
      "Iteration: 13500 Training Accuracy: 0.96875 Loss: 0.00105649558827281\n",
      "Iteration: 13510 Training Accuracy: 0.96875 Loss: 0.001237797550857067\n",
      "Iteration: 13520 Training Accuracy: 0.9375 Loss: 0.001955918036401272\n",
      "Iteration: 13530 Training Accuracy: 1.0 Loss: 0.0006313133635558188\n",
      "Iteration: 13540 Training Accuracy: 0.96875 Loss: 0.0014271910768002272\n",
      "Iteration: 13550 Training Accuracy: 0.984375 Loss: 0.0007220712723210454\n",
      "Iteration: 13560 Training Accuracy: 0.96875 Loss: 0.001333820284344256\n",
      "Iteration: 13570 Training Accuracy: 0.953125 Loss: 0.0013446748489513993\n",
      "Iteration: 13580 Training Accuracy: 0.96875 Loss: 0.002122398465871811\n",
      "Iteration: 13590 Training Accuracy: 0.96875 Loss: 0.0011873674811795354\n",
      "Iteration: 13600 Training Accuracy: 1.0 Loss: 0.00031311361817643046\n",
      "Iteration: 13610 Training Accuracy: 1.0 Loss: 0.0008848527795635164\n",
      "Iteration: 13620 Training Accuracy: 0.984375 Loss: 0.000859189429320395\n",
      "Iteration: 13630 Training Accuracy: 0.984375 Loss: 0.0007202386623248458\n",
      "Iteration: 13640 Training Accuracy: 1.0 Loss: 0.0004978062934242189\n",
      "Iteration: 13650 Training Accuracy: 1.0 Loss: 0.000554532278329134\n",
      "Iteration: 13660 Training Accuracy: 0.984375 Loss: 0.0006003741873428226\n",
      "Iteration: 13670 Training Accuracy: 0.984375 Loss: 0.0005832827882841229\n",
      "Iteration: 13680 Training Accuracy: 0.984375 Loss: 0.0010125063126906753\n",
      "Iteration: 13690 Training Accuracy: 0.953125 Loss: 0.0018831505440175533\n",
      "Iteration: 13700 Training Accuracy: 0.984375 Loss: 0.0007904994417913258\n",
      "Iteration: 13710 Training Accuracy: 0.96875 Loss: 0.001497895922511816\n",
      "Iteration: 13720 Training Accuracy: 0.984375 Loss: 0.0013222486013546586\n",
      "Iteration: 13730 Training Accuracy: 1.0 Loss: 0.0008204414043575525\n",
      "Iteration: 13740 Training Accuracy: 0.96875 Loss: 0.0009912168607115746\n",
      "Iteration: 13750 Training Accuracy: 0.953125 Loss: 0.001780291902832687\n",
      "Iteration: 13760 Training Accuracy: 0.984375 Loss: 0.0006669509457424283\n",
      "Iteration: 13770 Training Accuracy: 0.984375 Loss: 0.0018994035199284554\n",
      "Iteration: 13780 Training Accuracy: 1.0 Loss: 0.0004722219018731266\n",
      "Iteration: 13790 Training Accuracy: 0.984375 Loss: 0.0015025513712316751\n",
      "Iteration: 13800 Training Accuracy: 1.0 Loss: 0.0010645334841683507\n",
      "Iteration: 13810 Training Accuracy: 1.0 Loss: 0.0005979489651508629\n",
      "Iteration: 13820 Training Accuracy: 0.984375 Loss: 0.0007292811060324311\n",
      "Iteration: 13830 Training Accuracy: 0.96875 Loss: 0.0013342732563614845\n",
      "Iteration: 13840 Training Accuracy: 1.0 Loss: 0.00027270635473541915\n",
      "Iteration: 13850 Training Accuracy: 1.0 Loss: 0.0006411080248653889\n",
      "Iteration: 13860 Training Accuracy: 0.96875 Loss: 0.001218281569890678\n",
      "Iteration: 13870 Training Accuracy: 0.96875 Loss: 0.0014924678253009915\n",
      "Iteration: 13880 Training Accuracy: 0.96875 Loss: 0.0015754572814330459\n",
      "Iteration: 13890 Training Accuracy: 0.984375 Loss: 0.0011352101573720574\n",
      "Iteration: 13900 Training Accuracy: 1.0 Loss: 0.0009005022002384067\n",
      "Iteration: 13910 Training Accuracy: 1.0 Loss: 0.00040007210918702185\n",
      "Iteration: 13920 Training Accuracy: 1.0 Loss: 0.0006505263736471534\n",
      "Iteration: 13930 Training Accuracy: 0.96875 Loss: 0.0009715825435705483\n",
      "Iteration: 13940 Training Accuracy: 0.96875 Loss: 0.0015626170206815004\n",
      "Iteration: 13950 Training Accuracy: 1.0 Loss: 0.0004145030688960105\n",
      "Iteration: 13960 Training Accuracy: 1.0 Loss: 0.0005109034827910364\n",
      "Iteration: 13970 Training Accuracy: 1.0 Loss: 0.0004265974275767803\n",
      "Iteration: 13980 Training Accuracy: 1.0 Loss: 0.0008724790532141924\n",
      "Iteration: 13990 Training Accuracy: 1.0 Loss: 0.0009075856069102883\n",
      "Iteration: 14000 Training Accuracy: 1.0 Loss: 0.0005963635048829019\n",
      "Iteration: 14010 Training Accuracy: 0.96875 Loss: 0.0010218833340331912\n",
      "Iteration: 14020 Training Accuracy: 0.984375 Loss: 0.001277053146623075\n",
      "Iteration: 14030 Training Accuracy: 1.0 Loss: 0.0002502230345271528\n",
      "Iteration: 14040 Training Accuracy: 1.0 Loss: 0.0003297646762803197\n",
      "Iteration: 14050 Training Accuracy: 1.0 Loss: 0.0006521634059026837\n",
      "Iteration: 14060 Training Accuracy: 0.984375 Loss: 0.0009680701186880469\n",
      "Iteration: 14070 Training Accuracy: 1.0 Loss: 0.00047545009874738753\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9206666666666666\n",
      "epoch: 15\n",
      "Iteration: 14080 Training Accuracy: 0.984375 Loss: 0.001321484218351543\n",
      "Iteration: 14090 Training Accuracy: 1.0 Loss: 0.000602833169978112\n",
      "Iteration: 14100 Training Accuracy: 0.984375 Loss: 0.0010984212858602405\n",
      "Iteration: 14110 Training Accuracy: 0.984375 Loss: 0.0009885841282084584\n",
      "Iteration: 14120 Training Accuracy: 1.0 Loss: 0.0009017313132062554\n",
      "Iteration: 14130 Training Accuracy: 0.921875 Loss: 0.002957652322947979\n",
      "Iteration: 14140 Training Accuracy: 0.984375 Loss: 0.0017720566829666495\n",
      "Iteration: 14150 Training Accuracy: 0.96875 Loss: 0.001307204132899642\n",
      "Iteration: 14160 Training Accuracy: 0.953125 Loss: 0.0024725080002099276\n",
      "Iteration: 14170 Training Accuracy: 0.96875 Loss: 0.001451622461900115\n",
      "Iteration: 14180 Training Accuracy: 0.96875 Loss: 0.0010382317705079913\n",
      "Iteration: 14190 Training Accuracy: 0.984375 Loss: 0.0008552986546419561\n",
      "Iteration: 14200 Training Accuracy: 0.953125 Loss: 0.0020600040443241596\n",
      "Iteration: 14210 Training Accuracy: 0.921875 Loss: 0.003546640742570162\n",
      "Iteration: 14220 Training Accuracy: 1.0 Loss: 0.00045121338916942477\n",
      "Iteration: 14230 Training Accuracy: 1.0 Loss: 0.0008141660364344716\n",
      "Iteration: 14240 Training Accuracy: 0.96875 Loss: 0.0013083593221381307\n",
      "Iteration: 14250 Training Accuracy: 0.9375 Loss: 0.00245113018900156\n",
      "Iteration: 14260 Training Accuracy: 0.984375 Loss: 0.0006813707295805216\n",
      "Iteration: 14270 Training Accuracy: 0.984375 Loss: 0.001154295401647687\n",
      "Iteration: 14280 Training Accuracy: 0.984375 Loss: 0.0010020233457908034\n",
      "Iteration: 14290 Training Accuracy: 0.984375 Loss: 0.0017461333191022277\n",
      "Iteration: 14300 Training Accuracy: 0.984375 Loss: 0.001273331930860877\n",
      "Iteration: 14310 Training Accuracy: 0.96875 Loss: 0.0013313954696059227\n",
      "Iteration: 14320 Training Accuracy: 0.984375 Loss: 0.0010638185776770115\n",
      "Iteration: 14330 Training Accuracy: 0.984375 Loss: 0.0015759654343128204\n",
      "Iteration: 14340 Training Accuracy: 0.953125 Loss: 0.0020797550678253174\n",
      "Iteration: 14350 Training Accuracy: 0.96875 Loss: 0.0016264544101431966\n",
      "Iteration: 14360 Training Accuracy: 0.984375 Loss: 0.0012884305324405432\n",
      "Iteration: 14370 Training Accuracy: 0.953125 Loss: 0.0015796159859746695\n",
      "Iteration: 14380 Training Accuracy: 0.953125 Loss: 0.0027641761116683483\n",
      "Iteration: 14390 Training Accuracy: 0.984375 Loss: 0.0013423277996480465\n",
      "Iteration: 14400 Training Accuracy: 0.96875 Loss: 0.0011007983703166246\n",
      "Iteration: 14410 Training Accuracy: 0.984375 Loss: 0.0005350660067051649\n",
      "Iteration: 14420 Training Accuracy: 0.984375 Loss: 0.0016715677920728922\n",
      "Iteration: 14430 Training Accuracy: 0.96875 Loss: 0.0011603611055761576\n",
      "Iteration: 14440 Training Accuracy: 0.96875 Loss: 0.0019430428510531783\n",
      "Iteration: 14450 Training Accuracy: 0.96875 Loss: 0.0008999078418128192\n",
      "Iteration: 14460 Training Accuracy: 0.984375 Loss: 0.0007664267905056477\n",
      "Iteration: 14470 Training Accuracy: 0.984375 Loss: 0.0008746847743168473\n",
      "Iteration: 14480 Training Accuracy: 1.0 Loss: 0.0006646333495154977\n",
      "Iteration: 14490 Training Accuracy: 1.0 Loss: 0.0006196750327944756\n",
      "Iteration: 14500 Training Accuracy: 0.96875 Loss: 0.0009967697551473975\n",
      "Iteration: 14510 Training Accuracy: 0.984375 Loss: 0.00122540770098567\n",
      "Iteration: 14520 Training Accuracy: 0.96875 Loss: 0.0012044166214764118\n",
      "Iteration: 14530 Training Accuracy: 0.984375 Loss: 0.0006043572211638093\n",
      "Iteration: 14540 Training Accuracy: 0.953125 Loss: 0.0013661059783771634\n",
      "Iteration: 14550 Training Accuracy: 0.984375 Loss: 0.0008591761579737067\n",
      "Iteration: 14560 Training Accuracy: 1.0 Loss: 0.000624617503490299\n",
      "Iteration: 14570 Training Accuracy: 0.984375 Loss: 0.001980299362912774\n",
      "Iteration: 14580 Training Accuracy: 0.984375 Loss: 0.0013557237107306719\n",
      "Iteration: 14590 Training Accuracy: 0.96875 Loss: 0.0010093484306707978\n",
      "Iteration: 14600 Training Accuracy: 1.0 Loss: 6.944310007384047e-05\n",
      "Iteration: 14610 Training Accuracy: 0.953125 Loss: 0.0016394462436437607\n",
      "Iteration: 14620 Training Accuracy: 0.96875 Loss: 0.0010086920810863376\n",
      "Iteration: 14630 Training Accuracy: 1.0 Loss: 0.000380382698494941\n",
      "Iteration: 14640 Training Accuracy: 0.96875 Loss: 0.0014114725636318326\n",
      "Iteration: 14650 Training Accuracy: 0.96875 Loss: 0.0010496324393898249\n",
      "Iteration: 14660 Training Accuracy: 1.0 Loss: 0.0003922863397747278\n",
      "Iteration: 14670 Training Accuracy: 0.96875 Loss: 0.001411172212101519\n",
      "Iteration: 14680 Training Accuracy: 1.0 Loss: 0.00040037522558122873\n",
      "Iteration: 14690 Training Accuracy: 0.953125 Loss: 0.0017431352753192186\n",
      "Iteration: 14700 Training Accuracy: 1.0 Loss: 0.000538959982804954\n",
      "Iteration: 14710 Training Accuracy: 1.0 Loss: 0.0010100206127390265\n",
      "Iteration: 14720 Training Accuracy: 1.0 Loss: 0.0006298258667811751\n",
      "Iteration: 14730 Training Accuracy: 0.984375 Loss: 0.001310204155743122\n",
      "Iteration: 14740 Training Accuracy: 1.0 Loss: 0.0005378414643928409\n",
      "Iteration: 14750 Training Accuracy: 0.984375 Loss: 0.0014396504266187549\n",
      "Iteration: 14760 Training Accuracy: 1.0 Loss: 0.00042403003317303956\n",
      "Iteration: 14770 Training Accuracy: 1.0 Loss: 0.000716990907676518\n",
      "Iteration: 14780 Training Accuracy: 1.0 Loss: 0.0005888862069696188\n",
      "Iteration: 14790 Training Accuracy: 1.0 Loss: 0.0005727674579247832\n",
      "Iteration: 14800 Training Accuracy: 0.96875 Loss: 0.0010991274612024426\n",
      "Iteration: 14810 Training Accuracy: 1.0 Loss: 0.00037250638706609607\n",
      "Iteration: 14820 Training Accuracy: 0.96875 Loss: 0.0012108493829146028\n",
      "Iteration: 14830 Training Accuracy: 1.0 Loss: 0.0008124120067805052\n",
      "Iteration: 14840 Training Accuracy: 0.984375 Loss: 0.0015641578938812017\n",
      "Iteration: 14850 Training Accuracy: 0.96875 Loss: 0.001755874720402062\n",
      "Iteration: 14860 Training Accuracy: 1.0 Loss: 0.00031912169652059674\n",
      "Iteration: 14870 Training Accuracy: 0.984375 Loss: 0.0011120426934212446\n",
      "Iteration: 14880 Training Accuracy: 1.0 Loss: 0.0011731452541425824\n",
      "Iteration: 14890 Training Accuracy: 0.96875 Loss: 0.0011853512842208147\n",
      "Iteration: 14900 Training Accuracy: 0.953125 Loss: 0.0015302111860364676\n",
      "Iteration: 14910 Training Accuracy: 0.96875 Loss: 0.0011001063976436853\n",
      "Iteration: 14920 Training Accuracy: 1.0 Loss: 0.00017182865121867508\n",
      "Iteration: 14930 Training Accuracy: 1.0 Loss: 0.00042095285607501864\n",
      "Iteration: 14940 Training Accuracy: 1.0 Loss: 0.0002393759787082672\n",
      "Iteration: 14950 Training Accuracy: 0.984375 Loss: 0.0008716973825357854\n",
      "Iteration: 14960 Training Accuracy: 1.0 Loss: 0.0006590051343664527\n",
      "Iteration: 14970 Training Accuracy: 1.0 Loss: 0.0004104758845642209\n",
      "Iteration: 14980 Training Accuracy: 0.984375 Loss: 0.0009371269261464477\n",
      "Iteration: 14990 Training Accuracy: 0.984375 Loss: 0.0006818920955993235\n",
      "Iteration: 15000 Training Accuracy: 1.0 Loss: 0.0007251129718497396\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9215\n",
      "epoch: 16\n",
      "Iteration: 15010 Training Accuracy: 0.953125 Loss: 0.002508528996258974\n",
      "Iteration: 15020 Training Accuracy: 0.953125 Loss: 0.0021624695509672165\n",
      "Iteration: 15030 Training Accuracy: 0.9375 Loss: 0.0025948621332645416\n",
      "Iteration: 15040 Training Accuracy: 0.984375 Loss: 0.001032778061926365\n",
      "Iteration: 15050 Training Accuracy: 0.953125 Loss: 0.002156591974198818\n",
      "Iteration: 15060 Training Accuracy: 1.0 Loss: 0.00068810791708529\n",
      "Iteration: 15070 Training Accuracy: 1.0 Loss: 0.0003932912368327379\n",
      "Iteration: 15080 Training Accuracy: 0.984375 Loss: 0.0008332174620591104\n",
      "Iteration: 15090 Training Accuracy: 0.96875 Loss: 0.0008209740626625717\n",
      "Iteration: 15100 Training Accuracy: 1.0 Loss: 0.0006183098303154111\n",
      "Iteration: 15110 Training Accuracy: 0.984375 Loss: 0.0019132352899760008\n",
      "Iteration: 15120 Training Accuracy: 0.9375 Loss: 0.002913964446634054\n",
      "Iteration: 15130 Training Accuracy: 1.0 Loss: 0.0006252072635106742\n",
      "Iteration: 15140 Training Accuracy: 0.9375 Loss: 0.002918623387813568\n",
      "Iteration: 15150 Training Accuracy: 0.984375 Loss: 0.0012946613132953644\n",
      "Iteration: 15160 Training Accuracy: 1.0 Loss: 0.0010647086892277002\n",
      "Iteration: 15170 Training Accuracy: 0.984375 Loss: 0.0016257287934422493\n",
      "Iteration: 15180 Training Accuracy: 0.96875 Loss: 0.0013049212284386158\n",
      "Iteration: 15190 Training Accuracy: 0.984375 Loss: 0.0011925074504688382\n",
      "Iteration: 15200 Training Accuracy: 1.0 Loss: 0.0005766306421719491\n",
      "Iteration: 15210 Training Accuracy: 1.0 Loss: 0.00029448632267303765\n",
      "Iteration: 15220 Training Accuracy: 1.0 Loss: 0.0005792338633909822\n",
      "Iteration: 15230 Training Accuracy: 0.984375 Loss: 0.0010164655977860093\n",
      "Iteration: 15240 Training Accuracy: 0.96875 Loss: 0.0013768108328804374\n",
      "Iteration: 15250 Training Accuracy: 1.0 Loss: 0.0003130456025246531\n",
      "Iteration: 15260 Training Accuracy: 0.96875 Loss: 0.0012927930802106857\n",
      "Iteration: 15270 Training Accuracy: 0.984375 Loss: 0.0022704852744936943\n",
      "Iteration: 15280 Training Accuracy: 0.96875 Loss: 0.0011790517019107938\n",
      "Iteration: 15290 Training Accuracy: 0.953125 Loss: 0.0030584041960537434\n",
      "Iteration: 15300 Training Accuracy: 1.0 Loss: 0.000657222350127995\n",
      "Iteration: 15310 Training Accuracy: 1.0 Loss: 0.00019413043628446758\n",
      "Iteration: 15320 Training Accuracy: 0.9375 Loss: 0.0022030919790267944\n",
      "Iteration: 15330 Training Accuracy: 0.953125 Loss: 0.0014130065683275461\n",
      "Iteration: 15340 Training Accuracy: 0.984375 Loss: 0.0005897631635889411\n",
      "Iteration: 15350 Training Accuracy: 0.984375 Loss: 0.0005366250989027321\n",
      "Iteration: 15360 Training Accuracy: 0.96875 Loss: 0.002205934375524521\n",
      "Iteration: 15370 Training Accuracy: 1.0 Loss: 0.000703718513250351\n",
      "Iteration: 15380 Training Accuracy: 0.9375 Loss: 0.0012758532539010048\n",
      "Iteration: 15390 Training Accuracy: 0.96875 Loss: 0.0013842973858118057\n",
      "Iteration: 15400 Training Accuracy: 1.0 Loss: 0.0008104680455289781\n",
      "Iteration: 15410 Training Accuracy: 0.953125 Loss: 0.0022697579115629196\n",
      "Iteration: 15420 Training Accuracy: 1.0 Loss: 0.0011757930042222142\n",
      "Iteration: 15430 Training Accuracy: 0.96875 Loss: 0.0013681973796337843\n",
      "Iteration: 15440 Training Accuracy: 0.984375 Loss: 0.0008921250700950623\n",
      "Iteration: 15450 Training Accuracy: 1.0 Loss: 0.00045346823753789067\n",
      "Iteration: 15460 Training Accuracy: 0.984375 Loss: 0.0013181011890992522\n",
      "Iteration: 15470 Training Accuracy: 0.96875 Loss: 0.0017028306610882282\n",
      "Iteration: 15480 Training Accuracy: 0.984375 Loss: 0.0010415723081678152\n",
      "Iteration: 15490 Training Accuracy: 1.0 Loss: 0.00045635062269866467\n",
      "Iteration: 15500 Training Accuracy: 1.0 Loss: 0.000677664065733552\n",
      "Iteration: 15510 Training Accuracy: 0.984375 Loss: 0.0009617116302251816\n",
      "Iteration: 15520 Training Accuracy: 0.984375 Loss: 0.0010827912483364344\n",
      "Iteration: 15530 Training Accuracy: 1.0 Loss: 0.0007051517022773623\n",
      "Iteration: 15540 Training Accuracy: 0.953125 Loss: 0.0015715467743575573\n",
      "Iteration: 15550 Training Accuracy: 1.0 Loss: 0.0004039494087919593\n",
      "Iteration: 15560 Training Accuracy: 1.0 Loss: 0.0005593507667072117\n",
      "Iteration: 15570 Training Accuracy: 0.984375 Loss: 0.00068430055398494\n",
      "Iteration: 15580 Training Accuracy: 1.0 Loss: 0.0003179980849381536\n",
      "Iteration: 15590 Training Accuracy: 0.984375 Loss: 0.0009804442524909973\n",
      "Iteration: 15600 Training Accuracy: 1.0 Loss: 0.0003771122428588569\n",
      "Iteration: 15610 Training Accuracy: 1.0 Loss: 0.0013368205400183797\n",
      "Iteration: 15620 Training Accuracy: 0.984375 Loss: 0.0015500838635489345\n",
      "Iteration: 15630 Training Accuracy: 0.96875 Loss: 0.001214976073242724\n",
      "Iteration: 15640 Training Accuracy: 0.984375 Loss: 0.0008201926830224693\n",
      "Iteration: 15650 Training Accuracy: 1.0 Loss: 0.0004099455545656383\n",
      "Iteration: 15660 Training Accuracy: 0.984375 Loss: 0.0010738453129306436\n",
      "Iteration: 15670 Training Accuracy: 1.0 Loss: 0.00018871818610932678\n",
      "Iteration: 15680 Training Accuracy: 0.96875 Loss: 0.0012971818214282393\n",
      "Iteration: 15690 Training Accuracy: 0.96875 Loss: 0.0014388056006282568\n",
      "Iteration: 15700 Training Accuracy: 0.984375 Loss: 0.0006613954319618642\n",
      "Iteration: 15710 Training Accuracy: 0.984375 Loss: 0.0009164457442238927\n",
      "Iteration: 15720 Training Accuracy: 0.984375 Loss: 0.0010493019362911582\n",
      "Iteration: 15730 Training Accuracy: 0.96875 Loss: 0.0010469595436006784\n",
      "Iteration: 15740 Training Accuracy: 0.953125 Loss: 0.0014149912167340517\n",
      "Iteration: 15750 Training Accuracy: 0.984375 Loss: 0.0005799863720312715\n",
      "Iteration: 15760 Training Accuracy: 1.0 Loss: 0.0006115656578913331\n",
      "Iteration: 15770 Training Accuracy: 1.0 Loss: 0.0006766370497643948\n",
      "Iteration: 15780 Training Accuracy: 1.0 Loss: 0.0010110549628734589\n",
      "Iteration: 15790 Training Accuracy: 0.96875 Loss: 0.002441234653815627\n",
      "Iteration: 15800 Training Accuracy: 0.984375 Loss: 0.0006718799122609198\n",
      "Iteration: 15810 Training Accuracy: 1.0 Loss: 0.0004834431456401944\n",
      "Iteration: 15820 Training Accuracy: 0.984375 Loss: 0.0008630929514765739\n",
      "Iteration: 15830 Training Accuracy: 0.984375 Loss: 0.0007279490819200873\n",
      "Iteration: 15840 Training Accuracy: 0.984375 Loss: 0.001893229316920042\n",
      "Iteration: 15850 Training Accuracy: 0.984375 Loss: 0.0010675295488908887\n",
      "Iteration: 15860 Training Accuracy: 0.96875 Loss: 0.0017210678197443485\n",
      "Iteration: 15870 Training Accuracy: 1.0 Loss: 0.0005566691979765892\n",
      "Iteration: 15880 Training Accuracy: 1.0 Loss: 0.00019453269487712532\n",
      "Iteration: 15890 Training Accuracy: 0.96875 Loss: 0.0009189790580421686\n",
      "Iteration: 15900 Training Accuracy: 0.984375 Loss: 0.0011306972010061145\n",
      "Iteration: 15910 Training Accuracy: 0.984375 Loss: 0.0009006267646327615\n",
      "Iteration: 15920 Training Accuracy: 1.0 Loss: 0.0010765967890620232\n",
      "Iteration: 15930 Training Accuracy: 1.0 Loss: 0.00041327570215798914\n",
      "Iteration: 15940 Training Accuracy: 1.0 Loss: 0.00044636937673203647\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9218333333333333\n",
      "epoch: 17\n",
      "Iteration: 15950 Training Accuracy: 0.953125 Loss: 0.002495657652616501\n",
      "Iteration: 15960 Training Accuracy: 0.96875 Loss: 0.000752923428080976\n",
      "Iteration: 15970 Training Accuracy: 1.0 Loss: 0.0004368627560324967\n",
      "Iteration: 15980 Training Accuracy: 1.0 Loss: 0.0008940116968005896\n",
      "Iteration: 15990 Training Accuracy: 1.0 Loss: 0.00047772127436473966\n",
      "Iteration: 16000 Training Accuracy: 0.953125 Loss: 0.002495991997420788\n",
      "Iteration: 16010 Training Accuracy: 0.984375 Loss: 0.0008281688205897808\n",
      "Iteration: 16020 Training Accuracy: 0.9375 Loss: 0.0024879046250134706\n",
      "Iteration: 16030 Training Accuracy: 0.96875 Loss: 0.0009423884330317378\n",
      "Iteration: 16040 Training Accuracy: 1.0 Loss: 0.0003983832721132785\n",
      "Iteration: 16050 Training Accuracy: 0.96875 Loss: 0.0014961953274905682\n",
      "Iteration: 16060 Training Accuracy: 0.9375 Loss: 0.0019317278638482094\n",
      "Iteration: 16070 Training Accuracy: 0.984375 Loss: 0.0012906917836517096\n",
      "Iteration: 16080 Training Accuracy: 0.96875 Loss: 0.001994890160858631\n",
      "Iteration: 16090 Training Accuracy: 1.0 Loss: 0.0006338395178318024\n",
      "Iteration: 16100 Training Accuracy: 0.984375 Loss: 0.0010724110761657357\n",
      "Iteration: 16110 Training Accuracy: 0.96875 Loss: 0.0016553431050851941\n",
      "Iteration: 16120 Training Accuracy: 0.984375 Loss: 0.000562923785764724\n",
      "Iteration: 16130 Training Accuracy: 0.984375 Loss: 0.0013707884354516864\n",
      "Iteration: 16140 Training Accuracy: 0.984375 Loss: 0.001085270312614739\n",
      "Iteration: 16150 Training Accuracy: 0.984375 Loss: 0.00039756123442202806\n",
      "Iteration: 16160 Training Accuracy: 0.953125 Loss: 0.001585905672982335\n",
      "Iteration: 16170 Training Accuracy: 1.0 Loss: 0.0005412810132838786\n",
      "Iteration: 16180 Training Accuracy: 0.953125 Loss: 0.002161572687327862\n",
      "Iteration: 16190 Training Accuracy: 1.0 Loss: 0.0007510158466175199\n",
      "Iteration: 16200 Training Accuracy: 0.953125 Loss: 0.001341475173830986\n",
      "Iteration: 16210 Training Accuracy: 0.96875 Loss: 0.0008993354858830571\n",
      "Iteration: 16220 Training Accuracy: 1.0 Loss: 0.0006498442380689085\n",
      "Iteration: 16230 Training Accuracy: 0.984375 Loss: 0.0024653624277561903\n",
      "Iteration: 16240 Training Accuracy: 0.96875 Loss: 0.0013862961204722524\n",
      "Iteration: 16250 Training Accuracy: 1.0 Loss: 0.0004637149104382843\n",
      "Iteration: 16260 Training Accuracy: 0.984375 Loss: 0.0011421566596254706\n",
      "Iteration: 16270 Training Accuracy: 1.0 Loss: 0.0010674011427909136\n",
      "Iteration: 16280 Training Accuracy: 0.96875 Loss: 0.0017272436525672674\n",
      "Iteration: 16290 Training Accuracy: 1.0 Loss: 0.000882732798345387\n",
      "Iteration: 16300 Training Accuracy: 1.0 Loss: 0.0003468440263532102\n",
      "Iteration: 16310 Training Accuracy: 1.0 Loss: 0.0009232955053448677\n",
      "Iteration: 16320 Training Accuracy: 0.984375 Loss: 0.0010268802288919687\n",
      "Iteration: 16330 Training Accuracy: 0.953125 Loss: 0.002475576940923929\n",
      "Iteration: 16340 Training Accuracy: 0.96875 Loss: 0.0009037755662575364\n",
      "Iteration: 16350 Training Accuracy: 0.984375 Loss: 0.0005117971450090408\n",
      "Iteration: 16360 Training Accuracy: 0.96875 Loss: 0.0013315139804035425\n",
      "Iteration: 16370 Training Accuracy: 0.96875 Loss: 0.0008455562638118863\n",
      "Iteration: 16380 Training Accuracy: 0.96875 Loss: 0.001082681817933917\n",
      "Iteration: 16390 Training Accuracy: 0.96875 Loss: 0.0022024589125066996\n",
      "Iteration: 16400 Training Accuracy: 1.0 Loss: 0.0007804087363183498\n",
      "Iteration: 16410 Training Accuracy: 0.984375 Loss: 0.000941126374527812\n",
      "Iteration: 16420 Training Accuracy: 0.96875 Loss: 0.0006443954771384597\n",
      "Iteration: 16430 Training Accuracy: 1.0 Loss: 0.0010116815101355314\n",
      "Iteration: 16440 Training Accuracy: 0.953125 Loss: 0.001971887657418847\n",
      "Iteration: 16450 Training Accuracy: 0.96875 Loss: 0.002332426141947508\n",
      "Iteration: 16460 Training Accuracy: 0.96875 Loss: 0.0011039047967642546\n",
      "Iteration: 16470 Training Accuracy: 0.953125 Loss: 0.001733934972435236\n",
      "Iteration: 16480 Training Accuracy: 0.984375 Loss: 0.0017855383921414614\n",
      "Iteration: 16490 Training Accuracy: 0.953125 Loss: 0.00346015184186399\n",
      "Iteration: 16500 Training Accuracy: 0.96875 Loss: 0.001921301824040711\n",
      "Iteration: 16510 Training Accuracy: 0.984375 Loss: 0.0009385706507600844\n",
      "Iteration: 16520 Training Accuracy: 0.984375 Loss: 0.0013483852380886674\n",
      "Iteration: 16530 Training Accuracy: 1.0 Loss: 0.0004640502738766372\n",
      "Iteration: 16540 Training Accuracy: 0.953125 Loss: 0.00143156829290092\n",
      "Iteration: 16550 Training Accuracy: 0.953125 Loss: 0.002425492275506258\n",
      "Iteration: 16560 Training Accuracy: 0.984375 Loss: 0.0010674985824152827\n",
      "Iteration: 16570 Training Accuracy: 0.984375 Loss: 0.0020396243780851364\n",
      "Iteration: 16580 Training Accuracy: 0.984375 Loss: 0.0009164483635686338\n",
      "Iteration: 16590 Training Accuracy: 1.0 Loss: 0.00045601819874718785\n",
      "Iteration: 16600 Training Accuracy: 1.0 Loss: 0.0006261584931053221\n",
      "Iteration: 16610 Training Accuracy: 0.984375 Loss: 0.0007611394394189119\n",
      "Iteration: 16620 Training Accuracy: 0.984375 Loss: 0.0005805211258120835\n",
      "Iteration: 16630 Training Accuracy: 1.0 Loss: 0.0004786740173585713\n",
      "Iteration: 16640 Training Accuracy: 0.984375 Loss: 0.0009586550295352936\n",
      "Iteration: 16650 Training Accuracy: 1.0 Loss: 0.0011151956859976053\n",
      "Iteration: 16660 Training Accuracy: 0.984375 Loss: 0.0005091169150546193\n",
      "Iteration: 16670 Training Accuracy: 0.984375 Loss: 0.0014732576673850417\n",
      "Iteration: 16680 Training Accuracy: 1.0 Loss: 0.0005149258067831397\n",
      "Iteration: 16690 Training Accuracy: 0.96875 Loss: 0.0015818354440853\n",
      "Iteration: 16700 Training Accuracy: 1.0 Loss: 0.0010038017062470317\n",
      "Iteration: 16710 Training Accuracy: 0.96875 Loss: 0.0016854002606123686\n",
      "Iteration: 16720 Training Accuracy: 0.984375 Loss: 0.0010640763211995363\n",
      "Iteration: 16730 Training Accuracy: 0.984375 Loss: 0.001227696891874075\n",
      "Iteration: 16740 Training Accuracy: 0.96875 Loss: 0.0007714548846706748\n",
      "Iteration: 16750 Training Accuracy: 0.984375 Loss: 0.0012356244260445237\n",
      "Iteration: 16760 Training Accuracy: 0.984375 Loss: 0.0009958769660443068\n",
      "Iteration: 16770 Training Accuracy: 0.984375 Loss: 0.0008865332929417491\n",
      "Iteration: 16780 Training Accuracy: 0.984375 Loss: 0.0009795688092708588\n",
      "Iteration: 16790 Training Accuracy: 0.9375 Loss: 0.0023056543432176113\n",
      "Iteration: 16800 Training Accuracy: 0.96875 Loss: 0.0012997861485928297\n",
      "Iteration: 16810 Training Accuracy: 0.984375 Loss: 0.0006480918382294476\n",
      "Iteration: 16820 Training Accuracy: 1.0 Loss: 0.0009537179139442742\n",
      "Iteration: 16830 Training Accuracy: 0.984375 Loss: 0.0010035088052973151\n",
      "Iteration: 16840 Training Accuracy: 0.984375 Loss: 0.0014396482147276402\n",
      "Iteration: 16850 Training Accuracy: 0.984375 Loss: 0.0008825914701446891\n",
      "Iteration: 16860 Training Accuracy: 1.0 Loss: 0.0010281938593834639\n",
      "Iteration: 16870 Training Accuracy: 0.96875 Loss: 0.001263823127374053\n",
      "Iteration: 16880 Training Accuracy: 0.984375 Loss: 0.0004896653699688613\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9216666666666666\n",
      "epoch: 18\n",
      "Iteration: 16890 Training Accuracy: 1.0 Loss: 0.0008415350457653403\n",
      "Iteration: 16900 Training Accuracy: 0.96875 Loss: 0.001897252630442381\n",
      "Iteration: 16910 Training Accuracy: 1.0 Loss: 0.0006921866443008184\n",
      "Iteration: 16920 Training Accuracy: 0.984375 Loss: 0.0010105533292517066\n",
      "Iteration: 16930 Training Accuracy: 1.0 Loss: 0.0006187758408486843\n",
      "Iteration: 16940 Training Accuracy: 0.984375 Loss: 0.0010015707230195403\n",
      "Iteration: 16950 Training Accuracy: 0.953125 Loss: 0.0021960530430078506\n",
      "Iteration: 16960 Training Accuracy: 0.96875 Loss: 0.002127290004864335\n",
      "Iteration: 16970 Training Accuracy: 1.0 Loss: 0.0006779574323445559\n",
      "Iteration: 16980 Training Accuracy: 0.96875 Loss: 0.0013577016070485115\n",
      "Iteration: 16990 Training Accuracy: 0.96875 Loss: 0.0010011596605181694\n",
      "Iteration: 17000 Training Accuracy: 0.984375 Loss: 0.0008781716460362077\n",
      "Iteration: 17010 Training Accuracy: 0.984375 Loss: 0.001023822114802897\n",
      "Iteration: 17020 Training Accuracy: 1.0 Loss: 0.0005743833025917411\n",
      "Iteration: 17030 Training Accuracy: 1.0 Loss: 0.000495579035487026\n",
      "Iteration: 17040 Training Accuracy: 0.96875 Loss: 0.0020801429636776447\n",
      "Iteration: 17050 Training Accuracy: 1.0 Loss: 0.0006639509811066091\n",
      "Iteration: 17060 Training Accuracy: 0.984375 Loss: 0.0008312049903906882\n",
      "Iteration: 17070 Training Accuracy: 1.0 Loss: 0.00036806915886700153\n",
      "Iteration: 17080 Training Accuracy: 0.984375 Loss: 0.000492299092002213\n",
      "Iteration: 17090 Training Accuracy: 1.0 Loss: 0.0007282480364665389\n",
      "Iteration: 17100 Training Accuracy: 0.953125 Loss: 0.0033747656270861626\n",
      "Iteration: 17110 Training Accuracy: 0.984375 Loss: 0.0007517496705986559\n",
      "Iteration: 17120 Training Accuracy: 0.984375 Loss: 0.0008249415550380945\n",
      "Iteration: 17130 Training Accuracy: 1.0 Loss: 0.0007183869602158666\n",
      "Iteration: 17140 Training Accuracy: 0.984375 Loss: 0.0009243427193723619\n",
      "Iteration: 17150 Training Accuracy: 0.984375 Loss: 0.001501769875176251\n",
      "Iteration: 17160 Training Accuracy: 1.0 Loss: 0.0008445504354313016\n",
      "Iteration: 17170 Training Accuracy: 1.0 Loss: 0.00036979056312702596\n",
      "Iteration: 17180 Training Accuracy: 0.96875 Loss: 0.0011533041251823306\n",
      "Iteration: 17190 Training Accuracy: 1.0 Loss: 0.00037206150591373444\n",
      "Iteration: 17200 Training Accuracy: 0.984375 Loss: 0.0008121713763102889\n",
      "Iteration: 17210 Training Accuracy: 1.0 Loss: 0.0003328790480736643\n",
      "Iteration: 17220 Training Accuracy: 0.96875 Loss: 0.0015982645563781261\n",
      "Iteration: 17230 Training Accuracy: 1.0 Loss: 0.0006136885494925082\n",
      "Iteration: 17240 Training Accuracy: 0.984375 Loss: 0.0007177721709012985\n",
      "Iteration: 17250 Training Accuracy: 0.96875 Loss: 0.0007223151624202728\n",
      "Iteration: 17260 Training Accuracy: 0.984375 Loss: 0.0011192613746970892\n",
      "Iteration: 17270 Training Accuracy: 0.96875 Loss: 0.0018683780217543244\n",
      "Iteration: 17280 Training Accuracy: 0.984375 Loss: 0.0013686311431229115\n",
      "Iteration: 17290 Training Accuracy: 0.984375 Loss: 0.0005691218539141119\n",
      "Iteration: 17300 Training Accuracy: 1.0 Loss: 0.0008954735239967704\n",
      "Iteration: 17310 Training Accuracy: 1.0 Loss: 0.0004941703518852592\n",
      "Iteration: 17320 Training Accuracy: 0.984375 Loss: 0.0011793291196227074\n",
      "Iteration: 17330 Training Accuracy: 0.984375 Loss: 0.0010815782006829977\n",
      "Iteration: 17340 Training Accuracy: 0.984375 Loss: 0.0007846037042327225\n",
      "Iteration: 17350 Training Accuracy: 0.953125 Loss: 0.0012503196485340595\n",
      "Iteration: 17360 Training Accuracy: 0.953125 Loss: 0.0018044005846604705\n",
      "Iteration: 17370 Training Accuracy: 0.984375 Loss: 0.0009168099495582283\n",
      "Iteration: 17380 Training Accuracy: 0.984375 Loss: 0.0005531209753826261\n",
      "Iteration: 17390 Training Accuracy: 0.984375 Loss: 0.0008869646699167788\n",
      "Iteration: 17400 Training Accuracy: 0.984375 Loss: 0.0012193297734484076\n",
      "Iteration: 17410 Training Accuracy: 0.984375 Loss: 0.0008656505960971117\n",
      "Iteration: 17420 Training Accuracy: 1.0 Loss: 0.000437568174675107\n",
      "Iteration: 17430 Training Accuracy: 1.0 Loss: 0.0010531626176089048\n",
      "Iteration: 17440 Training Accuracy: 1.0 Loss: 0.0005133321392349899\n",
      "Iteration: 17450 Training Accuracy: 1.0 Loss: 0.00040256508509628475\n",
      "Iteration: 17460 Training Accuracy: 0.96875 Loss: 0.0009307551081292331\n",
      "Iteration: 17470 Training Accuracy: 0.984375 Loss: 0.001055962173268199\n",
      "Iteration: 17480 Training Accuracy: 0.984375 Loss: 0.0011347898980602622\n",
      "Iteration: 17490 Training Accuracy: 0.984375 Loss: 0.0016312221996486187\n",
      "Iteration: 17500 Training Accuracy: 1.0 Loss: 0.000421893666498363\n",
      "Iteration: 17510 Training Accuracy: 0.96875 Loss: 0.0012391484342515469\n",
      "Iteration: 17520 Training Accuracy: 0.984375 Loss: 0.0008976064855232835\n",
      "Iteration: 17530 Training Accuracy: 0.96875 Loss: 0.001548759639263153\n",
      "Iteration: 17540 Training Accuracy: 1.0 Loss: 0.0005020034732297063\n",
      "Iteration: 17550 Training Accuracy: 1.0 Loss: 0.0007413902785629034\n",
      "Iteration: 17560 Training Accuracy: 0.984375 Loss: 0.0009267405839636922\n",
      "Iteration: 17570 Training Accuracy: 0.953125 Loss: 0.0015490431105718017\n",
      "Iteration: 17580 Training Accuracy: 1.0 Loss: 0.000555106089450419\n",
      "Iteration: 17590 Training Accuracy: 0.984375 Loss: 0.0006181243225000799\n",
      "Iteration: 17600 Training Accuracy: 0.984375 Loss: 0.0015393063658848405\n",
      "Iteration: 17610 Training Accuracy: 1.0 Loss: 0.0004059936327394098\n",
      "Iteration: 17620 Training Accuracy: 0.953125 Loss: 0.0016622315160930157\n",
      "Iteration: 17630 Training Accuracy: 0.984375 Loss: 0.001414816826581955\n",
      "Iteration: 17640 Training Accuracy: 1.0 Loss: 0.0007849844987504184\n",
      "Iteration: 17650 Training Accuracy: 1.0 Loss: 0.0009831900242716074\n",
      "Iteration: 17660 Training Accuracy: 0.96875 Loss: 0.0012636431492865086\n",
      "Iteration: 17670 Training Accuracy: 1.0 Loss: 0.0008281393675133586\n",
      "Iteration: 17680 Training Accuracy: 1.0 Loss: 0.000565841794013977\n",
      "Iteration: 17690 Training Accuracy: 0.96875 Loss: 0.0011389930732548237\n",
      "Iteration: 17700 Training Accuracy: 0.984375 Loss: 0.0013826950453221798\n",
      "Iteration: 17710 Training Accuracy: 0.96875 Loss: 0.002003835281357169\n",
      "Iteration: 17720 Training Accuracy: 0.984375 Loss: 0.0007684263400733471\n",
      "Iteration: 17730 Training Accuracy: 1.0 Loss: 0.0004341116873547435\n",
      "Iteration: 17740 Training Accuracy: 0.96875 Loss: 0.0013291637878865004\n",
      "Iteration: 17750 Training Accuracy: 1.0 Loss: 0.0007171394536271691\n",
      "Iteration: 17760 Training Accuracy: 0.984375 Loss: 0.0010811469983309507\n",
      "Iteration: 17770 Training Accuracy: 0.9375 Loss: 0.002598155289888382\n",
      "Iteration: 17780 Training Accuracy: 1.0 Loss: 0.0005337476031854749\n",
      "Iteration: 17790 Training Accuracy: 1.0 Loss: 0.0007255586097016931\n",
      "Iteration: 17800 Training Accuracy: 0.984375 Loss: 0.0012769673485308886\n",
      "Iteration: 17810 Training Accuracy: 0.953125 Loss: 0.0019200409296900034\n",
      "Iteration: 17820 Training Accuracy: 0.984375 Loss: 0.0006183729856275022\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9216666666666666\n",
      "epoch: 19\n",
      "Iteration: 17830 Training Accuracy: 1.0 Loss: 0.0003951441030949354\n",
      "Iteration: 17840 Training Accuracy: 1.0 Loss: 0.00044992321636527777\n",
      "Iteration: 17850 Training Accuracy: 0.96875 Loss: 0.001382038346491754\n",
      "Iteration: 17860 Training Accuracy: 0.984375 Loss: 0.0007515851175412536\n",
      "Iteration: 17870 Training Accuracy: 0.96875 Loss: 0.0015995136927813292\n",
      "Iteration: 17880 Training Accuracy: 0.984375 Loss: 0.0008202672470360994\n",
      "Iteration: 17890 Training Accuracy: 0.953125 Loss: 0.0014043623814359307\n",
      "Iteration: 17900 Training Accuracy: 0.984375 Loss: 0.0013894218718633056\n",
      "Iteration: 17910 Training Accuracy: 0.984375 Loss: 0.0012711288873106241\n",
      "Iteration: 17920 Training Accuracy: 0.984375 Loss: 0.0012938943691551685\n",
      "Iteration: 17930 Training Accuracy: 1.0 Loss: 0.00037675799103453755\n",
      "Iteration: 17940 Training Accuracy: 0.9375 Loss: 0.0022724620066583157\n",
      "Iteration: 17950 Training Accuracy: 0.953125 Loss: 0.0016467012465000153\n",
      "Iteration: 17960 Training Accuracy: 0.96875 Loss: 0.0010323460446670651\n",
      "Iteration: 17970 Training Accuracy: 0.96875 Loss: 0.002054806100204587\n",
      "Iteration: 17980 Training Accuracy: 0.96875 Loss: 0.001242650905624032\n",
      "Iteration: 17990 Training Accuracy: 0.984375 Loss: 0.0009106054785661399\n",
      "Iteration: 18000 Training Accuracy: 0.984375 Loss: 0.0006402393919415772\n",
      "Iteration: 18010 Training Accuracy: 0.984375 Loss: 0.0010014225263148546\n",
      "Iteration: 18020 Training Accuracy: 1.0 Loss: 0.00033451156923547387\n",
      "Iteration: 18030 Training Accuracy: 0.984375 Loss: 0.0006320330430753529\n",
      "Iteration: 18040 Training Accuracy: 1.0 Loss: 0.0009470632066950202\n",
      "Iteration: 18050 Training Accuracy: 0.96875 Loss: 0.0013943160884082317\n",
      "Iteration: 18060 Training Accuracy: 0.96875 Loss: 0.0011470485478639603\n",
      "Iteration: 18070 Training Accuracy: 0.984375 Loss: 0.00124719412997365\n",
      "Iteration: 18080 Training Accuracy: 0.96875 Loss: 0.0011402892414480448\n",
      "Iteration: 18090 Training Accuracy: 0.953125 Loss: 0.0018956551793962717\n",
      "Iteration: 18100 Training Accuracy: 0.921875 Loss: 0.0019291626522317529\n",
      "Iteration: 18110 Training Accuracy: 0.953125 Loss: 0.0013620010577142239\n",
      "Iteration: 18120 Training Accuracy: 0.984375 Loss: 0.0009128344245254993\n",
      "Iteration: 18130 Training Accuracy: 0.953125 Loss: 0.0022620283998548985\n",
      "Iteration: 18140 Training Accuracy: 0.984375 Loss: 0.0020324576180428267\n",
      "Iteration: 18150 Training Accuracy: 0.984375 Loss: 0.0008634579717181623\n",
      "Iteration: 18160 Training Accuracy: 1.0 Loss: 0.0001345836353721097\n",
      "Iteration: 18170 Training Accuracy: 1.0 Loss: 0.0005501930136233568\n",
      "Iteration: 18180 Training Accuracy: 1.0 Loss: 0.0006485843332484365\n",
      "Iteration: 18190 Training Accuracy: 0.96875 Loss: 0.0010357634164392948\n",
      "Iteration: 18200 Training Accuracy: 0.96875 Loss: 0.0012273325119167566\n",
      "Iteration: 18210 Training Accuracy: 0.9375 Loss: 0.0017815488390624523\n",
      "Iteration: 18220 Training Accuracy: 1.0 Loss: 0.0006363311549648643\n",
      "Iteration: 18230 Training Accuracy: 0.96875 Loss: 0.0014164876192808151\n",
      "Iteration: 18240 Training Accuracy: 0.984375 Loss: 0.0007012283895164728\n",
      "Iteration: 18250 Training Accuracy: 0.96875 Loss: 0.0013426253572106361\n",
      "Iteration: 18260 Training Accuracy: 0.953125 Loss: 0.0011559386039152741\n",
      "Iteration: 18270 Training Accuracy: 0.96875 Loss: 0.0019497377797961235\n",
      "Iteration: 18280 Training Accuracy: 0.96875 Loss: 0.0010767860803753138\n",
      "Iteration: 18290 Training Accuracy: 1.0 Loss: 0.0003141317283734679\n",
      "Iteration: 18300 Training Accuracy: 1.0 Loss: 0.0008767583058215678\n",
      "Iteration: 18310 Training Accuracy: 0.984375 Loss: 0.0008109533810056746\n",
      "Iteration: 18320 Training Accuracy: 0.984375 Loss: 0.0006301780813373625\n",
      "Iteration: 18330 Training Accuracy: 1.0 Loss: 0.00046062670298852026\n",
      "Iteration: 18340 Training Accuracy: 1.0 Loss: 0.0005110990023240447\n",
      "Iteration: 18350 Training Accuracy: 0.984375 Loss: 0.0006115463329479098\n",
      "Iteration: 18360 Training Accuracy: 1.0 Loss: 0.0005853897891938686\n",
      "Iteration: 18370 Training Accuracy: 0.984375 Loss: 0.0010527879931032658\n",
      "Iteration: 18380 Training Accuracy: 0.953125 Loss: 0.0017436338821426034\n",
      "Iteration: 18390 Training Accuracy: 0.984375 Loss: 0.0007548059802502394\n",
      "Iteration: 18400 Training Accuracy: 0.96875 Loss: 0.0014235787093639374\n",
      "Iteration: 18410 Training Accuracy: 0.984375 Loss: 0.001250085886567831\n",
      "Iteration: 18420 Training Accuracy: 1.0 Loss: 0.0007773824036121368\n",
      "Iteration: 18430 Training Accuracy: 0.984375 Loss: 0.0008834778564050794\n",
      "Iteration: 18440 Training Accuracy: 0.96875 Loss: 0.0017588513437658548\n",
      "Iteration: 18450 Training Accuracy: 0.984375 Loss: 0.0006689472356811166\n",
      "Iteration: 18460 Training Accuracy: 0.984375 Loss: 0.001937363762408495\n",
      "Iteration: 18470 Training Accuracy: 1.0 Loss: 0.00048214715206995606\n",
      "Iteration: 18480 Training Accuracy: 0.984375 Loss: 0.0015132097760215402\n",
      "Iteration: 18490 Training Accuracy: 1.0 Loss: 0.000981721910648048\n",
      "Iteration: 18500 Training Accuracy: 1.0 Loss: 0.0005754465237259865\n",
      "Iteration: 18510 Training Accuracy: 0.984375 Loss: 0.0007092824671417475\n",
      "Iteration: 18520 Training Accuracy: 0.96875 Loss: 0.0013809807132929564\n",
      "Iteration: 18530 Training Accuracy: 1.0 Loss: 0.0002695776929613203\n",
      "Iteration: 18540 Training Accuracy: 1.0 Loss: 0.0006397307151928544\n",
      "Iteration: 18550 Training Accuracy: 0.96875 Loss: 0.0011882875114679337\n",
      "Iteration: 18560 Training Accuracy: 0.96875 Loss: 0.0014786861138418317\n",
      "Iteration: 18570 Training Accuracy: 0.96875 Loss: 0.001576554961502552\n",
      "Iteration: 18580 Training Accuracy: 0.984375 Loss: 0.00106086116284132\n",
      "Iteration: 18590 Training Accuracy: 1.0 Loss: 0.0008675127173773944\n",
      "Iteration: 18600 Training Accuracy: 1.0 Loss: 0.0003956766158808023\n",
      "Iteration: 18610 Training Accuracy: 1.0 Loss: 0.0007169780437834561\n",
      "Iteration: 18620 Training Accuracy: 0.96875 Loss: 0.0009978660382330418\n",
      "Iteration: 18630 Training Accuracy: 0.96875 Loss: 0.0015034611569717526\n",
      "Iteration: 18640 Training Accuracy: 1.0 Loss: 0.00040840869769454\n",
      "Iteration: 18650 Training Accuracy: 1.0 Loss: 0.0005164361209608614\n",
      "Iteration: 18660 Training Accuracy: 0.984375 Loss: 0.0004508035781327635\n",
      "Iteration: 18670 Training Accuracy: 1.0 Loss: 0.0008589219069108367\n",
      "Iteration: 18680 Training Accuracy: 1.0 Loss: 0.0009156567393802106\n",
      "Iteration: 18690 Training Accuracy: 1.0 Loss: 0.0005844313418492675\n",
      "Iteration: 18700 Training Accuracy: 0.96875 Loss: 0.0010366023052483797\n",
      "Iteration: 18710 Training Accuracy: 0.984375 Loss: 0.0012879300629720092\n",
      "Iteration: 18720 Training Accuracy: 1.0 Loss: 0.0002489583275746554\n",
      "Iteration: 18730 Training Accuracy: 1.0 Loss: 0.00033606908982619643\n",
      "Iteration: 18740 Training Accuracy: 1.0 Loss: 0.0007012858986854553\n",
      "Iteration: 18750 Training Accuracy: 0.984375 Loss: 0.0009743451955728233\n",
      "Iteration: 18760 Training Accuracy: 1.0 Loss: 0.0004181943368166685\n",
      "Training Accuracy = 1.0\n",
      "Validation Accuracy = 0.9211666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "get_accuracy() got an unexpected keyword argument 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_built():\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_resnet34_1_1.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# unfreeze parameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 72\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, batch_size, num_epochs, learning_rate, momentum, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print the final accuracies\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Training Accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m))\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_accuracy(model, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: get_accuracy() got an unexpected keyword argument 'train'"
     ]
    }
   ],
   "source": [
    "# use resnet \n",
    "model = RESNET34()\n",
    "# freeze parameters\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "torch.save(model, 'full_resnet34_1_1.pth')\n",
    "\n",
    "\n",
    "# unfreeze parameters\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "torch.save(model, 'full_resnet34_1_2.pth')\n",
    "\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'full_resnet34_1_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull_resnet34_1_1.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# unfreeze parameters\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mresnet\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'full_resnet34_1_1.pth'"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"full_resnet34_1_1.pth\")\n",
    "\n",
    "# unfreeze parameters\n",
    "for param in model.resnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "\n",
    "if torch.backends.mps.is_built():\n",
    "    model.to(\"mps\")\n",
    "\n",
    "train(model=model, data=train_loader, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, momentum=momentum, verbose=True)\n",
    "\n",
    "torch.save(model, 'full_resnet34_1_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get_accuracy(model, train_loader)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mget_accuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# get the output using resnet\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# select index with maximum prediction\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 17\u001b[0m, in \u001b[0;36mRESNET34.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Forward pass through your fully connected layers\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# get_accuracy(model, train_loader)\n",
    "get_accuracy(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RESNET34(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=30, bias=True)\n",
       "  (batchnorm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
